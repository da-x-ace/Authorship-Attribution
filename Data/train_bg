Since Benchmark’s investment in Ebay 15 years ago, we have been fascinated by online marketplaces. Entrepreneurs accurately recognize that the connective tissue of the Internet provides an opportunity to link the players in a particular market, reducing friction in both the buying and selling experience. The arrival of the smartphone amplifies these opportunities, as the Internet’s connective tissue now extends deeper and deeper into an industry with the participants connected to the marketplace 24×7 – whether they are in the office, at home, or out in the field. It is a special experience to see an entrepreneur go from a PowerPoint describing a new marketplace opportunity to having established an online hub at the epicenter of a particular industry.  Following our investment in Ebay, we have been fortunate enough to invest in several companies that link consumers and suppliers through a successful online marketplace. Companies such as OpenTable, Yelp, Zillow, oDesk, GrubHub, 1stdibs, UShip, and Uber have all reached significant scale within their respective markets. But we have also invested in several companies that we thought had marketplace opportunities that simply did not play out as expected. Simply put, some industries are much more susceptible to the arrival and success of online marketplaces than others.  A true marketplace needs natural pull on both the consumer and supplier side of the market. Aggregating suppliers is a necessary, but insufficient step on its own. You must also organically aggregate demand. With each step, it should get easier to acquire the incremental consumer AS WELL AS the incremental supplier. Highly liquid marketplaces naturally “tip” towards becoming a clearinghouse where neither the consumer nor the supplier would favor an alternative. That only happens if your momentum is increasing, and both consumers and suppliers are sensing an increasing importance of your place in the world. Much easier said than done.  Here are 10 factors to consider when evaluating the potential success of a new marketplace opportunity:  New Experience vs. the Status Quo. Great marketplaces do not simply aggregate a market; they enhance it. They leverage the connective tissue to offer the consumer a user experience that simply was not possible before the arrival of this new intermediary. OpenTable enables the consumer to search reservation availability across hundreds and hundreds of restaurants in a matter of seconds. That capability never existed before, and as a result the delta of the new experience vs. the incumbent experience (dialing restaurants one by one) is extremely high. Another company with a high experience delta is Uber. By aggregating thousands of licensed limousine drivers, and overlaying that with a new-age supply chain management solution, Uber gives it users an experience that is drastically improved compared to the previous alternative. Today, GrubHub announced “Track Your Grub”, a service that allows you to watch your food on the way to your house. When this experience delta is great enough, it creates “wow” moments for new users. “Wow” moments lead to word-of-mouth viral growth and high net promoter scores. Economic Advantages vs. the Status Quo. Some marketplaces provide enhanced economic advantages. oDesk enables companies to easily provision programming talent from all corners of the globe. This helps purchasers procure a cheaper alternative, while also providing brand-new economic lift to the programmer (supplier). Both sides experience an economic advantage. Another interesting example of this bi-directional advantage is AirBNB. For the property owner, the income is “found money” that simply didn’t exist prior to the marketplace. And in many cases the consumer receives a better price as well. If you can positively change the economics of an industry, you will find the participants on both sides rooting for your success. This gives you a huge head start when it comes to tipping the marketplace. Opportunity for Technology to Add Value. In many marketplaces, the technology offering greatly enhances the user experience. Zillow provides homebuyers with an abundance of data that was historically kept in proprietary systems. They have overlaid this data with maps and search technology that provide remarkable richness to the home buyer. Smartphones take this even further, with the ability to learn a great deal about any property with a one-click GPS enabled search. At Uber, the system has “perfect” information in an industry where just two years ago there was a complete lack of visibility (on both sides of the network) that led to enormous waste of resources. Uber’s system enables higher car utilization, more fares per hour for the driver, and faster and faster pickup times for the consumer. At oDesk, the platform enables the planning, development, and transfer of code from the supplier to the purchaser. The marketplace is also a work-flow system that enhances the overall experience for all parties. Facilitating work-flow reduces work for the participants, as well as increasing switching costs. High Fragmentation. High buyer and supplier fragmentation is a huge positive for an online marketplace. Likewise, a concentrated supplier (or purchaser) base greatly diminishes the likelihood of a successful online marketplace. A highly concentrated supplier base will be reluctant to allow a new intermediary in their market, and as a result will likely fight rather than support your arrival. They will also be very reluctant to share in the economics of the industry, as anyone in the online travel industry can confirm. The large airlines have all but obliterated the economics of online ticketing marketplaces, leading all the online players to focus on hotels where the fragmentation and therefore the economics are higher. If you look at the list above of successful Benchmark investments, you will see a common theme of fragmented supplier base. Friction of Supplier Sign-Up. In some markets signing up suppliers is relative easy. In others, it can be a painfully slow process that requires lots of touch and local presence. At companies such as Yelp, Uber, and GrubHub, new city launches are relatively quick after a process model had been established for how to launch those cities. The opposite was true for OpenTable where the installation of a personal computer and internet connectivity were part of the early roll-out requirements. High friction supplier signup can be a barrier to entry (as it is for OpenTable) if you are able to build a successful marketplace.  But in the early stages, this friction slows your roll-out and increases the costs associated with supplier aggregation. Remember, however, that supplier aggregation is the easy part. Aggregating demand is much harder and more critical. Size of the Market Opportunity. A proper TAM (total available market) analysis is imperative, but it is easy to make mistakes looking only at TAM. As a starter, if all the other factors are negative, it will not matter that the market is large. Some markets are crappy candidates for marketplaces. Second, you should also consider the percentage of the market that is likely to use the online alternative. In certain industries, there may be large portions of the market that may not be available to the new online marketplace. An interesting example is healthcare, which is unquestionably a very large market. However, the oligopoly of large players in this market controls a massive percentage of market and is unlikely to support a new alternative. You can also miss-analyze TAM in the other direction. In the case of OpenTable many investors missed the opportunity by mistakenly assuming the TAM was too low. In this case, they underestimated the percentage of the market that OpenTable could penetrate. OpenTable recently passed 10 million diners a month with less than 20% of transactions in North America currently online. You must combine a TAM analysis with the likelihood of marketplace success and penetration. Expand the Market.  Another potential error that can be made while analyzing TAM is to fail to understand that the features and enhancements of the new marketplace may actual expand the market opportunity for the whole industry. This may sound like a brazen claim, but certain marketplaces do indeed expand the market — by exploring new price points or enhancing convenience or usability. oDesk greatly simplifies the process of outsourcing code development, and as such many of its use cases are expansive to the overall market. oDesk’s presence increases the number of first time software outsourcers. Uber’s ease of use and simplicity have led many of its users to greatly increase the number of times they use an alternative car service. Some customers now use it as a second car alternative. As such, the company is meaningfully expands the market for black car services, which is in turn a huge boon to the suppliers that share in the economic expansion. Frequency. All things being equal, a higher frequency is obviously better. Yelp, GrubHub, OpenTable, 1stdibs (for the designer) and Uber are all high frequency use cases, where the consumers can rely on the marketplace as a utility. Many failed marketplaces attack purchasing cycles that are simply way too infrequent, which makes it much more difficult to build brand awareness and word-of-mouth customer growth. Another repeated mistake is attacking verticals where a satisfactory supplier “match” end’s the customer’s need to re-enter the market in search of an alternative. This second point negatively impacts many vertical service provider markets (such as pediatricians) where customers are actually prefer a monogamous relationship. Payment Flow. All things being equal, being part of the payment flow is superior to not being a part of the payment flow. This is due to the fact that it is much easier to extract reasonable economics when you are in the flow of payment. The supplier not only looks to you as a provider of revenue, but they receive that revenue “net of the fee.” Contrast this with a marketplace where you add value first, and then send a bill to the supplier at later date for services rendered. In this latter case the marketplace appears as an expense, and it’s easier for the supplier to view it is a “tax” versus a distribution relationship. Cash is king, and if you bring the cash, you are king. Unfortunately, some industries (like autos) just are not set up for this type of arrangement, as the payment likely lives at the end of a long purchasing process. Network Effects. Network effects are tricky and hard to describe but fundamentally turn on the following question: Can the marketplace provide a better experience to customer “n+1000” than it did to customer “n” directly as a function of adding 1000 more participants to the market? You can pose this question to either side of the network – demand or supply. If you have something like this in place it is magic, as you will get stronger over time not weaker. In the early days of OpenTable we noticed that the reservations per restaurant in a given city were correlated to market penetration. Clearly, the more restaurants that were on the network, the better the value proposition was for the consumer. Something similar occurs with Uber. As the company grows, they are able to facilitate more cars on the road, and along with their investment in route and load optimization, this allows for shorter and shorter pickup times. The experience gets better and better the longer they are in the market. UGC sites like Yelp and TripAdvisor also have strong network effects. Network effects are rare but golden. If you don’t have one try to find one; if you do have one, try to enhance it. It is unlikely that you will find a marketplace opportunity that would score ten out of ten with respect to this list. However a 7 or 8 out of 10 would imply that your opportunity of success is much, much higher than if you only match 3 or 4 out of 10.  It is also important to realize that finding a great opportunity is only a start, and this analysis could easily mislead one into underestimating the critical role that execution plays when it comes to marketplace businesses. Great marketplace execution is more nuanced and less systematic than other venture backed categories, and for every successful marketplace, you will find an amazing entrepreneur that out-executed the many others that had chosen to attack the same market. In addition to great marketplace characteristics, you also need a world-class entrepreneur to make the dream come true.
Earlier today, DogVacay, an exciting new startup in Los Angeles, announced that Benchmark has led its most recent round of financing. DogVacay is an online marketplace that links dog owners with passionate dog care providers who open up their own home as an alternative to the traditional cage-oriented kennel. At first blush, a web site that allows owners to book a “Dog Vacation” for their esteemed pet may seem like an unusual choice for a venture investment. However, a more analytical and detailed look at the market uncovers that this is a high potential, high probability online marketplace opportunity. The most recent Above the Crowd blog post, titled All Marketplaces Are Not Created Equal, outlines ten different ways to judge the potential effectiveness of an online marketplace. You may be surprised how well DogVacay checks out against this list: After taking a detailed look at the crowd-sourced dog care market; we became quite excited about the opportunity at DogVacay. In addition to our analysis, we had the added benefit that the company had been live since March, and we were able to confirm our analysis by witnessing amazing early traction in the field. DogVacay’s six-month ramp and current monthly gross transaction revenue are very reminiscent of the very best of our previously funded successful marketplaces. From out perspective, DogVacay is a winner in the making. We are super excited to be working with Aaron Hirschhorn, the founder and CEO of DogVacay. He is not only an amazingly smart entrepreneur, but also a passionate dog owner. We are also thrilled to be working with First Round once again (as in Mint, Uber), and are excited about our first partnership with Peter Pham, Mike Jones, and the team at Science in Los Angeles.
Many consumer Internet business executives are loyalists of the Lifetime Value model, often referred to as the LTV model or formula. Lifetime value is the net present value of the profit stream of a customer. This concept, which appears on the surface to be quite benign, is typically used to compare the costs of acquiring a customer (often referred to as SAC, which stands for Subscriber Acquisition Costs) with the discounted positive cash flows that will come from that customer over time. As long as the sum of the discounted future cash flows are significantly higher than the SAC, then people will argue it is warranted to “push the accelerator,” which typically means burning capital by aggressively spending on marketing.  This is a simplified version of the formula:   The key statistics are as follows:  ARPU (average revenue per user) Avg. Cust. Lifetime, n (This is the inverse of the churn, n=1/[annual churn]) WACC (weighted average cost of capital) Costs (annual costs to support the user in a given period) SAC (subscriber acquisition costs, sometimes refereed to as CAC = customer acquisition costs) The LTV formula, when used correctly, can be a good tactical tool for monitoring and comparing like-minded variable market programs, especially across channels. But like any model, its proper use is entirely dependent on the assumptions used in that model. Also, people who have a hidden agenda or who confuse a model with reality can misuse it. For many companies that subscribe to its wisdom, the formula slowly takes on more importance than it should. Seduced by the model, its practitioners often lose sight of the more important elements of corporate strategy, and become narrowly fixated on the dogmatic execution of the formula. In these cases, the formula can be confused, misused, and abused, much to the detriment of the business, and in many cases the customer as well.  Here are ten reasons to avoid worshiping at the LTV altar:  It’s a Tool, Not a Strategy.  Heavy LTV companies forget that the LTV model does not create sustainable competitive advantage. You shouldn’t’ confuse output with input. The LTV formula is a measurement tool to be used by marketing to test the effectiveness of their marketing spend – nothing more and nothing less. If one asserts that buying customers below what they charge them is a corporate strategy, this is in essence an arbitrage game, and arbitrage games rarely last. Too many of the variables (specifically ARPU and SAC) are outside of your control, and nothing would prevent another player from executing the exact same strategy. It’s not rocket science; it’s a formula that any business school graduate can calculate. Do not fool yourself into believing it creates a proprietary advantage. The LTV Model Is Used To Rationalize Marketing Spending.  Marketing executives like big budgets, as big budgets make it easier to grow the top line.  The LTV formula “relaxes” the need for near term profitability and “justifies” the ability to play it forward – to spend today for benefits that are postponed into the future. It is no coincidence that companies that put a heavy emphasis on LTV are also the ones that have massive losses as they scale, frequently even through an IPO. Consider that most companies limit any “affiliate fee” they would be willing to spend to 5-10% of sales. Yet when they are marketing, they use different math. They use LTV math, and all the sudden it’s acceptable to spend 30-50% of revenue on customer acquisition. Find the most boisterous executive recommending excessive spending, and you will usually find a loyal servant of the LTV religion. The Model is Confused and Misused. Frequently the same group that is arguing for more spending is the same one that “owns” the LTV calculation. (This is a mistake – finance should monitor LTV).  As a result, it is not uncommon for one to see shortcuts taken that allow for greater freedom. As an example, marketers often divide spend by total customers to calculate SAC rather than just those customers that were “purchased.” If you have organic customers, they shouldn’t be included in the spend calculus. They would have arrived regardless of spend. Also, many people discount “revenues” rather than marginal cash contribution. It is critical to bundle all future variable costs of supporting the customer in order to fairly estimate the future contribution. As an example of the sloppiness that exists around the formula, consider this blog post (http://blog.kissmetrics.com/how-to-calculate-lifetime-value/) from KISS metrics, a company whose aim is to “help you make smarter business decisions.” Not only do they include a version of the model that specifically ignores future costs, but also they recommend taking an average of three different results, two of which are clearly flawed. This voodoo-math has no place as part of a multi-million dollar marketing exercise. Business Isn’t Physics – The Formula Is Not Absolute. LTV zealots often hold an overly confident view of the predictive nature of the formula. It’s not “hard science” like say predicting gravity. It’s at best a “good guess” about how the future will unfold. Businesses are complex adaptive systems that cannot be modeled with certainty. The future LTV results are simply predictions based on many assumptions that may or may not hold. Yet the LTV practitioner often moves forward with a brazen naiveté, evocative of the first time stock buyer who just found out about the price/earnings ratio, or the newcomer to Vegas who has just been taught the basics of twenty-one. LTV models win arguments because executives perceive them to be grounded in science. Just because its math, doesn’t mean its good math. The LTV Variables “Tug” at One Another. This may be the single most important issue and it lies at the heart of why the LTV model eventually breaks down and fails to scale ad infinitum. Tren Griffin, a close friend that has worked for both Craig McCaw and Bill Gates refers to the five variables of the LTV formula as the five horsemen. What he envisions is that a rope connects them all, and they are all facing different directions. When one horse pulls one way, it makes it more difficult for the other horse to go his direction. Tren’s view is that the variables of the LTV formula are interdependent not independent, and are an overly simplified abstraction of reality. If you try to raise ARPU (price) you will naturally increase churn. If you try to grow faster by spending more on marketing, your SAC will rise (assuming a finite amount of opportunities to buy customers, which is true). Churn may rise also, as a more aggressive program will likely capture customers of a lower quality. As another example, if you beef up customer service to improve churn, you directly impact future costs, and therefore deteriorate the potential cash flow contribution. Ironically, many company presentations show all metrics improving as you head into the future. This is unlikely to play out in reality. Growing Becomes a Grind. Let’s say you have a company that estimates it will do $100mm in revenue this year, $200mm the next, and $400mm the year after that. In order to accomplish those goals it is going to invest heavily in marketing – say 50% of revenues. So the budget for the next three years is $50mm, $100mm, and $200mm. How realistic is it to assume that your SAC will drop as you 4X your spend? Supply and demand analysis suggests the exact opposite outcome. As you try to buy more and more of a limited good, the price will inherently increase. The number one place on the planet for marketing spend is Google Adwords, and make no mistake about it, this is an increasingly finite resource. Click-outs are not growing at a meaningful pace, and key word purchases are highly contested. Assuming you will “get better” at buying while trying to buy more is a daunting assumption. The game will likely get tougher not easier. Purchased Customers Underperform Organic on Almost Every Metric. Organic users typically have a higher NPV, a higher conversion rate, a lower churn, and more satisfied than customers acquired through marketing spend. LTV heavy companies are in denial about this point. In fact, many of them will argue until they are blue in the face that the customer dynamics are the same while this is rarely the case. A customer that “chooses” your firm’s services will be much more staisfied than one that is persuaded to buy your product through spend. Find any high-marketing spend consumer subscription company, and I will show you a company with numerous complaints at the Better Business Bureau. These are companies that make it almost impossible to terminate your subscription. When you are scheming on how to trap the customer from finding the exit you are not building a long-term brand. The Money Could Go to the Customer.Think about this. If you are a company that spends millions and millions of dollars on marketing, wouldn’t you be better off handing that money to the customer versus handing it to a third-party who has nothing to do with the future life-time value of the customer? Providing a better value-proposition to the customer is much more likely to endure goodwill than spending on marketing. A heavy marketing spend necessitates a higher margin (to cover the spend), and therefore a higher end user price to the customer! So the customer is negatively impacted by the presence or “need” of the marketing program. Plus, a margin umbrella now exists for competition that chooses to undercut your margin model with a more efficient customer acquisition strategy (such as giving the customer the money).“More and more money will go into making a great customer experience, and less will go into shouting about the service. Word of mouth is becoming more powerful. If you offer a great service, people find out.” – Jeff Bezos LTV Obsession Creates Blinders. Many companies that obsess over LTV, become overwhelmed by LTV. In essence, the formula becomes a blinder that restricts creativity and open-mindedness. Some of the most efficient forms of marketing are viral, social, and effective PR (public relations). Most companies that obsess about LTV are less skilled at these more leveraged techniques. Ironically, it’s the scrappy and capital starved startup with absolutely no marketing budget that typically finds a clever way to scale growth organically.  I love this historic slide from Skype comparing their SAC with that of Vonage, an iconic disciple of LTV analysis.  10. Tomorrow Never Arrives.  The Utopian destination imagined by the LTV formula is a mirage. It almost never works out as planned in the long run. Either growth begins to slow, or you run out of capital to continue to fund losses, or Wall Street cries uncle and asks to see profitability. When this happens the frailty of the model begins to appear. SAC is a little higher than expected. You met your growth target, but the projected loss was bigger than expected. Wall Street is hounding you for churn numbers, but you are reluctant to give them out. The lack of transparency then leads to cynicism, and everyone assumes the worse. It turns out that the excessive marketing spend was also propping up repeat purchase, and pulling back to achieve profitability is increasing churn. Moreover, a negative PR cycle has ensued as a result of your stock decline, and the press’ new doubts about your model. This also impacts results, and customer perception of your brand. The bottom line is that “one day we can stop spending and be remarkably profitable” rarely comes to fruition.  It is not impossible to create permanent equity value with the LTV approach, but it’s a dangerous game of timing – you don’t want to be the peak investor. Let’s say a new business starts with an early market capitalization of A (see graph below). Through aggressive marketing techniques, and aggressive fund raising, the company is able to achieve amazing revenue growth (and corresponding losses), but nonetheless creates a rather sizable organization. At this point, the company is value at point B. Eventually, however, gravity ensues and the constraints outlined herein raise their head, resulting in a collapse to point C.  For early founders and investors at point A, they may do OK (as long as C>A), but it will be accomplished on the backs of later stage investors that helped fund the unsustainable push to point B. This is the story of many a telecom and cable provider expansion history, as well as a few recent Internet companies.   This should not be misconstrued as a eulogy for the LTV formula. It has a very important place in business as a way to contrast and compare alternative marketing programs and channels. It is a tactical marketing tool that requires candor and thoroughness in its implementation. The fundamental reason that it is so amazingly dangerous and seductive is its simplicity and certainty. Generic marketing is conceptual. LTV marketing is specific. Building a plan to grow to a million users organically is an order of magnitude more difficult than doing it with the aid of the LTV formula. There is comfort in its determinism, and it is simply easier to do.  Some people wield the LTV model as if they were Yoda with a light saber; “Look at this amazing weapon I know how to use!” Unfortunately, it is not that amazing, it’s not that unique to understand, and it is not a weapon, it’s a tool. Companies need a sustainable competitive advantage that is independent of their variable marketing campaigns. You can’t win a fight with a measuring tape.
While “Social-Mobile-Local” is certainly an overused buzz phrase, most of the attention has been placed on the “social” and “mobile” parts of the phrase. In social, the spectacular rise of Facebook and Twitter is clearly a disruptive and critical trend. In mobile, the adoption of the smartphone (led by Apple’s iPhone and now catapulted forward by Android) is also a fundamentally important platform transition. Much less attention has been paid to the third concept, “local,” which is ironic since it may be a much larger real business opportunity than either social media or Smartphone application revenue. Over the next five years, this massive opportunity will come into focus as local businesses embrace the Internet and adopt new interactive technologies that increasingly automate the connections between their customers and themselves.  A HUGE OPPORTUNITY  The attached slide will look familiar to readers in Silicon Valley. It appears to be a disruptive, up-and-to-the-right graph that we normally associate with break-out technology companies. This slide, however, maps the rise of the Yellow Pages industry in North America from 1920 to 2007. As you can see, the Yellow Pages business saw incredible revenue growth as the phone became the key point of connectivity for interaction with local business. At its peak in 2007, the North American Yellow Pages business topped out somewhere between $14-16 billion, depending on the source.  Total local advertising and promotion is much larger than just the Yellow Pages. A separate analysis done by Advertising Age, suggests that in 2007, local U.S. businesses spent around $123 billion annually on local media. However, starting in 2008, this market began to materially erode. Why? Newspapers, magazines, local radio, and Yellow Pages represent about 80% of this spend, and the rise of the Internet is unquestionably undermining the  core structure of these industries. Since 2007, Yellow Pages revenues have fallen in half in five years, after taking 87 years to reach their peak. Many newspapers have closed, and others teeter on the edge of bankruptcy. This is not at all shocking. We  know that consumers are using these products less frequently every day. The Yellow Pages business itself suffers from a terminal disease.  If you think back to five years ago, the small business owner was clearly an Internet skeptic. People would say things like “you should have a web site,” but for most local business owners — like a pet-shop or a locksmith — this didn’t mean anything. They had a phone, it was listed in the Yellow Pages – and people could find them. And if the potential consumer went online, the phone number could be found there as well. No problem. For those that did put up a web site, it was, in many cases, a non-event. Some customers might find it, but only the ones that were already looking for them. What’s the big deal?  AN ONLINE AWAKENING  Two things then happened. The first is the critical success of Yelp. Local merchants were suddenly profiled in an environment where the consumer, not the business owner, controlled the copy and the narrative. At first, it was easy to disregard this thing called Yelp as a passing fad. But the voices  got louder and louder – both the happy and the unhappy ones. Accountability and transparency had arrived at the local level. One has to suspect that Facebook’s pervasiveness played a roll in awakening the small business owner too.  By 2011, Facebook had reached 71% penetration of all 221mm U.S. Internet users. Regardless of  industry, when the small business owner now went home, his or her family was constantly on the Internet – playing games, doing research, connecting with friends. The Internet’s pervasiveness could no longer be denied.  Today, the small business owner’s attitude has shifted from denial to anxiety, and, as a result, these local business owners are rushing to the Internet in droves. In Benchmark’s own portfolio, we have eight companies (OpenTable, Uber, Zillow, Yelp, DemandForce, GrubHub, 1stdibs, and Peixe Urbano, *) that generate the majority of their revenue directly from local businesses. Based on estimates, these companies will represent approximately $735mm in revenue in calendar year 2012. Four of these companies have already seen a liquidity event (OpenTable, Zillow, and Yelp have had successful IPOs, and DemandForce was recently purchased by Intuit for $425 million). As small business owners embrace the Internet, the local Internet is firing on all cylinders. Not bad for a customer segment that was once considered a “do not enter” zone for venture capitalists.  THE SMARTPHONE AS A CATALYST  If the decay of the Yellow Pages was a catalyst for the local Internet, then the rise of the smartphone is an accelerant. Smartphone adoption is staggering. Today, there are over 1 billion smartphone users worldwide, and in the U.S., smartphone penetration recently passed 50%. Google has announced that Android is activating over 850K new users a day. These mobile devices are frequently the preferred device (vs. a personal computer) when a consumer looks to interact with local businesses. For the eight companies mentioned above, mobile usage already represents between 25-50% of overall customer usage depending on time of day and day of week. And mobile usage looks destined to increase from here: DigitalBuzz predicts that mobile Internet users will pass desktop Internet users within the next 3 years.  The rise of the Smartphone as a new platform is a huge benefit for entrepreneurs. Simply put, large incumbents are typically slow to make shifts to new platforms. This is either because they are overly focused on their current strength, or simply too large and bureaucratic to move quickly. Often, it is a combination of both. Startups on the other hand are eager to find a point of leverage or advantage, and rush to new platforms. New platforms typically have  “hooks” that enable features that never existed on the previous platform, further differentiating the startups offerings. A great example on the Smartphone is using GPS for one button local search. New platforms also require new distribution techniques, and in such a “jump ball” scenario the incumbent’s advantage evaporates. One could argue the incumbents are even at a disadvantage as they are less likely to have the cutting edge technical employees who understand the new platforms.  Changing the Game: Going Deep  But there is an even greater limitation on the power of incumbents than their discomfort with new platforms. As the market moves away from Yellow Pages-like listings and directories as a proxy for advertising, many young companies, taking a page out of the playbook of data-driven software-as-a-service companies, have created deep vertical integration within their spaces in order to drive traffic and enable services. By organizing small business owners, supplementary service providers, and customers on a single canonical set of data, these companies are not only providing new ways for customers to discover local businesses: they are creating new ways for local businesses to interact with customers. They are moving from “listing” services to “automation” services, and they are stitching these Internet services deep into the nervous system of the target industry.  For example, a company like OpenTable provides, on a stand-alone basis, a premises based computer that is an extremely effective tool for restaurants to manage their tables — a digital version of the reservation book on the maitre d’s desk. By connecting that same data on the Internet, and aggregating that data from other restaurants, you have OpenTable’s incredible online reservation system. Along that same data spine, customers can add reviews, limousine services and florists can enhance the dining experience, and a location-aware Smartphone app can tell you what restaurant within walking distance of where you are has a table available right now. The “offering” is the complete network, not just one specific piece, and the pieces alone are less compelling.  Going “deep” like this is a significant challenge for larger incumbents. The playbook requires a deep understanding of the industry, access to all the key content and its structure, a targeted and experienced sales structure, and a willingness to invest in a market that may seem “niche” to the broader service provider. You have to be willing to get your hands dirty. These large companies favor a horizontal, one-size-fits-all approach, offering a widget that all local companies would potentially use (such as virtual loyalty cards). But these lightweight offerings from the incumbents will fall well short of the “automation” features and functionality enabled by the innovators digging deeper into the vertical.  We’ve already seen a couple of recent examples of this with Google. In mortgages, Google launched a product but ultimately retreated, citing prioritization concerns and “taking a hard look at products that haven’t been as successful as we had hoped.” A seemingly simple category like mortgages proved difficult to nail within the overall Google strategic framework. Likewise, in order to gain a foothold in travel — a space where deep verticals thrived for many years —Google ultimately realized they had to pay $700mm for ITA Software in order to acquire the vertical tools they needed to be successful.  The Real Winner: The Customer  If you look closely at many of the leading companies developing these deep verticals, like Zillow or OpenTable or Uber or AirBnB, they are providing far more than just advertising opportunities for local businesses. These companies are using new technologies like mobility and location to improve communication, interaction and overall customer experience.  The amazing thing about these new local Internet companies is how much value the consumer gets from this data-driven, vertically-integrated experience. Watching your Uber driver approaching your location on GPS forever alters your experience of taxis and limos, while at the same time providing total transparency up and down the value chain, from dispatcher to driver to fleet manager.  But the really exciting part is that we are still really early in this process of transformation away from listing/directory advertising to a local Internet.  By way of comparison, in the fourth quarter of 2011, Southwest Airlines reported that 86% of its revenue was booked online.  By comparison, only 12% of US restaurant reservations are booked online. Only 15% of dentists are connected to customers through services like DemandForce.  Only 3% of takeout orders are processed through online offerings like GrubHub. And less than 1% of realtors are premier agents on Zillow.  We all know intuitively where those numbers are headed in the future.  *Benchmark Capital is also super excited about its investment in Nextdoor, the leading social network for local neighborhoods and communities. Join 3,000 other local communities who have revolutionized how neighbors interact online. 
This morning, Intuit announced its agreement to acquire one of Benchmark’s portfolio companies, Demandforce, for $424mm. As with Instagram, Benchmark Capital is the largest institutional investor in Demandforce. Unlike Instagram, which is a consumer application and is extremely well known, Demandforce focuses on local professional businesses and has chosen to keep an intentionally low profile – a strategy that has served them well.  Great entrepreneurs often blaze their own trails, and the founder and CEO of Demandforce, Rick Berry, is no different. In a day and age of social media, where many companies project a persona much larger than reality, Demandforce chose instead to focus on its customers and its products. We never even announced Benchmark’s funding of the company, which I believe is unprecedented. The Demandforce team always felt that the attention should be focused on the customer rather than the company.  Demandforce’s customer mission has always been the same – to help small businesses thrive in an evolving and increasingly complex connected world. Today, they are the leading provider of interactive “front office” SAAS services to thousands and thousands of professional small business owners. The Demandforce product is a powerful web-based application that seamlessly integrates with existing workflow systems, works automatically, and delivers guaranteed results. Through this, Demandforce provides local businesses – like salons, auto shops, chiropractors, dentists, and veterinarians – with affordable and easy access to the tools and platforms that large enterprises use to communicate with customers, build a strong online reputation and leverage network marketing. It you have ever received an automated communication from your dentist, it was likely sent through Demandforce.  Demandforce’s success puts it at the forefront of the burgeoning “Local Internet” wave. The combination of Internet pervasiveness and smartphone penetration has led to a complete reconfiguration with regard to how local businesses interact with their customers. These local businesses have traditionally spent over $125B/year on traditional media, and this is only in the U.S. But the channels they have historically used, such as the newspaper and the yellow pages, are increasingly compromised. These business owners know they need new solutions, and these dollars will be reallocated to these exciting new platforms. Benchmark believes this “Local Internet” wave is many times larger than the “social” and “mobile” themes with which it is often contrasted. In addition to DemandForce, Benchmark is fortunate to have backed such “Local Internet” market leaders as OpenTable (OPEN), Zillow (Z), Yelp (YELP), Peixe Urbano, GrubHub, Uber, and Nextdoor.  It has been an honor and a pleasure to work with Rick Berry, Patrick Barry, Hoang Vuong, Mark Hale, Sam Osman and Annie Tsai at Demandforce. This is truly one of the best teams ever assembled. It was also a pleasure to work with Steve Kostyshen as well as Mike Maples of Floodgate and Peter Ziebelman of Palo Alto Venture Partners, all of whom preceded us in their investment, and all of whom are passionate fans of the company.  It is certainly thrilling to see a team of entrepreneurs reach a significant milestone such as this.  That said, it is equally bittersweet as it means we will no longer be working directly with them on this incredibly compelling mission. Our loss is unquestionably Brad Smith and Intuit’s gain. Combining the leading “front office” small business SAAS vendor with the iconic Silicon Valley small business company is an incredibly compelling combination.h
For the past two months, I changed the default search engine on my browser (ironically Chrome) from Google to Bing. I have used Bing almost exclusively for this period, and have two quick conclusions.  1) With regards to core search, the Bing results were perfectly fine. I never struggled to find anything. I never forced myself to redo the search on Google. So I would say Bing is on-par in terms of traditional, core search quality.  2) Where I did struggle was with the non-core search searches (i.e. maps, images, videos, news). Microsoft and Google use slightly different UIs on these non-core searches, and I had no idea how trained I was on the Google UI. Trying to learn the Bing tools and features was quite frustrating, and on those searches – I kept returning to Google. Plus, I didn’t realize how often I transition from one type of search to another (from core to maps, or core to news). This was another point of frustration. Keep in mind, I did not have a problem with Bing’s non-core results, just rather the navigational elements.  At the end of the day, for me, my user “lock-in” is associated not with the quality of Google results, but rather with the understanding of the UI features and levers.  More like a traditional software application.
A few relevant scenes from the recent blockbuster Moneyball:  Peter Brand: Billy, Pena is an All Star. Okay? And if you dump him and this Hatteberg thing doesn’t work out the way that we want it to, you know, this is…this is the kind of decision that gets you fired. It is! Billy Beane: Yes, you’re right. I may lose my job, in which case I’m a forty four year old guy with a high school diploma and a daughter I’d like to be able to send to college. You’re twenty five years old with a degree from Yale and a pretty impressive apprenticeship. I don’t think we’re asking the right question. I think the question we should be asking is, do you believe in this thing or not? Peter Brand: I do. Billy Beane: It’s a problem you think we need to explain ourselves. Don’t. To anyone. Peter Brand: Okay.  ———————————  Grady Fuson: No. Baseball isn’t just numbers, it’s not science. If it was then anybody could do what we’re doing, but they can’t because they don’t know what we know. They don’t have our experience and they don’t have our intuition. Billy Beane: Okay. Grady Fuson: Billy, you got a kid in there that’s got a degree in Economics from Yale. You got a scout here with twenty nine years of baseball experience. You’re listening to the wrong one. Now there are intangibles that only baseball people understand. You’re discounting what scouts have done for a hundred and fifty years, even yourself!  March 26, 2012: These two scenes from Moneyball illustrate something that may be essential to modern business: the incredible value of youth and innovative thinking relative to traditional experience. It turns out that the Moneyball character Peter Brand’s real name is not Peter Brand (played by Jonah Hill), but rather Paul DePodesta. And he didn’t go to Yale, but instead Harvard. He was indeed young – twenty-seven when he went to work for Billy Beane – and he did have an actual degree in Economics. What’s more, as you can see in the interaction above, Billy valued Paul’s (Peter Brand’s) opinions and decisions – despite the fact that he was a complete novice with respect to baseball operations.  A month or two ago, I had the unique opportunity to share the stage with Billy Beane at a management offsite for one of the leading companies in the Fortune 500. We were both fielding questions about innovation, and what one can do to keep their organization innovative. I talked about how many of the partners that have joined Benchmark Capital have been extremely young when they joined, including our most recent partner Matt Cohler who joined us at the age of 31. At Benchmark, we believe that young partners have many compelling differentiators. First, they will ideally have strong connections and compatibility with young entrepreneurs, who are frequently the founders of the largest breakout companies. They are also likely to be frequent users of the latest and greatest technologies (all the more important with today’s consumer Internet market). Like the “Moneyball” situation described herein, young VCs are open to new ways of doing things. This form of “rule-breaking,” or intentionally ignoring yesterday’s doctrine, may in fact be a requirement for successful venture capital investing.  When I mentioned this intentional bias towards youth, Billy Beane abruptly concurred. He noted that injecting youth into the A’s organization is also a key philosophy of his. Paul DePodesta may have been the first young gun that Billy hired, but he was far from the last. Billy continues to recruit young, bright, talented people right out of college to help shake up the closed-minded thinking that can develop with an “experience only” staff. Also noted was the fact that if a certain “experience” is shared by all teams in the league, then it is no longer a strategic weapon. You can only win with a unique advantage.  The impact of youth on the technology scene is undeniable. The included table lists the founding age of some of the most prominent founders of our time. The facts are humbling and intimidating, especially for someone who is no longer in their twenties or early thirties. Can someone in their forties be innovative? Or, do the same things that produce “experience” constrain you from the creativity and perspective needed to innovate?  Lets look at some of the specific advantages of youth. First, as mentioned before, without the blinders of past experience, you don’t know what not to try, and therefore, you are willing to attempt things that experienced executives will not consider. Second, you are quick to leverage new technologies and tools way before the incumbent will see an opportunity or a need to pay attention. For me this may be the bigger issue. The rate of change on the Internet is extremely high. If the weapon du jour is constantly changing, being nimble and open-minded far outweighs being experienced. Blink and you are behind. Youth is a competitive weapon.  The point Billy raised regarding the fleeting value of experience is also important to consider. As the world becomes more and more aware of a trick or a skill, the value of that experience begins to decay. If word travels fast, the value of the skill diminishes quickly. Best practice becomes table stakes to stay-afloat, but not to get ahead. We see examples of this every day with Facebook application user acquisition techniques. Companies find a seam or arbitrage that creates a small window of opportunity in the market, but quickly others mimic the same technique and the advantage proves fleeting.  Back before the Yahoo BOD hired Carol Bartz, there was much speculation about the important traits for Yahoo’s next CEO. Most of the analysis honed in on two key traits for the company’s next leader – the ability to lead and the ability to innovate. I remember trying to think about leaders that I thought would have a chance at having a measurable impact. On one hand, you could put a very young innovative executive into the role, but it is hard to imagine handing a $15B public company over to someone remarkably inexperienced. The other side of the coin is equally difficult – thinking of a seasoned executive who has the ability to dramatically innovate Yahoo’s products and business model.  There were only a handful of people (as few as three) that I could think of at the time that fit this second profile. Thinking back now, they all shared the following characteristic: despite being experienced CEOs, these individuals all “thought young” i.e. they were open-minded and curious. And they did not believe that experience gave them all the answers. These type of executives love diving head-first into the latest and greatest technologies as soon as they become available.  If you want to stay “young” and innovative, you have no choice but to immerse yourself in the emerging tools of the current and next generation. You MUST stay current, as it is illusionary to imagine being innovative without being current. Also realize that the generational shifts are much shorter than they were in the past. If you were an innovative Internet company five short years ago, you might have learned about SEM and SEO. Most of the newly disruptive companies are no longer using these tools as paths to success – they have moved on to social/viral techniques. The game keeps changing, and if you are not “all-in” in terms of learning what’s new, than you may be falling rapidly behind.  Consider these questions:  When a new device or operating system comes out do you rush out to get it as soon as possible – just because you want to play with the new features? Or do you wait for the dust to settle so that you don’t make a mistaken purchase. Or because you don’t want to waste your time. Do you use LinkedIn for all of your recruiting, or do you mistakenly think that LinkedIn is only for job seekers? How many connections do you have? Is your profile up to date? (When Yahoo announced Carol Bartz as CEO, I did a quick search on LinkedIn.  She was not a registered user.) When you heard that Zynga’s Farmville had over 80MM monthly users, did you immediately launch the game to see what it was all about, or do you make comments about how mindless it is to play such a game? Have you ever launched a single Facebook game? Do you have an Android phone or do you still use a Blackberry because your Chief Security Officer says you have to? I know many “innovators” who carry an iPhone and an Android, simply because they know these are the smartphones that customers use. And they want exposure to both platforms – at a tactile level. Do you use the internal camera app on your iPhone because it’s easy, or have you downloaded Instgram to find out why 27mm other people use that instead? Do you leverage Twitter to improve your influence and position in your industry or is it more comfortable for you to declare, “why would I tweet?,” before you even fully understand the product or why people in similar roles are leveraging the medium? Do you follow the industry leaders in your field on Twitter? Do you follow your competitors and customers? Do you track your company’s products and reputation? How many apps are on your smart phone? Do you have well over 50, or even 100, because you are routinely downloading each and every app from each peer and competitor you can to see how others are exploiting the environment? Do you know how WhatsApp, Voxer, and Path leveraged the iphone contact list for viral distribution? Do you know what Github is and why most startups rely on it as the key center of their engineering effort? Have you ever mounted an AWS server at Amazon? Do you know how AWS pricing works? Does it make sense to you to use HTML5 as your mobile solution so that you don’t have to code for multiple platforms? Does it bother you that none of the leading smartphone app vendors take this approach? When you are on the road on business, do you let your assistant book the same old car service, or do you tell them, “I want to use Uber just to see how it works?” When Facebook launched the new timeline feature did you immediately build one to see what the company was up to, or did you dismiss this as something you shouldn’t waste your time on? Have you been to Glassdoor.com to see what employees are saying about your company? Or have you rationalized why it’s not important, the way the way the old-school small business owner formerly dismissed his/her Yelp review. The really great news is that being a “learn-it-all” has never been easier. With the Internet, high-speed broadband, SAAS, Cloud-services, 4G, and smart-phones, you can learn about new things, 24 hours a day, no matter where you are or what you do. All you need is the internal drive and insatiable curiosity to understand why the world is evolving the way it is. It is all out there for you to touch and feel. None of it is hidden.  There are in fact many “over 30” executives who can go toe-to-toe with these young entrepreneurs, precisely because they keep themselves youthful by leaning-in and understanding the constantly evolving frontier. My favorite “youthful” CEOs are people like Marc Benioff and Michael Dell, who frequently can be found signing up for brand new social networking tools and applications. Reed Hastings has more than once answered Netflix questions directly in Quora.  Jason Kilar frequently communicates directly with his customers through Hulu’s blog. Rich Barton, the co-founder of Expedia and Zillow is one of those people carrying both an Iphone and an Android, and is constant learning mode. I would also include Mark Cuban, whose curiosity is voracious. The other NBA owners never saw him coming. And lastly, there is Jeff Bezos, who seems to live beyond the edge, imagining the future as it unfolds. Watch the launch of Kindle Fire in NYC, and you will have no doubt that Jeff plays with these products directly and frequently.  Our last table highlights the stats from the Twitter account of some of these “youthful,” learn-it-all executives (sans Mr. Bezos – we all wish he tweeted). If you don’t find this list interesting, think about the thousands and thousands of executives out there who are nowhere to be found with respect to social media. They take the easy way out, likely blaming their legal department. They intentionally choose not to learn and not to be innovative. And they refuse to indoctrinate themselves to the very tools that the disrupters will use to attack their incumbency. That may in fact be the most dangerous path of all.
Back in October, Techcrunch announced that Dropbox had raised $250mm at a seemingly absurd valuation. Many firms, including my firm Benchmark Capital, participated. When this happened, many people asked us why this was a special company that would cause us to break our standard investment paradigm. They didn’t quite understand why this was a company that deserved once-in-a-generation special attention.  The first answer to this question is rather straightforward, but not earth shattering. Drew Houston and his team had taken a hard problem — file synchronization — and made it brain dead simple. Anyone that had used previous file synchronization programs, including Apple’s own iDisk, constantly encountered state problems. Modifications in one location would get out of synch with those in another, ruining the  entire premise of seamless synchronization. It wasn’t that these other companies did not understand the problem, it was just that they could not execute on the solution. The Dropbox team solved this, which was a critical innovation.  Although this was critical, nailing technical synchronization would not necessarily warrant outsized valuations. In order to be worth $40B one day (which is 10X the $4B reported round, the objective return of a VC investment), the company would need to hold a place in the ecosystem that is far more strategic than that of a simple high-tech problem solver. So what is it Dropbox does that is so special?  This evening, TechCrunch reported that Dropbox would automatically synch your Android photos. Once again, someone could suggest “so what, how hard is it to do that?, and why is that worth billions?”  Here is why. Once you begin using Dropbox, you become more and more indifferent to the hardware you are using, as well as the operating system on that device. Dropbox commoditizes your devices and their OS, by being your “state” system in the sky. Storing credentials and configurations of devices, and even applications are natural next steps for this company. And the further they take it, the less dependent any user becomes of the physical machine (HW and SW) that is accessing that data (and state). Imagine the number of companies, as well as the previous paradigms, this threatens.  That is a major, major deal. And it comes at a time where there are many competing platforms on both desktop and mobile. This “unsure” market backdrop ensures the need for a cross-platform solution and plays right into Dropbox’s hand. You can lose your desktop computer, you can lose your smartphone. It doesn’t matter, because all you really care about is in the Dropbox cloud.
Attached are my thoughts on the Facebook S-1 along with some quick stabs at valuation.  Brief disclosure, Benchmark Capital has a minority position in Facebook as a result of the acquisition of FriendFeed, a company that was incubated in our offices.  I thought it would be useful to look at Facebook using the scorecard from our May 24 blog post, “All Revenue is Not Created Equal, the Keys to the 10X Revenue Club.” For those that want to save time, the key point of this piece is that there is a broad disparity of Price/Revenue multiples for global Internet stocks, and that only a very small fraction of these companies achieve a multiple over 10X. We also created a list of 10 factors that public investors consider when trying to qualify if a company is deserved of such a prestigious and lofty valuation.  On a roll, these factors are:  1. Sustainable Competitive Advantage – how big is the competitive Moat? 2. Presence of Network Effects – does the model tip to a single vendor? 3. Visibility/Predictability – is the revenue consistent 4. Customer Lock-in / High Switching Costs – is it expensive to leave? 5. Gross margin levels – How much leverage exists is the business? 6. Marginal Profitability Calculation – is the leverage still expanding? 7. Customer Concentration – are there key dependencies? 8. Major Partner Dependencies – are there key dependencies here as well? 9. Organic Demand vs. Marketing Spend – is customer acquisition expensive? 10. Growth – how big will the future be?  So how does Facebook score on these metrics? As you would expect, pretty well.  Metric:	Comments:	Grade: Sustainable Competitive Advantage	It would be extremely hard to launch a direct-on competitor to Facebook.  Look at what has happened to Friendster, MySpace, Bebo, and is happening to Orkut in Brazil.  Google+ as a FB competitor is a tough slog.	A+ Presence of Network Effects	These are about as strong as you could design. All current non-US Facebook users have immediate connections if they log-in.	A+ Visibility/Predictability	This is fairly strong as well, simply because there is no lumpiness.  There is a small dependency on Zynga that could cause variability. Also, a premium product would offer more consistency than pure ads.  That said, this is not an issue.	A Customer Lock-In / Switching Costs	Leaving Facebook is possible, but finding an alternative with all your friends on it is not really possible.  Obviously, the inclusion of Timeline works to increase this even more by creating a permanent dependence on past content. Also, Facebook’s DAU number is staggering. Over half of all users check-in daily. That is uber lock-in.	A+ Gross Margin Levels	Gross margin has hovered between 75-80% for the last several quarters.  This is a fantastic overall gross margin. It would be great to think they have more leverage here, but as the largest Internet site in the world, this probably represents peak margins.	A Marginal Profitability Calculation	On this one Facebook doesn’t score so well.  Peak profitability (on a margin % basis) was in Q4 of 2010, and since then spending has kept pace with revenue growth. It is likley that the team would argue they are “investing for the long-term,” but if the long term is forever, than EPS growth is permanently tied to revenue growth.	B- Customer Concentration	Zynga is 12% of revenues, but this is fairly low and they are the only company over 10%. Plus, if Zynga stopped competing for these ad purchases, there are many, many Zynga look-alikes that would rush to fill that void. So even if they left tomorrow (which they won’t) the number would not go away completely.	A Partner Dependency	Facebook has grown to be the largest site in the world with the help of no one. No partners. No dependency.	A+ Organic Demand	All of Facebook’s customers are organic. This is as good as it gets.  The pure stuff.	A+ Growth	Facebook grew the top line 88% in 2011. That’s quite amazing. Q4 of 2011, however, was only 55%.  People will definitely be watching this number in Q1. If growth rate hurts the company, then it’s a direct result of waiting too long to go public – past peak growth.	B The bottom line is that these scores are fantastic. Facebook is a shoe-in for the 10X+ revenue club. Perhaps the only question is which years’ revenue you consider. If the company grows 50-60% in 2012, you end up with roughly $5.5-6B in revenue. With all the hype, assume a 12x multiple on the $6, and you end up right at $72B. You can double-check this with earnings. As operating margin is stable, 60% growth would result in $1.6B in after-tax earnings. At $72B, this is a 45 PE ratio for a company growing at 60%. At a 60 PE, you would have a $96B market capitalization. The bottom line is that the banker range looks right to me. Of course, overt and ecstatic demand for the hottest IPO of the past 10 years could easily lead to much higher speculative valuations. But it’s hard to argue that the $70-100B range is wrong. Feels quite right to me.  Here are a few other interesting things from the S-1:  Tax Rate. Warren Buffet’s secretary would be happy. Facebook’s tax rate is already north of 40%. Other multi-national companies typically have found a way to reduce this. Facebook is paying full-boat. Model appears set. With gross margin relatively fixed, and peak operating margins over 5 quarter ago, investors should get comfortable that bottom-line growth is limited by top line growth. Management could change their attitude later, but experience suggests that founders like Zuckenburg want to invest for the long term. As a result, one shouldn’t expect these super healthy margins to go any higher. Sales > R&D. It is somewhat surprising that sales expense is greater than R&D expense. The ad units clearly are not self-serve. Interestingly, this ratio is very similar for Google. Seasonality. The company has more seasonality than I would have expected (geared towards Q4). The prospectus says this is tied to traditional advertising seasonality. Facebook’s unique RSU program. In an effort to avoid the restrictions of 409A, Facebook long ago created an RSU structure whose shares vest on a liquidity event.  As a result, a large amount of stock (close to $1B in value) will all “vest” on the IPO. This will result in an enormous one-time, non-cash charge. What I still can’t figure out, is how this will effect the overall share count. If you know let me know, and I will append the post. If auditors and the SEC are happy with this RSU structure, I would expect to see other startups adopt it, as it avoids the restrictions of 409A. Cash. Over $3.9B in cash already. And they will raise $5B more. That’s a lot of cash.
Each January, being the season of New Year’s resolutions, it is common to find people you know discussing the pros and cons of various dietary pursuits. Individuals across the globe are eager to turn over a new leaf, get on a new bandwagon, make a new start. Yet, even with a strong will, its not at all obvious what the right recipe should be. Pick almost any diet, and you will find several experts and PHDs praising it, and an equal number panning it. You would think that with all our technology and understanding of the human body, there would be more consistency in our approach. I saw a tweet yesterday that said, “Diet guides are the political blogs of personal improvement.” This feels right. But why do discussions about something that is supposed to be scientific, feel like religious or political arguments?  I happend to “consume” three interesting pieces of content this past year on the subject of nutrition (two of these come via my partner @peterfenton). For reasons which I will disclose later, I recommend you “consume” each of them, regardless of whether you have a strong pro or con bias after hearing the descriptions.  Most recently I just finished Gary Taubes new book, Why We Get Fat. For those in the know, this book is a toned down, more reader friendly, less technical version of Taubes 2008 New York Times best seller, Good Calories, Bad Calories. Taubes, a successful science journalist and researcher, obliterates the past 30-40 years of  medical rhetoric when it comes to diet and nutrition. He not only explains the physiology behind why the perspectives of the past are misguided, but also highlights the rather obvious point that “it ain’t working.”  Obesity rates are exploding. If we knew what to do, wouldn’t that be contained? The second  piece of content is a video lecture titled Sugar: The Bitter Truth, by Robert H. Lustig, a MD and professor at UCSF. Most 89 minute professorial lectures in medicine fall way short of two million views on YouTube, but Robert’s lecture is nearly at that milestone. Lustig pulls no punches in pointing directly at sugar (specifically high fructose corn syrup – HFCS) as the clear cause of the obesity epidemic we now face — not red meat, not fat, not the lack of a balanced diet, and not too little exercise. Moreover, he notes that food processors increasingly inject HFCS into a large majority of the packaged foods we feed ourselves and our children. This lecture is very compelling. As an added bonus, here is a lengthy article of Taubes reviewing Lustig: Is Sugar Toxic? Lastly, a briefer entry. This past July, Jane Brody of the New York Times, penned Counting Calories? Your Weight Loss Plan May Be Outdated. This article is a summary of a detailed 20-year research effort from five experts at Harvard that looked into the specific diets of 120,000 individuals. The main point of Brody’s title is that, based on these results, not all calories are created equal. In fact, this study found that potato chips, french fries, and sweetened beverages have a high correlation with weight gain, whereas other foods actually had correlation with weight loss. If you want to see the full study it can be found here: Changes in Diet and Lifestyle and Long-Term Weight Gain in Women and Men. These three pieces of content had a few common themes that will likely sound “heretic” to many readers. The real enemy are sugars and carbohydrates. Taubes and Lustig make this explicitly clear. Our body is quite efficient with processing excess fat andprotein that we eat, but excess carbohydrates covert into fat on our bodies. Remember the argument that excess fats cause obesity and heart disease? Complete bullshit according to Taubea. Our physicians, our government, and our schools all rallied behind a 30-year movement to lower fat intake. As Lustig notes, it worked…we did lower fat intake…yet we kept getting fatter. Carbohydrates and sugars are addictive. Addictive the way cigarettes are. If you become a slave to massive carbohydrate intake, your body will actually crave more carbohydrates. And the bigger you are, the more you will crave. It’s hard to lose weight if you are consuming an addictive food. The calorie-in, calorie out ideal is a complete farce. How many times have you heard someone say, “all you have to do is burn more calories than you consume.” This notion that a calorie is a calorie is a calorie suggest that our bodies process each of these food types the same. Taubes and Lustig say absolutely not. Moreover, the Harvard project highlights the dangerous impact of potatoes, a seemingly harmless food that is present in every child’s school cafeteria. All food is certainly not created equal. You can’t exercise your way to thin. Simply put, you cannot burn enough calories to make yourself thin (with the exception of extreme amounts of exercise). However, with the right diet, you can lose weight without even exercising. How often do you hear that from a doctor? Now despite what you may think, my point is not to convince you that these guys have it right. I don’t actually have a horse in this race. What I find amazing is that very educated and well reasoned experts can come to a conclusion that runs so counter to the conventional wisdom of our entire healthcare profession and our government health agencies. Moreover, despite whether you agree with their conclusion, they make a remarkably cogent arguments. Should it be this easy to prove everyone (i.e. the majority) wrong? And once again, why didn’t we have it right in the first place? And why are people so emotionally driven when it comes to their perspectives on topics such as this? (I am certain people will post comments to this blog post along the lines of “Taubes’ an idiot!”)  The human body is a complex system. Complex systems, such as stock markets, weather patterns, ant colonies, and large governments, all behave in ways that make specific prediction extremely difficult. This is because these systems involve millions of variables that are interconnected in non-linear ways that may be dynamic and dependent on potential initial states that could number in the billions. Such systems, which are well studied, are known for being unpredictable, difficult to understand, and are easy to underestimate. [One of the most influential books I have ever read is Complexity, a 1993 work by Mitchell Waldrop.]  One of the primary issues with complex systems is that people draw misleading conclusions regarding cause and effect of certain variables and how they relate to the overall system. As an example, one might note that 9 times out of 10 when variable X is set to 1, the sun is out, and so they proclaim that variable X causes the sun to come out. But the truth is they have no idea whether the sun drives the variable or the variable drives the sun. Or perhaps an entirely different variable that we are not looking at drives the sun, and all we are witnessing is ten random data points that happen to have 9-1 organization. One doesn’t really know.  But we still assume. And we try. Humans like answers and patterns. The truth is we always have. The Greek and Norse gods were early human attempts at understanding the sky, stars, and oceans. If we don’t have a specific answer we think up the best one we have, and we all glom onto it; it is better than the alternative of admitting to everyone that we don’t have a clue. Then we teach it to everyone else, and they all believe it too. Ironically, the more you come to know something through this passing of memes or ideas, the more argumentative, fanatical, or “religious” you might be. The lack of a fundamental understanding opens the door for a spiritual one. No one has an uber-passionate view on how gravity works. But politics, stock prices, and diets are a different matter. In these complex worlds, people “believe” what they cannot know.  Can we all get it wrong? When it comes to understanding complex systems, we can and we do. If you are looking for one more piece of content to consume, I recommend you watch this lecture from the late Michael Crichton: States of Fear: Science or Politics? Chrichton shows numerous examples from history where the majority misread and misunderstood complex systems. Additionally, he highlights how the mass opinion can lead to action that has well-intended but negative implications on the system. Perhaps it should go without saying, but it is particularly hard to influence a system you don’t fully understand.  By now, you may be wondering “what is my point?” Here it is. When it comes to not fully understood complex systems, it is easy to get things wrong. In fact, its easy for everyone to get them wrong. Don’t fear the new idea or the fresh perspective, and don’t believe something just because everyone else does. But watch out for the preacher with certainty — the ones that are spewing hellfire and brimstone. They are the ones most certainly to be wrong.  [Spencer Rascoff of Zillow pointed out this great New York Times article highlighting how all the smart powers that be completely missed the housing crisis, despite all the signs being there. Another example of everyone (including the experts) getting it wrong.]
Twitter is having a remarkable year. Active users have soared to over 100 million per month, with daily actives now above 50 million. Tweets per day are over 250 million. Most top actors, athletes, and artists are all active on Twitter. Every news and sports program proudly advertises its Twitter account handle. No one would consider running for public office without a strong Twitter presence. Global news in any region breaks first and spreads fast on Twitter. Even uber-socialist Hugo Chavez of Venezuela has 2.24 million followers (which puts him slightly behind Mandy Moore, but just ahead of Queen Latifah).  So, Twitter’s traffic has been growing in leaps and bounds. It has become an indispensable tool for managing personal and corporate brands. And Twitter, along with its verb form “tweet”, have become words in everyday usage all over the world. Yet despite these impressive strides, Twitter’s upside is far, far greater and its user base will expand by an order of magnitude – as soon as the service can overcome a major perception problem.  Twitter suffers from two key misperceptions that need to be resolved before the business can reach its true potential. The first misperception is that Twitter is simply another social network, like Facebook. People commonly think of Twitter as a variant of Facebook. The press frequently positions the two together as “leaders in social networking.” This pairing erroneously implies that the two services are used for the exact same thing, even though the two platforms are very different. Facebook is a few-to-few communication network designed for sharing information and life events with friends. Twitter, on the other hand, is a one-to-many information broadcast network. The only way magic happens on Facebook is through reciprocity: I friend you and you friend me back – then information flows. But on Twitter, I can get something out of following Shaquile O’Neil who has no social obligation to follow me back.  As its roots are in communication, a key part of the Facebook value proposition is sharing information. Any potential anxiety with regards to Facebook sharing is reduced by the fact that these communications are generally seen only by one’s friends. In fact, users react quite negatively when this information is unknowingly shared more broadly. For the people who view Twitter as a Facebook variant, they immediately assume the platform’s core purpose is for the user to broadcast his or her own thoughts and personal information (like Facebook), but to a much broader public audience. For those with this perception, the notion of potentially exposing their own private thoughts to the broad public Internet is overwhelming and uninteresting.  The second, and more critical, Twitter misperception is that you need to tweet, to have something to say and broadcast, for the service to be meaningful to you. For many non-Twitter users, Twitter is an intimidating proposition. “Why would I tweet?,” and “…but I don’t want to tweet” are two common refrains from the non-adopter that highlight this key misperception. But this completely misses the point as to why Twitter has become such an amazingly powerful Internet destination for 100 million others. For the vast majority of Twitter’s next 900 million users, the core usage modality will have very little to do with “tweeting,” and everything to do with “listening” or “hearing.”  Twitter is an innovative and remarkable information service. While it is amazingly democratic and allows literally anyone to broadcast publicly as a “tweeter,” the core value in today’s Twitter is the amazing flow of curated and customized information that emanates from its crowd-sourced user feeds. Other Internet networks like to keep the user “inside.” Much like Google, Twitter points out to the world. It’s a “discovery engine” and an “information utility” rolled into one. With Twitter, you get news faster, you see updates from your favorite artists, you hear directly from key politicians, and gain insights from influencers in a wide variety of specializations. Just as Facebook is symmetric in terms of its poster-reader relationship, Twitter is highly asymmetric. The majority of the tweets on Twitter are posted by a small sub-set of the users. And the majority of the users get value from “reading” or “listening” to the tweets from these core influencers. Once again, for most users it’s more about what you hear, learn, and find than the fact that you can tweet.  In many ways, Twitter is much more of a competitor to other “discovery tools” and “information sources” than it is to Facebook. Facebook is unquestionably the number one resource for “sharing with the people in your life.” From this perspective, Facebook competes (extremely well) with email, instant messengers, and certainly other symmetric social networks like MySpace. Twitter, on the other hand, competes most directly with other tools that help you find important links, news, and information. It is in this broad, non-friend based crowd-sourcing and speed of discovery where Twitter truly shines. A recent Tweet by famed sci-fi author William Gibson highlights this point. Having become accustomed to the non-linear speed of information flow on Twitter, Gibson grew frustrated watching news of the Osama bin Laden killing on TV: “Network news feels like trying to suck cold tar through a milkshake straw.”  Some who understand this point have suggested that Twitter is merely a “Better RSS reader.” While this analogy is directionally more accurate than the Facebook comparison, it greatly underestimates the power and value of Twitter. RSS feeds are simply computerized information “routers” that require complex setup, initialization, and maintenance. Twitter has three breakthroughs that make it dramatically more powerful than simple RSS. First and foremost, your personalized Twitter feed is human-curated by a potential universe of millions of curators. When you “check Twitter” you are looking at the specific articles and links purposefully chosen by people you have chosen to follow. That is powerful leverage. Second, it is easily extensible. Due primarily to the concept of “retweeting,” the simple act of using Twitter exposes you to new and interesting sources to follow. It evolves into a richer and more customized offering over time. You discover new people as well as new information. Lastly, Twitter’s unique handles and follower networks create a strong-form network effect that has high lock-in and high switching costs. Twitter and its top tweeters have a deeply symbiotic relationship.  So what can Twitter do to solve this misperception problem? The first thing they can fix is the new user registration flow, a process that has already begun. Earlier this year, a new user would be encouraged to “tweet” very early in the registration process, basically reinforcing the perception problem. Today’s “first 60-second” Twitter experience is quite different and revolves around choosing the influencers you will follow. You should expect even more evolution in this direction in the future. Next, Twitter must make it crystal clear to the press and prospective user that there is an amazingly powerful value proposition for non-broadcasting users. This will not be easy, as it requires a reprogramming of perception across a broad audience. Not only will this aid in incremental adoption, but it will also help subdue the confusion with respect to Facebook.  Twitter is on an amazing trajectory and will continue to increase in usage and influence.  However, the power of this discovery platform is much more about the tweets themselves, and not simply about every single user having the ability to tweet.
Many journalists have offered their opinion on Netflix’s recent changes, its stock price decline, and their even more recent branding changes (Qwikster). Yet in each article, it appears as if the journalist all agree that the price move (creating separate prices for streaming and DVDs) was a bad strategic move. As an example, Techcrunch notes:  “Raising prices for those of us who opt for both streaming and DVDs would have been fine if Netflix had a deeper streaming catalog. But the gap is still too big, and the price hike seemed premature. Your customers are extremely loyal. Don’t piss them off.”  The problem with this perspective is, in my opinion, the price move was not a “decision,” so much as a “reality” presented to Netflix from the content owners in Hollywood.  Hollywood is a unique place, and understanding “business” in Silicon Valley leaves you ill-prepared to understand what makes Hollywood tick (for more on this see: When It Comes To Television Content, Affiliate Fees Make The World Go ‘Round). Very few people understand the key underpinning of the Netflix “original” business model — a 1908 Supreme Court Ruling known as “first sale doctrine.” From Wikipedia:  “The doctrine allows the purchaser to transfer (i.e., sell, lend or give away) a particular lawfully made copy of the copyrighted work without permission once it has been obtained.”  Because of the first-sale doctrine, any DVD reseller, including Netflix, can basically buy a DVD at WalMart, and turn around and rent it to someone else the very same day. The content owners have absolutely no control over whether the copy can be resold or rented. Period. As such, Netflix has the ability to rent (via DVD) any movie which has ever been sold on DVD, and its costs are relatively fixed as a result of the retail price of the actual DVD. In some ways, it is a perfect storm.  Fast forward to digital streaming and all bets are off.  More specifically, the first-sale doctrine does not apply. That’s right. For DVDs, Netflix’s rights are unlimited and its costs are constrained. For digital, its rights are constrained and its costs are unlimited. In the absence of the first-sale doctrine, Netflix must negotiate each and every title, and the price of the right to stream that digital title is up to the whim of the content owner. For many titiles, you cannot even obtain digital rights, because they can’t find all the people the need to release the rights to do so.  So here is what I think happened with Netflix’s recent price change (for the record, I have no inside data here, this is just an educated guess). Netflix has for the past several years been negotiating with Hollywood for the digital rights to stream movies and TV series as a single price subscription to users. Their first few deals were simply $X million dollars for one year of rights to stream this particular library of films. As the years passed, the deals became more elaborate, and the studios began to ask for a % of the revenues. This likely started with a “percentage-rake” type discussion, but then evolved into a simple $/user discussion (just like the cable business). Hollywood wanted a price/month/user.  This is the point where Netflix tried to argue that you should only count users that actually connect digitally and actually watch a film. While they originally offered digital streaming bundled with DVD rental, many of the rural customers likely never actually “connect” to the digital product. This argument may have worked for a while, but eventually Hollywood said, “No way. Here is how it is going to work. You will pay us a $/user/month for anyone that has the ‘right’ to connect to our content – regardless of whether they view it or not.” This was the term that changed Netflix pricing.  With this new term, Netflix could not afford to pay for digital content for someone who wasn’t watching it. This forced the separation, so that the digital business model would exist on it’s own free and clear. Could Netflix have simply paid the digital fee for all its customers (those that watched and not)? One has to believe they modeled this scenario, and it looked worse financially (implied severe gross margin erosion) than the model they chose. It is what it is.  Netflix is an amazing company, and Reed Hastings is one of the best CEO’s Silicon Valley has ever seen. That said, at age fourteen, the digital world is forcing Netflix to execute a pivot. And the world they are entering is radically different from the world they are leaving. There is no longer a first-sale doctrine to keep things neat and tidy.
As you likely know, I am a big believer that the IPO can play a key role in the development of a company’s life. Moreover, I have argued that many in our ecosystem have an unhealthy anxiety regarding the dangers and consequences of being public. Lastly, I have argued that the IPO window is wide open for great companies – something I still believe today. All that said, I have been quite surprised by the recent trend in companies that file and then chose to delay. If you are going to file the S-1, it is imperative that you are prepared to follow through. Standing too long in the middle of the financial equivalent of the river Styx can have severe consequences.  Why is this a bad thing? The longer a company remains on file without pricing, the more questions arise about “why” the company may be struggling to move forward. Did they miss their numbers already? Are they having cold-feet? Are they not ready?  Do investors not like the company? Have the bankers lost their belief on the company? Employees may begin to wonder the same thing. As you are in a quiet period, it may be difficult for you to respond to concerns through the press. If you then take the added step and “pull” your IPO, you now risk being considered a “broken” deal and potentially a “broken” company. Potential acquirers will certainly see it that way. These problems can be especially acute in Silicon Valley, where competition for talent is intense. Lastly, to file and not price is to give up all the benefits of being private with none of the gains of being public. You have been exposed, but you have nothing to show for it.  There are many things that can cause delays in filed IPOs. The most common factor is unexpected questions from the SEC that cause iteration and re-filing. This is especially true of the SEC questions that require the auditors to revisit their original assumptions. Shaky investor sentiment as a result of a weak broader stock market can cause both investors and bankers to have “cold feet.” There may also be concerns with valuation and dilution. If your company looks like it is going to price at a 30% discount to what your bankers conveyed on filing date, you may not want to suffer unexpected dilution. Lastly, there may simply not be enough demand for your IPO – which is an amazingly tough position for your company.     The attached table shows the # of days from pricing to filing for some recent IPOs as well as the days on file for Zynga, GroupOn, and Kayak. These five companies had an average pricing-filing span of just under 100 days. Two of the IPOs in which Benchmark was lucky enough to be an investor (Zillow and ServiceSource) had particularly good showing on this “pricing-to-filing” metric with 93 and 94 days respectively. (*Just added Bankrate, which had an error-free 62 day filing to pricing window). GroupOn is starting to move outside this ban, but recent news suggests they may be back “on track” with a target date of late October (this would equate to 150 days on file). Zynga’s IPO is listed as “delayed” on Yahoo Finance while standing at 75 days. Kayak, a leader in the travel search space, had been on file for 301 days – a precarious position for any company.  While many of these potential causes of delay appear external and “out of your control,” there are in fact many things you can do to minimize the number of days between filing and pricing.  Don’t start the process until you are ready. This certainly includes knowing your business is performing well, but also includes having the auditors ready, having your financials in order, having a strong CFO and general counsel, having your BOD ready to go, and generally being prepared for what is about to happen. Talk to other CEOs who have kept the process on time, and find out how they prepared. Pick a banker who understands that you are sensitive to filing-pricing timing. Some bankers will tell you this metric is not critical. You own the problem if you are stuck in a filed but un-priced company. You should tell the service provider what is important to you, not the other way around. Great investment bankers have a strong understanding of SEC process, SEC rules, and may even have an ex-SEC representative on staff. These things matter, and you should be able to tell whether or not they matter to your banker. Also, find out before you file if your banker believes in you and your business. If you are defending your business to your banker “after” filing the S-1, you had a clear sequencing problem. Watch out for “wedding planners.” IPO are expensive and as such, they tend to attract “service providers” encouraging you to purchase the “royal package” at every turn. The argument, just as with a wedding, is that you only do this once, and therefore; expense should be of little concern. There are two problems with this logic. First, companies about to be public should not be carelessly wasting money. Second, the “royal” package takes more time and slows things down, and will inherently contribute to extending the pricing-filing window. Pick experienced professionals in every slot.  There are many constituencies that are involved in your IPO process – auditors, valuation firms, compensation firms, external counsel, underwriter’s counsel, bankers, analysts, even these whacky constituents known as “printers.” You want professionals who know how to get things done, which is very different from the “wedding planners.” Think Harvey Keitel as Winston Wolf from Pulp Fiction. “I solve problems.” Facilitation is key.[youtube=http://www.youtube.com/watch?v=wWmRTjLRMfU] Intentionally target a smaller offering. Many investment banks will encourage larger offerings (see point 3). While this serves them well, it may be at odds with maximizing the probability of a successful pricing. Less supply means less demand is required to pull off a successful offering. A smaller offering also will make all shareholders less sensitive to dilution and therefore pricing. Once again, do not file if you do not plan to price, and this includes all prices in the planned offering range. Don’t disrespect the precious nature of an open window. The four companies above were on file during a very strong IPO window, and as a result had seemingly error-free processes. Being prepared to go when things are good means avoiding the situation where you file, and the global market melts down in your face. If (1) your company has the numbers to be public, (2) your company is ready and prepared to be public, and (3) the IPO market is healthy and the window is clearly open and you still chose to wait to go public than you are accepting the timing risk of the future.  As Geddy Lee of Rush says, “If you choose not to decide, you still have made a choice.” Growth can slow, markets can turn, new competitors can show up. Going public too early clearly has risks – but so does waiting too long and missing your opportunity. IPO markets will always have “pulled” and “delayed” IPOs. This is simply the nature of the beast. An open IPO window attracts two types of companies – those that should go public, and those that “need” to go public for capital reasons. Portions of the “need” group will always fail to find supporters, and therefore you should not view delays and withdrawals as signs of a weak IPO market. That said, certain delays can and should be avoided. If you are stepping up to the plate for an IPO, be ready, be prepared, and be committed to seeing it through. Don’t submit an S-1 if you don’t plan to price. Waiting on file for extended periods of time can be catastrophic.
With the IPO market now blown wide-open, and the media completely infatuated with frothy trades in the bubbly late stage private market, it is common to see articles that reference both “valuation” and “revenue” and suggest that there is a correlation between the two. Calculating or qualifying potential valuation using the simplistic and crude tool of a revenue multiple (also known as the price/revenue or price/sales ratio) was quite trendy back during the Internet bubble of the late 1990s. Perhaps it is not peculiar that our good friend the price/revenue ratio is back in vogue. But investors and analysts beware; this is a remarkably dangerous technique, because all revenues are not created equal.  What drives true equity value? Those of us with a fondness for finance will argue until we are blue in the face that discounted cash flows (DCF) are the true drivers of value for any financial asset, companies included. The problem is that it is nearly impossible to predict with any accuracy what the long-term cash flows are for a given company; especially a company that is young or that might be using an innovative and new business model. Additionally, knowing what long-term cash flows look like requires knowledge of a vast number of disparate future variables. What is the long-term growth rate? What is the long-term operating margin? How long will this company hold off competition? How much will they be required to reinvest? Therefore, from a purely practical view, the DCF is an unruly valuation tool for young companies. This is not because it is a bad theoretical framework; it is because we don’t have accurate inputs. Garbage in, garbage out.  Because of the difficulty of getting DCF right, investors commonly use a handful of other shortcuts to determine valuations. “Price earnings ratio” and “enterprise value to EBITDA” are common shortcuts, with their own benefits and limitations. I want to argue that for a variety of reasons, the price/revenue multiple is the crudest valuation tool of them all.  The following chart highlights 2012 forward price/revenue ratios for 122 global Internet stocks. The broad range of results is nothing short of staggering. On one end is Overstock, trading at 0.2X analyst’s 2012 revenue estimates. On the other end is Youku.com, the leading Chinese video website (recent IPO LinkedIn is not included in this list).  Youku trades at 21.7X analysts average 2012 revenue estimate. The other companies live at many different places along this wide continuum. Now consider that the press and some investors frequently use price/revenue as their primary valuation tool when our data suggests there is a 100X difference in value per sales dollar from Overstock to Youku.com. Talk about room for error! What is that hot new company worth? This graph would suggest that the company’s revenue alone is a very poor guide.   Before we talk about why there is such disparity, it is important to highlight a few more points. As you can see in the above graph, there is a very long tail to the left. Basically, there are many more low-price/revenue multiple companies than high. The following table shows this statistically. Over 72% of the companies have a 2012 price/revenue multiple below 4x. Also, you can see that only 12 of these 122 companies (<10%) have multiples over 7X. There are only 5 above 10X. Also recognize that the majority of these high multiple companies are domiciled outside the U.S. This is important because the press tends to favor the higher multiples, such as 10X revenues, as their “defaults.” The problem is, only a handful of companies deserve to be in the “10X club.”   What causes such a wide dispersion of price/revenue multiples? While one might not have the specific numbers required to complete an accurate DCF, we do know which business qualities would have a positive impact on a DCF exercise, all things being equal. When investors see a large number of these traits, they then have an increased confidence that the elements are in place that will lead to a strong DCF value over time. You often hear people refer to companies with strong DCF characteristics as having high “revenue quality.” Companies with characteristics that are inconsistent with a strong DCF model are said to have low “revenue quality.”  Here are some of the key business characteristics that would be used to separate high quality revenue companies from low quality revenue companies, and therefore are the distinguishing traits that warrant high price/revenue multiples.  1. SUSTAINABLE COMPETITIVE ADVANTAGE (WARREN BUFFET’S MOAT)  By far, the most critical characteristic that separates high multiple companies from low multiple companies is competitive advantage. This concept, well explained in Porter’s book by the same name, basically asks the question, “How easy is it for someone else to provide the same product or service that you provide?”  If your company has “high barriers to entry,” Wall Street will be super excited, as investors will have confidence discounting cash flows many, many years into the future. Coca-Cola has a 5% estimated 2012 growth rate, and a 3.6x price/revenue multiple. RIM has a 12% estimated 2012 growth rate and a 0.77x price/revenue multiple. What gives? Investors expect Coke to be around in pretty much its same form 50 years from now. It is much harder to say that with confidence about RIM. Warren Buffet famously refers to these barriers to entry as an “economic moat,” inferring an image of the body of water that protects access to a castle.  [For more on this topic, I highly reccomendan amazing paper on this subject, Competitive Advantage Period “CAP,” The Neglected Value Driver by Mike Mauboussin, the Chief Investment Strategist at Legg Mason, and an adjunct finance professor at Columbia Business School.]  If high price/revenue multiple companies have wide moats or strong barriers to entry, then the opposite is also true. Companies with little to no competitive advantage, or companies with relatively low barriers to entry, will struggle to maintain above-average price/revenue multiples. If an investor fears that a company’s competitive position (which allows them to create excess cash flow) is tenuous and will deteriorate, then the value of the enterprise may be worth the cash flows only from the next several years.  2. THE PRESENCE OF NETWORK EFFECTS  No discussion of competitive advantages and barriers to entry is complete without a nod to perhaps the strongest economic moat of all, network effects. In a system where the value to the incremental customer is a direct function of the customers already in the system, you have a powerful dynamic that tips towards winner take all. Perhaps the definitive piece on this type of advantage is Brian Arthur’s Increasing Returns and Two Worlds of Business published in HBR back in 1996.  This “second world” that Brian refers to is one where the market leader has an unfair advantage that is reinforced by network effects.  There are a few important things to remember about network effects. Some network effect systems are stronger than others. What is key is the decay rate of value of the incremental user to the customer value function. Second, networks effects are discussed way more than they exist. Many things people indentify as network effects are merely economies of scale, which are not nearly as powerful. Unfortunately, strong form network effect companies are far and few between. Fortunately, when they do exist, they are typically leading candidates for the 10X+ price/revenue multiple club.  Microsoft, Ebay, Skype, Google Adwords, and Facebook (in their prime) all benefited from network effects.  3. VISIBILITY/PREDICTABILITY ARE HIGHLY VALUED  For the same reason that investors favor companies with sustainable competitive advantages, investors favor pricing models that provide a high level of predictability and consistency in the future. It is easy to see why revenue visibility would have a positive impact on a DCF analysis. The more certain you can be of future cash flows, the higher premium you will put on a business, and as a result, you will see a higher price/revenue multiple. One obvious example of this is the predictable nature of SAAS subscription revenue. Salesforce.com trades at a staggering 7.5x 2012 estimated revenues. SuccessFactors trades at 7.9x 2012 estimated revenues.  Subscription revenue businesses take longer to grow than traditional software businesses, but once you reach scale investors put premium multiples on the predictable future revenue streams.  The opposite of subscription revenue is revenue that is one-time or episodic. Traditional software models are one-time in nature. Consulting revenue is also typically one-time. Revenue that will only happen once, or that is highly likely to go away in future years, will command much lower price/revenue multiples. As a general rule, game companies, where the “hit” nature of the product offering will eventually ensure a finite life of most of its products, typically trade at discounted price/revenue multiples. Activision trades at just over 2X 2012 estimated revenues. Electronic Arts trades at 1.7x times the same estimate. Non-publisher game companies, where revenues may often come from a single title, will have even lower price/revenue multiples. Conversely, the game companies that get higher multiples are ones that own more of a publishing/distribution platform, such as TenCent in China. These companies are able to extract rent from whatever the hot game happens to be, and are therefore less vulnerable to “hit” risk.  4. CUSTOMER LOCK-IN / HIGH SWITCHING COSTS  If investors value predictability, than retaining customers for long periods of time is obviously a positive. Conversely, if customers are churning away from your company, this is a huge negative. Investors are highly fixated on churn rates, as they should be. Churn has a direct and significant impact on a DCF model. With subscription models, a low-churn customer is quite valuable. In fact, companies with excessively low churn rates (5% annually or less) are very likely to have price/revenue multiples in the top decile. Obviously, high churn rates are really bad for all valuation multiples.  For non-subscription businesses, customer-switching costs also play an important role. If it is relatively easy for your customer to switch back and forth from your products to you competitors, you will likely have a lower price/revenue multiple as your pricing power will be quite limited. On the other hand, if it is quite difficult for a customer to switch away from your product/service, you are likely to have stronger pricing power, and longer customer life, which will inevitably result in better DCF dynamics. Switching costs can take many forms – technical lock-in, data lock-in, high startup costs with a new vendor, and downstream revenue dependencies are just a few.  All things being equal, high switching costs are a positive for price/revenue multiples, and low switching costs are a negative.  5. GROSS MARGIN LEVELS  This may seem super-basic or even tautological but there is a huge difference between companies with high gross margins and those with lower gross margins. Using the DCF framework, you cannot generate much cash from a revenue stream that is saddled with large, variable costs. As a result, lower gross margin companies will trade a highly discounted price/revenue multiples. Amazon (20% gross margin), which is certainly among the very best retailers when it comes to execution, trades at a low 1.5x 2012 revenue estimates. Wal-Mart (25% gross margin) trades at 0.41x 2012 revenues. Best Buy (24% gross margin) trades at only 0.22x forward revenues. All things being equal, gross margin percentage should have a direct impact on price/revenue multiple, as there will obviously be more gross margin dollars to contribute to free cash flow. Journalists who quickly apply 10x multiples to all private companies should at the very least consider gross margin levels in their analysis.  6. MARGINAL PROFITABILITY CALCULATION  Investors love companies with scale. What this means is that investors love companies where, all things being equal, higher revenues create higher profit margins. Microsoft had wonderful scale in this manner for many, many years. Selling more copies of the same piece of software (with zero incremental costs) is a business that scales nicely. Companies that are increasing their profit percentage while they grow are capable of carrying very high valuation multiples, as future periods will have much higher earnings and free cash flow due to the cumulative effect of growth and increased profitability.  In order to measure how a business is scaling, many investors look at marginal incremental profitability. This can be done on a quarter-over-quarter basis, or a year-over-year basis. Simply look at the change in revenue versus the change in costs, and then calculate the incremental operating margin of the two results. If this marginal profitability number is much higher than historical profitability, a company is scaling nicely, and the investor has picture proof of that occurrence. If this number is lower than historic profitability, it raises a red flag for investors, who may be concerned that investments in new growth initiatives are yielding lower cash flow per dollar than previous investments.  Google’s recent first quarter results provide a nice example here. As you can see in the graph, Google’s incremental marginal profitability for Q1 was actually negative on both a year-over-year and a quarter-over-quarter basis. If a company is scaling nicely, you will see a marginal incremental profitability that is actually higher than the current profit margin. Google stated on its earnings call, that the company was simply investing for the long-term over the short-term, and was not concerned about this trend. Investors viewed things differently, and sent the stock down $48 the next day, representing a 7% fall from $578 to $530/share.   This is also the reason that “human capital” businesses like consulting businesses often have trouble with low valuations on Wall Street.  If the majority of costs are people, and people are also the key input for any work product, you will find the ability to generate increased marginal profitability quite difficult.  7. CUSTOMER CONCENTRATION  In their S-1, companies are required to highlight all customers that represent over 10% of their overall revenue?  Why do investors care about this?  Once again, all things being equal, you would rather have a highly fragmented customer base versus a highly concentrated one. Customers that represent a large percentage of your revenue have “market power” that is likely to result in pricing, feature, or service demands over time. And because of your dependence on said customer, you are likely to be responsive to those requests, which in the long run will negatively impact discounted cash flows. You also have an obvious issue if your top 2-5 customers can organize against you. This will severely limit pricing power. The ideal situation is tons of very small customers who are essentially “price takers” in the market.  Google’s AdWords program is a great example.  8. MAJOR PARTNER DEPENDENCIES  Investors will discount the price/revenue valuation of any company that is heavily dependent on another partner is some way or form. A high profile example of this is Demand Media’s reliance on Google’s SEO traffic. Google isn’t the customer per se, but they can heavily impact the outcomes for Demand. And even if they don’t impact them (the recent quarter was in line with expectations), the mere awareness that they could, can have drastic impact on long-term valuation, and therefore price/revenue multiple.  These dependencies are also disclosed in the S-1 under “Risk Factors.” Here is the example of the risk disclosure of Demand’s dependence on Google from an SEO perspective:  “We depend in part on various Internet search engines, such as Google, Bing, Yahoo!, and other search engines to direct a significant amount of traffic to our owned and operated websites. For the quarter ended September 30, 2010, approximately 41% of the page view traffic directed to our owned and operated websites came directly from these Internet search engines (and a majority of the traffic from search engines came from Google), according to our internal data.”  These strong dependencies eat away at investors simply because the company is exposed to issues that are out of the control of management. As an example, Kayak’s potential IPO buyers will need to get comfortable with Google’s acquisition of ITA, Kayak’s use of ITA, and whether or not Google goes from being a source of traffic to a competitor. Likewise, if and when Zynga files for an IPO, new investors will be inherently betting on whether or not Zynga’s Facebook dependency is a positive or a negative. No one wants a partner policy or algorithm change to have unpredicted negative impacts on a public company. These risks are accounted for with lower valuation multiples.  9. ORGANIC DEMAND VS. HEAVY MARKETING SPEND  All things being equal, a heavy reliance on marketing spend will hurt your valuation multiple. Think about this simplistic example. There are two stores in the middle of town. One has a product/service that customers love, and as a result, customers flock to the store day in and day out all on their own. These customers then tell other potential customers, and through this “word of mouth” process, the customer base grows even larger. The second storeowner advertises frequently, and all new customers are a result of this advertisement and promotion (which obviously costs $$). Which business would you prefer to own?  Which one would likely have higher cash flows?  If you have to “buy” or “rent” your customers, you have a non-optimal business model – plain and simple.  The empirical data backs this up. You will be hard pressed to find a company with a heavy marketing spend with a high price/revenue multiple. Perhaps the very best Internet company that invests heavily in marketing is Netflix (marketing is about 15% of sales in recent quarter). When it comes to execution, Netflix is considered by many to be the best of the best. So you have a company that is highly regarded for their management prowess, and that is growing over 50% year over year. Yet, they trade at 4X 2011 revenue estimates and 3X 2012 estimates. And this is the best of the best.  The majority of companies that are heavy marketers trade at price/revenue multiples well below Netflix.  Consider another point. Most of the companies that have really high multiples, and that have been highly respected by investors all have or have had organic growth: Yahoo, Ebay, Google, Facebook, Skype, OpenTable, Baidu. These business models did not require marketing. The picture included below is borrowed from a Skype slide deck from a few years back, and does an amazing job of highlighting the difference between “bought traffic” and organic growth. As Niklas highlighted, the cost of acquiring a new Skype user was $0.001, versus $400 for Vonage, a very heavy marketer. Which company deserved a higher price/revenue multiple?   For a period of time, Jeff Bezos was a heavy investor in marketing, but after a while he retrenched. “About three years ago we stopped doing television advertising. We did a 15-month-long test of TV advertising. And it worked, but not as much as the kind of price elasticity we knew we could get from taking those ad dollars and giving them back to consumers,” said Bezos. “More and more money will go into making a great customer experience, and less will go into shouting about the service. Word of mouth is becoming more powerful. If you offer a great service, people find out.”  This should not be read as a blanket condemnation of all marketing programs, but rather a simple point that if there are two businesses that are otherwise identical, if one requires substantial marketing and one does not, Wall Street will pay a higher valuation of the one with organic customers.  10. GROWTH  We saved the best for last. Nothing contributes to a higher valuation multiple like good ole’ growth. Obviously, the faster you are growing, the larger, and larger future revenues and cash flows will be, which has direct implications for a DCF. High growth also implies that a company has tapped into a powerful new market opportunity, where customer demand is seemingly insatiable. As a result, there is typically a very strong correlation between growth and valuation multiples, including the price/revenue multiple.  There is another reason why the premium paid for growth in 2011 may be even higher than it has been in the past.  As you can see from the table below, some of the largest names in technology are really struggling to grow. When you combine this fact with the paucity of IPOs from the past five years, the public technology investor has been starved from investing in companies with interesting growth characteristics. As such, they are likely to be super-excited by any company with a growth rate over 25%. If its over 50 or 100%, they will be ecstatic. Trading in and out of companies with low growth rates is simply not that interesting to an investor.   So growth is good, correct? There is a reason to save growth for last. While growth is quite important, and even thought we are in a market where growth is in particularly high demand, growth all by itself can be misleading. Here is the problem. Growth that can never translate into long-term positive cash flow will have a negative impact on a DCF model, not a positive one. This is known as “profitless prosperity.”  In the late 1990s, when Wall Street began to pay for “revenue” and not “profits” many entrepreneurs figured out a way to give them the revenues they wanted. It turns out that if all you want to do is grow revenues, with disregard for the other variables, it is quite simple to “manufacture” awe-inspiring revenue growth. To prove the point, consider this oft-used example from the Internet bubble. What if I had a business where I sold dollars for $0.85? What would my revenue growth look like? Obviously, you could grow this business to $ billions in revenue tomorrow. While this may be tongue and cheek, the real world example of the “dollar for $0.85” metaphor is any business where the value transfer to customers and suppliers and employees cannot be sustained at a positive profit. The customer will be thrilled with any “below market” offering, and will rush in to get all they can. In this case, the growth was actually created by the demand for the unsustainable offering.  There is another situation where growth can be misleading. If a company stumbles on to a hot new market, but lacks “barriers to entry” or does not have a sustainable competitive advantage, there will eventually be trouble. In fact, the very success of the first company in the field will act as a siren inviting others into the market, which, in the absence of a competitive advantage, will lead to margin erosion. Many electronics products follow this trend as some hot new product is quickly commoditized.  THE 10X CLUB  So there are ten business characteristics that can impact a company’s chances of making it into the 10X+ price/revenue multiple club. Clearly, some of these variables are interdependent, and clearly you may find a company or two without every single characteristic, that still make the club. That said, most of the companies that trade at 10X or higher in terms of price/revenue will do extremely well against this scorecard.  All of which brings us to last week’s real world example, LinkedIn.  There has been much written about the LinkedIn IPO, and its tremendous after-market performance. As of Monday, LinkedIn’s market capitalization was $8.3 billion. Analysts have not published forward revenue estimates, but we have heard of investor models that put 2012 revenue anywhere between $550 and $700mm. Assuming these are accurate, LinkedIn trades between 11.8-15x 2012 revenues. This lofty valuation has attracted scrutiny from around the globe, including skeptical analysis from both the New York Times and Barron’s.  In the table below, you will see that LinkedIn does extremely well against our 10X club criteria list. It has growth, it has very high barriers to entry, it has network effects, and it has little to no dependencies. The only criticism one might have is that they are not showing enough profitability or marginal profitability. Profitability increased from Q3 to Q4 last year, but the company ramped sales spending in Q1, and profitability waned. So, assuming that the company is willing to show profit expansion over the next few years, it’s not that unreasonable for the company to trade at a 10X price/revenue multiple.   However, all companies with which the press and public are enamored are not LinkedIn. There are many hot brand-names with lofty private valuations and strong revenues, that would not do so well on the “10X scorecard.” Over the next 12-18 months we should see these companies test the public markets, and with the benefit of data and a truly liquid marketplace, we should gain a better appreciation for real valuation. If we’ve learned anything from the past market cycles, it’s that the fundamentals eventually matter, and all revenues are decidedly not created equal.  Update (5/26/2011): After some feedback from readers, and some further thoughts, there are a few more additions to the list worth mentioning, although in less detail.  1. Capital Expenditure Intensity – All things being equal, a company with heavy CapEx will trade at a lower price/revenue multiple (for sure). Capital intensity requires constant funding which will dilute either shares (through increaased offferings) or directly use up earned cash.  2. Cash flow / Earnings - Some companies generate way more cash flow than earnings, and some do the opposite (generate way more earnings than cash flow). The higher your ratio is of cash/earnings the better off you are. This can be accomplished in numerous ways, but one of the more common is to collect cash from your customer ahead of your accounting driven revenue-recognition. Cash is king, and if your cash margin is better than your accounting net income margin, you are golden. The opposite is also true. Companies that genreate far less free cash flow than earnings are going to have lower valuation multiples.  3. Optionality – This topic is a bit more abstract, but sometimes a company, due to its market position, is in a strong position to have optionality on a whole new business. A few years back, Amazon was trading at 1x revenue and had just launched AWS. AWS was an “option” on a whole new business, and eventually began to be valued as such.  4. TAM – One of the readers asked about TAM, which stand for Total Available Market. The assertion is that TAM can affect valuation multiple. I understand the concept, but I have not seen this play out in reality. Most of the companies that suffer from TAM never make it to the public markets. Also, companies that have high price/revenue multiples typically have optionality into other markets. So basically, I think TAM can radically affect private company valuations, but less so for public.
Mark Vickery, On Thursday March 24, 2011, 4:58 pm EDT “BlackBerry maker Research In Motion (NasdaqGS: RIMM – News) beat its fiscal 4Q EPS estimates by 2 cents per share, but missed slightly on quarterly revenues and offered guidance well below the current consensus. This has sent RIMM shares down nearly 10% in after-market trading…”  Yesterday, after the market closed, Research in Motion, the makers of the Blackberry device, announced that they would be lowering their current quarter earnings due to lower average sales prices. In a separate announcement, the company proffered that their new tablet will support Android apps, yet the CEO also made it clear that he believes the world is overly focused on the criticality of having a large numbers of applications on your platform. They also suggested that the guidance issue is temporary, and relates mainly to a product cycle not a systematic change in the industry.  Despite all that has been written about Android, as well as its unquestionable early success, the world at large still doesn’t fully appreciate the raw power of this juggernaut. I have written about this in the past in Android or iPhone? Wrong Question, and Google Redefines Disruption: The “Less Than Free” Business Model. But even so, the more I see, the more I wonder if I too may have underestimated the unprecedented market disruption that is Android.  One of Warren Buffet’s most famous quotes is that “In business, I look for economic castles protected by unbreachable ‘moats’.” An “economic castle” is a great business, and the “unbreachable moat” is the strategy or market dynamic that heightens the barriers-to-entry and makes it difficult or ideally impossible to compete with, or gain access to, the economic castle. Here is a great post from the 37signals blog a few years back that walks through several different examples of potential moats.  For Google, the economic castle is clearly the search business, augmented by its amazing AdWords monetization framework. Because of its clear network effect, and amazing price optimization (though the customer bidding process), this machine is a monster. Also, because of its far-reaching usage both on and off of Google,AdWords has a volume advantage as well. Perhaps the most telling map with regards to the location of the castle can be found in Jonathan Rosenberg’s “Meaning of Open” blog post. In this open manifesto, Jonathan opines over and over again that open systems unquestionably result in the very best solutions for end customers. That is with one exception. “In many cases, most notably our search and ads products, opening up the code would not contribute to these goals and would actually hurt users.” As Rodney Dangerfield said in Caddyshack, “It looks good on you, though.”  AdWords is an highly respectable castle, and Google would clearly want to put a “unbreachable moat” around it. Warren himself is on record suggesting that Google’s moat is pretty good already. But where could you extend the moat? What are the potential threats to Google’s castle? Basically, any product that stands between the user and Google and has the potential to distract the choice of search destination is a threat. A great example is Firefox. Like many browsers, Firefox has a search bar built into the upper right corner. This leads to a substantial number of Google searches for which Google pays Firefox a handsome fee. From time to time, this fee must be negotiated, and as a result there are constant rumors that Firefox might chose another search engine, like Bing. Other examples include smart-phones and choices made by carriers and/or handset makers. As an example, a few years back, Verizon set the default search box on Blackberry’s to Bing instead of Google. Despite Warren’s faith in Google’s moat, there are ways to move the needle on search share, or at least hurt the economics by demanding more profit share for distribution.  So here is the kicker. Android, as well as Chrome and Chrome OS for that matter, are not “products” in the classic business sense. They have no plan to become their own “economic castles.” Rather they are very expensive and very aggressive “moats,” funded by the height and magnitude of Google’s castle. Google’s aim is defensive not offensive. They are not trying to make a profit on Android or Chrome. They want to take any layer that lives between themselves and the consumer and make it free (or even less than free). Because these layers are basically software products with no variable costs, this is a very viable defensive strategy. In essence, they are not just building a moat; Google is also scorching the earth for 250 miles around the outside of the castle to ensure no one can approach it. And best I can tell, they are doing a damn good job of it.  Google has organized this defensive play with precision. Carriers and handset makers that use Android are given economics to do so. The Android version of the “AppStore” shares the majority of its economics with the carrier and handset makers. Once again, they are not building a business, they are building a moat (sorry for the repetitiveness, it’s intentional). Because they are “giving away” money to use their product, this creates a rather substantial conundrum for someone trying to extract economic rent for a competitive product in the same market.  This is the part that amazes me the most. I don’t know if a large organized industry has ever faced this fierce a form of competition – someone who is not trying to “win” in the classic sense. They want market share, but they don’t need economics. Imagine if Ford were faced with GM paying people to take Chevrolets? How many would they be able to sell? What if you received $0.10 for every free Pepsi you consumed? Would you still pay $1.50 for a Coke?  The combined market capitalizations of companies that build desktop operating systems, handset operating systems, mapping software (they give this away with Android also!), as well as internal software that helps to differentiate mobile devices is well over $100B, and may be several times that. Yet, there is no economic law that necessitates that these industries remain in their current form. When software was first imagined as a business, it seemed like a miraculous dream. Because the variable costs were zero, you would make near 100% profit on each incremental unit that you sold. Perhaps the resulting counter-force to this is that if someone can afford to build a near equivalent code base, than they can at their option price to marginal cost ($0.00), the very definition of perfect competition.  One might yearn to suggest that there is a market unjust here that should be investigated by some government entity, but let us not forget that the consumer is not harmed here – in fact far from it. The consumer is getting great software at the cheapest price possible. Free. The consumer might be harmed if this activity were prevented. And as we just suggested above, the market is finally driving towards software pricing that represents “perfect competition.”  In Silicon Valley we like to make light of industries that are facing digital disruption such as newspapers, the record industry, and the movie industry, suggesting that their executives “just don’t get it.” Perhaps now we are witnessing the disruption of not just analog businesses, but also formerly interesting digital businesses as well.  John Doerr, once said “The Internet is the greatest legal creation of wealth in history.” Android may be the opposite of that, the greatest legal destruction of wealth in history.
If you could travel back in time to the early 1990’s and ask Silicon Valley’s top entrepreneurs and private company executives about their long-term career ambitions, you would hear a constant theme – they all wanted to be part of an Initial Public Offering (IPO). Back then, taking a company public, either as a CEO, CFO, or founder, held an allure similar to that of a young athlete dreaming of making it in the major leagues. Clearly, not everyone was able to go public, but that of course added to appeal. Everyone still wanted to go public. They all dreamed of playing on the business world’s biggest business stage.  A great deal has changed since then. First, we lived through the peculiar time now known as the Dot-com bubble, where the elite requirements for going public were greatly reduced. This was followed by a period of heavy regulation where many aspiring startups felt as if they were absorbing the burden of sins committed by the likes of Enron and WorldCom, two companies that are far away from Silicon Valley. If you believe what you read, we now live in a world where young entrepreneurs have a more cynical view of the IPO and being public in general. It is common today to read a phrase like “You don’t have to go public early to provide liquidity to early investors or employees.” It is critical to consider just how far away “don’t have to” is from “want to” or “dream of”.  How Did It Get This Bad?  There are many potential causes of this widespread pessimism. First and foremost, going public and being public are not nearly as much fun as they once were. The combination of a rise of ambulance-chaser shareholder lawsuits, Sarbanes-Oxley, the requirement for CEO and CFO signatures on financial filings, and limited personal trading flexibility has unquestionably made being public less enjoyable for executives. Increased bureaucracy and red-tape almost never lead to increased enthusiasm.  We may also have a perturbed notion of what a “healthy” IPO market looks like. For many, the go-go days of the late 1990’s stick in their mind as the definition of a strong IPO market. Unfortunately, the IPO market of 1999 was a myth, a façade, a once-in-a-lifetime mirage that you will never see again. While that period was economically fruitful, it was clearly manic and a long, long way from being healthy. Moreover, it was completely and utterly unsustainable. It also may have “cheapened” our view of the IPO. If anyone and everyone can go, it is no longer a heroic accomplishment.  One recent argument knocking the IPO is as follows: Wall Street is too short-term focused, and that if you want to run your company for the long-term you should remain private. There are three great reasons that this “can’t focus on the long term” argument falls short — Jeff Bezos, Marc Benioff, and Reed Hastings. All three of these amazing entrepreneurs turned CEOs took their company public on a standard IPO time frame. They also all three conveyed to Wall Street that they would postpone short-term earnings results in order to chase a greater long-term objectives and ambitions. The intelligent mutual fund investors that were swayed by their convincing arguments (there were many) were handsomely rewarded. Furthermore, Bezos, Benioff, and Hastings all three used “being public” as a bully-pulpit to tell their version of their industry’s story, thereby aiding their advantage. If you are unconvinced go ask Steve Riggio, Tom Siebel, or Blockbuster CEO Jim Keyes.  Certainly one contributor to the negativity surrounding the Silicon Valley view of the IPO market is the negative perception of the local press echoing off the hillsides of the Santa Cruz mountain peaks. Over the past several years, it has become quite common to read Silicon Valley articles and blog-post offering near-eulogies of the high-tech IPO. TechCrunch refers not to simply the “IPO” but to the “dreaded IPO,” or the “Poor, Pilloried, Tech IPO.” Famed early stage investor and typically glass-half-full blogger Fred Wilson recently penned “IPOs Just Aren’t What They Used To Be.” The San Francisco Chronicle stated that the “market for initial public offerings remains badly broken,” and the ecosystem “..has been destroyed.” And despite the numerous successful IPOs in the last two years that have supposedly put an “end to the IPO drought,” the only thing that doesn’t seem to go away is the use of the phrase “IPO drought.” If that were not enough, the NVCA (National Venture Capital Association) argues the situation is so dire that we need a Four Pillar Plan To Restore Liquidity. The pessimism is consistent and deafening. The glass isn’t simply half-empty; everyone seems to think there is a hole in the bottom of it.  How Bad Is It Really?  A more optimistic eye can see that the IPO data is actually improving. This quote from the NVCA’s second quarter update is rather straightforward:  Venture-backed company exit activity showed continued momentum during the second quarter of 2010, with the best quarterly total for venture-backed Initial Public Offerings (IPOs) since the fourth quarter of 2007, according to the Exit Poll report by Thomson Reuters and the National Venture Capital Association (NVCA). The quarter ended with 17 venture-backed IPOs, marking the third consecutive quarter for increased offerings, by number and by dollar amount.    Looking at the Q3-2010 NVCA data included above, you can see that 2010 is markedly improved over 2009. We have already tripled all of last year in the first three quarters of this year. Moreover, with a healthy Q4, we could meet or beat the annual numbers from 2005 and 2006. If you limit the data to VC backed companies in the U.S. in high technology (leaving out Pharma and bio med; which is different from the above), there were five in 2008, twelve in 2009, and 25 year-to-date in 2010. These data points are clearly up and to the right. And while they may not hit the bar we are looking from for a cyclical market high point, it surely makes it hard to say the IPO market is fatally flawed. And it unquestionably not “closed.”  The Majority of Recent IPOs Are Outside of Silicon Valley  U.S. High Technology / VC Backed IPOs Since the Beginning of 2008 (XLS)  The Excel spreadsheet embedded above contains a detailed look at all of the high-tech VC backed U.S. IPOs since the beginning of 2008. There is some very surprising data in this table. First, these IPOs have performed relatively well since their initial offering. On average, these IPOs have averaged 55.9% in price appreciation since their IPO date. This represents almost $14B of post IPO value creation as a group. Moreover, 19 of the 42 companies are worth over $1B. A full nineteen VC-backed companies with recent IPOs are now worth over $1b!  The press that keeps yearning for the next “big” IPO in Silicon Valley and complaining about the health of the IPO market, doesn’t spend much time talking about RackSpace ($3.3B market cap), RealPage ($1.8B market cap), GreenDot ($2.2B), or Ancestry ($1.1B) – all recent IPOs that have traded up quite nicely since they went public. Maybe there is a reason for this.  Is there any chance that the negative IPO sentiment that is reverberating through Silicon Valley is actually having an impact on the local IPO volume? One might expect, that as the epicenter of innovation, Silicon Valley would warrant more than its fair share of IPOs. But the data shows the exact opposite (see table below). In this same spreadsheet of recent IPOs, we have highlighted whether each company has its headquarters here in Silicon Valley or elsewhere in the United States. The shocking reality is that only 11 of the 42 high-tech, venture backed IPOs since 2008 reside in Silicon Valley. In other words, 74% of these IPOs hail from outside of the SV echo chamber.  If you look at the data in terms of initial IPO value, 78% of the overall value is from outside SV. In terms of value today its 73.5% (SV IPOS have outperformed those outside SV).  Perhaps these out-of-market IPOs aren’t well covered within Silicon Valley, and perhaps the negative IPO sentiment isn’t well heard outside of it. Our pessimism may have led to a self-fulfilling prophecy.    Demand or Supply Problem?  There is an interesting commentary at the end of the San Francisco Chronicle article that we previously discussed. “Brent Gledhill, with William Blair & Co., a small investment bank in Chicago, said he has buyers for small IPOs, but can’t get sellers.”  This argument, which was also supported by Paul Deninger of Jefferies, suggests that we have a “supply” problem, not a demand problem. He has BUYERS but not SELLERS. The problem is not that Wall Street doesn’t want product, it is the opposite; that we are not offering them enough of it. While it is clearly a chicken-egg argument, you simply cannot have a healthy IPO market if the leading high-quality companies are unwilling to file. The problem may be attitudinal, not structural.  To this point, and perhaps ironically to some, most of the people I know that work in high tech mutual funds and hedge funds would like to see more IPOs not less. They are tired of trading the same large technology names that are showing limited equity returns over the past 10 years, and have very low growth opportunities/ambitions. If you look at the the forward revenue growth estimates for technology bellwether stocks you may be surprised: Intel (3.5%), HP (5.6%), Microsoft (6.8%), Cisco (11.1%),Ebay (11.4%), and Yahoo (3.3%). And many of these stocks are flat to down for the past decade! Even Google, the youngest of the large cap tech plays has a go forward growth estimate of below 20%. As you can imagine, these traditional “must have” technology names are not contributing to mutual fund outperformance the way they once did. Fund managers desperately need more exposure to growth. They also crave exposure to new trends like social networking and mobile computing, but with limited IPOs they have limited ways to invest in these new innovative trends. They simply need more “quality” product.  Valuations Are “Higher” in the Public Market  As a result of this scarcity of growth across the broader set of public companies, strong category leaders like OpenTable, GreenDot, Realpage and Ancestry.com are seeing healthy valuations in the public market. These high growth Internet leaders trade at PE multiples (30-50x) that are roughly twice that of Internet leaders Microsoft, Yahoo, Ebay, and even Google. The IPO market is currently paying a substantial premium over the M&A market (the exact opposite of what you read). The same large companies that are struggling to find growth have reduced valuation multiples (P/E, P/S). This in turn makes it hard for them to pay strategically high prices in an acquisition. Therefore, entrepreneurs that follow the advice from the San Francisco Chronicle, and are “looking to be acquired” may be leaving ample money on the table.  As an example, drill down on RealPage, a Dallas based leader in SAAS solutions for property management companies. It is currently trading at $1.86B after a successful August IPO. They currently trade at about 8.7X annualized Q2 revenue. Which potential acquirer would pay this valuation for a private vertical industry specific SAAS play? Do you think Salesforce, who has never done a large acquisition, would? Do you think Oracle (who trades at 3.5x sales would)? What about SAP (3.8X sales)? IBM (1.7x sales)? Or consider GreenDot which went public in July and currently trades at a $2.2B market capitalization. This valuation equates to roughly 6 times 2010 revenues. Do you think American Express (currently trades at 2X revenues) would have offered that in a private transaction?  What about Ancestry.com?  This recent Internet IPO is currently trading at a market capitalization of $1.17B. Which large Internet company would have paid close to $1B for Ancestry? None is my answer.  We should also consider DataDomain, 3Par, and Arcsight, all companies with remarkable sell-side M&A transactions who went public BEFORE engaging in an M&A transaction. Being public is a wonderful way to establish a baseline valuation in an eventual corporate sale. There is no chance someone would make an offer at or below the current market price, as the expectation is to pay a market premium. And because the BOD has a very high duty in terms of maximizing shareholder value, these deals are often seen by multiple bidders and therefore more likely to be competitive than a private transaction. Lastly, and not to be ignored, public company sales have zero escrow provisions. These escrows typically put at risk 10-15% of the transaction value when a private company is acquired. Being public before you get acquired can be extremely valuable.  Your Company Is Not Facebook   A large contributor to the negative IPO press is Facebook’s definitive view that it prefers to postpone its IPO well into the future. Recent comments suggest an IPO may be put off until 2012. As a top three worldwide Internet site, the press is obviously interested in what Facebook wants to do. Also, because of its huge impact, and the emerging trend of social networking, the buy-side is quite interested in owning Facebook. The demand for an IPO, were one to happen, would be enormous. And that is probably an understatement. However, it is critical to put this in perspective relative to everyone else.  Facebook is the exception not the rule. They can do what they want when they want. They can raise money privately at any time if they feel the need to do a cash acquisition. There are literally firms willing to wait in line to give them money. They can hold a press events and everyone comes, so they certainly do not need to be public to broadcast their message. However, they are also a miserable proxy for the average private company CEO and BOD to consider. Your company is not like Facebook, and it should not build its IPO plans based on what Facebook does or does not do.  Things Are Looking Up  This entire problem may be self-correcting. The BOD and executives from the companies that have not gone public have certainly noticed the successful offerings, post-market performance, and valuations of the IPOs mentioned herein. As such many of these executives are now marshalling the forces for their own IPO. As an example, Betfair, a long awaited IPO in the UK (congrats @jdh) just went public and had strong results. Skype and ZipCar have filed, and all indications are that LinkedIn is working on its own filing. There is also a good chance that companies like AutoTrader and eHarmony will come public soon, and there have been multiple rumors of IPOs at companies such as Hulu and Pandora.  In addition, the very recent press seems to also be singing a different tune than the dire press from this summer.  Check out the following headlines from the last few weeks:  11/15 Analysts See Pickup In IPO Market In 2011 11/15 IPO Market Rising from the Financial Crisis Grave 11/15 Momentum in US IPO market continues to build 11/15 US Options Exchanges Watchful On Signs Of IPO Rebound 10/28 IPO market springs to life 10/26 Quietly, IPO market is staging a rally 10/24 Can the Top 12 IPOs of 2010 Go Any Higher? 10/1 IPO, M&A Boosting Venture Capital Fortunes I recently had the opportunity to hear the story of how Tim Sullivan, the former leader of Match.com, went into Ancestry.com five years ago as CEO.  At the time, the company’s growth had slowed and many had assumed it had seen its better days. Tim and his team and began a multi-year turn-around that would eventually lead to last year’s respectable IPO. Last week, the company completed a successful secondary offering. Tim shared with me all of the amazing work that went into reigniting this market leader (a very impressive story), but I was most surprised when he talked about the IPO process. His face broke out into this huge grin as he described watching the stock trade that first day. You could clearly see the type of IPO enthusiasm that once reigned supreme in Silicon Valley. For Tim, the dream was still alive, and more importantly he was able to turn his dream into reality.  Waves of pessimistic analysis can become self-reinforcing and began to influence rather than just inform. That appears to be the case with respect to local attitudes towards high-tech IPOs. Next time you hear someone talking about how broken the IPO market is, please let him or her know that despite what you read, many great companies are going public and are having remarkable success. And if they still doubt you, tell them reach out to Steven Streit at GreenDot, Zorik Gordon at ReachLocal, Doug Valenti at QuinStreet, or any of the 38 other CEOs who recently stood up and walked through the door that everyone else says isn’t open. Their story should be at least as compelling as focusing on the few companies that don’t seem all that interested.
Last night, Google reported financial results for the second quarter of 2010. While revenue growth was up 24% year over year, revenue was fairly flat compared with Q1 of 2010. Moreover, earnings fell short of average street estimates sending Google down $20 per share (4%) in the aftermarket. Based on current estimates (which might change tomorrow), Google currently trades at 18 times the street average for 2010 earnings, and 15.5 times the same number for 2011. These represent price/revenue multiples of 7.5 and 6.5 for 2010 and 2011 respectively. For a long-term tech investor, these valuation multiples seem surprisingly low for a proven market leader. What gives?  Over the past 30-40 years of tech investing, public investors have come to expect the market leaders in each sector to trade at valuation premiums. On average, these market leaders have had respectful PE multiples of over 30X forward 12-month earnings, and price/revenue multiples in the 6-10X range. As examples, Microsoft and Cisco held 30X+ PEs for many, many years. With this as a backdrop, many are surprised that Google, which is relatively young at only 12 years old, is burdened with a PE multiple that is typically associated with the senior-citizens of tech leadership. Google’s forward current PE multiple isn’t remarkably higher than Microsoft (12.75) or Cisco (15).  While there is no exact science for what leads to higher PEs, there are many theories. Some argue that PE’s relate to growth. That said, Google’s growth is much higher than Microsoft and Cisco, and yet it doesn’t have that much higher a multiple. My good friend, Mike Mauboussin of Legg Mason, suggests that higher PEs reflect more concrete competitive advantages.  I have always been a huge fan of this theory he calls CAP (which stands for Competitive Advantage Period). It also dovetails nicely with Warren Buffet’s competitive moat theory. However, when I look at Google, I see a company that is well positioned strategically. All attempts at dislodging their leadership have been unsuccessful to date. As such, I don’t think they suffer in this area. One other area typically cited is the more amorphous category of “execution.”  I also have a hard time seeing this as the culprit, as Google’s recent execution on Android is pure genius. Moreover, the Google Apps progress is extremely impressive.  So if its not growth, competitive position, or execution, what is the shortcoming that hurts Google’s valuation? Believe it or not, the problem is that their initial business model was “too good.” Before I explain take a look at the included chart. Microsoft was founded in 1975, it went public in 1986, reached $10B in sales in 1997, and fell below 20% growth in year 2000. Google was founded in 1998, went public in 2004, hit $10B in sales in 2006, and fell below 20% growth in 2009. So it took Microsoft 22 years to hit $10B in sales. Google did it in 8 years. Resultantly, Microsoft had growth of greater than 20% for 26 years; Google for only 11.   I would argue the reason for the noted disparity is pricing optimization and pricing power. When Microsoft first established DOS as a market standard, they reaped about $10/PC in royalties. By the heyday of Windows NT, Microsoft was receiving well over $120/PC in the enterprise. Additionally, they layered in Microsoft Office on top of the OS, which took revenue/PC well north of $200. That’s a 20X increase from where they started. Google’s brilliant bid/market based ad product optimized pricing remarkably quickly. As such, Google reached $10B in revenue in about 3X more quickly than Microsoft. Unfortunately, this coin has two sides.  With its ad optimization engine so amazingly efficient, Google has no obvious pricing power against its current installed base. There is simply no way to “double” the amount of spend from each customer, much less a way to take it up 20X. Additionally, they have not yet identified a product that would represent the Google’s version of Microsoft Office in terms of revenue leverage. The Enterprise Apps offering is phenomenal, but the numbers are simply too small relative to search. More importantly, the Entprise App product does not “sit right on top of” the search franchise (as Office was to Windows), limiting the ability to leverage the success of one product into the other.  Of course, there is little reason to weep for Google. As mentioned earlier, they appear to be rocking in many areas. Google mail and calendar are now used by over 2 million unique organizations. Also, the execution of the Andoid team will be talked about for decades to come. And even though they may be limited in their price leverage on core search advertising, they still have that blank wide open home page, that I suspect is at least $1 billion in forgone annual ad revenue.
“It’s funny how fallin feels like flyin, for a little while…”  - Jeff Bridges, Crazy Heart Soundtrack   On July 1st, Google announced its intention to acquire ITA Software. ITA owns a primarily B2B airfare search and pricing system called QPX. Several of the leading online travel sites, like Orbitz, Kayak, and Bing Travel, use information from QPX to power their airfare search. Many in the industry view this move as a seminal event in Google’s history, as the company makes a decisive step from being a general search engine, into more structured vertical search. Certainly, Google already offers vertical search in Images, Videos, Maps, News, and several other categories. Despite that, ITA feels different. Perhaps the difference is that this is a step into a vertical where many independent incumbents, like Priceline and Expedia, who are material customers of Google, have large established businesses.  There are two reasons mentioned for why Google feels compelled to dive deeper into verticals. The most straightforward explanation is competitive pressure. Following its own acquisition of Farecast, Microsoft has subsequently launched Bing Travel, a much richer travel search product than offered on Google today. The second argument given for the move is that by moving deeper into verticals, and closer to the actual transaction that Google can actual make more money per visit. This argument suggests that CPA (cost-per-action) is a fundamental improvement over Google’s current business model, CPC (cost-per-click). The competition argument seems obvious and accurate. However, it is not at all clear that going deeper in verticals will raise Google’s revenues. In fact, there are several scenarios where they could actually go down.  Let’s first address the easy part – competition. Bing buys Farecast and Google needs to respond. This make sense, but it is not the whole story. If you are searching for a book or an author you go to Amazon, or at the very least you do a search like “Man in Full Amazon” so that you go directly to the page you want on Amazon. The same is true for hotels with TripAdvisor and for restaurants with OpenTable. These sites offer deeper and richer experiences for a vertical searcher precisely because they incorporate deep meta-data, faceted search, transaction connectivity, and typically a form of community or UGC (user generated content). These things simply do not exist in the simple but limited Google user interface that Om Malik affectionately refers to as “10 blue links.”  So Google has competition in verticals not just from Microsoft, but also from best of breed vertical sites offering users a richer, deeper experience.  Some have suggested that Google’s move into a deeper vertical experience is more about greed (more money) than fear (competitive) response. The argument voiced by Barclay’s analyst Douglas Anuth and others, is that by moving closer to the transaction, Google can ask for CPA fees, which naturally carry higher margin. Clearly, a single CPA fee will be much higher than a single CPC fee, but you will also have much fewer of them. The variable that links the two is conversion. One can certainly argue that Google can drive higher conversion if they can help drive the customer closer to the actual result they need. This will require a materially better product. Even then, however, there are two key reasons Google may not see higher revenue with deeper vertical integration.  REASON #1: IRRATIONAL CPC PRICING ON GOOGLE TODAY, “UBER-OPTIMIZATION”  Some might argue that Google’s current bid-based CPC model results in “optimal” pricing. The argument would be that the market-clearing price settles out at the precisely rational price for each and every keyword pair. A market-place model naturally results in efficient pricing. While this makes logical sense, we all know that there are companies participating in the CPC purchasing game who simply are less sophisticated than others. Moreover, many of these buyers win bids and have huge CPC budgets. The point is that there are plenty of startups, arbitragists, and even large companies “experimenting” with CPC purchasing in an attempt to gain an edge. The winner in this bidding game isn’t the most rational, but simply the one with the highest price. Certainly, over the long run if a company irrationally pays too much for CPC ads, they will eventually go out of business. But the key word here is eventually. For a long period of time, they are paying an “uber-optimized” price for their keywords.  If there are enough of these players in the market, then Google’s CPC prices aren’t economically rational. Rather, they live slightly above that level driven by irrationality and experimentation in the market. If you have a hard time with this theory try typing a search term like “laser treatments” in Google and look through the list of CPC purchasers on the right side of the screen. Would you put money in these companies?  Do you have confidence they will be around in ten years?  Have you ever heard of them? It gets even better. Many believe that Google uses a low quality rating score on these “middle-men” to force them to pay a higher fee for a single CPC, thus getting an even higher price than previously discussed. That’s right, for certain CPC buyers, Google has a mechanism for extracting an even higher price, even if the buyer is already the high bidder!  As Google moves past its “10 blue links” model and connects directly to airlines, hoteliers, etc, it will be removing the irrational and arguably temporary middle-man from the system. This fleeting but determined participant, very likely has a negative long term ROIC, and Google, riding the brilliance of the CPC model, stands as the beneficiary. With this player out of the system, and with the connections directly to the service provider, the model will naturally trend to a more efficient pricing.  You will have fewer larger players, who are all more rational, and all more experienced. As such, you would have to expect more rational, and therefore lower, pricing. Building a better product could actually result in less lead-generation revenue.  REASON #2:  MOVING FROM A MARKETING CHANNEL TO A TRANSACTION CHANNEL  If you have ever sold anything on the Internet, ask yourself the following question. What is the maximum amount you would want to pay for a transaction fee?  5%?  10%? There is data in looking at typical affiliate fee percentages, which can range from say 4-15%. Amazon charges 6-15%. Ebay charges about 11% (with Paypal).  Comparison shopping engines make even less. When an etailer assumes they are “always going to pay” for something on every single transaction, they are very sensitive to the % of revenues, as this payment will always reduce their margin. One could assume the general average for all affiliate fees or similar distribution type arrangements is around 10%.  Now, ask someone in your marketing department how much they are willing to pay to “acquire a customer.” While I don’t pretend to support this logic, the Lifetime Value of the Customer (LTV) model depicted herein mesmerizes many marketing managers. Using this simplistic but highly regarded model, many marketers justify “acquiring a customer” not as a percentage of revenue, but as a percentage of life-time value. The key to reaching this Zen state of marketing awareness is to believe that Google is sending you this customer only this one time, but for here ever after this customer is going to come directly to your own site, bypassing Google. This logic supports a much larger denominator, known as LTV. With LTV, ad buyers are easily willing to spend 25-50% of a first purchase in order to “acquire a customer.”  We could talk forever about the LTV formula, and we could argue back and forth about its efficacy, but that would miss the point. Consider the following assertion. People that are buying CPC ads are frequently marketers, and marketers are much more likely to think in terms of LTV. When you enter into a CPA deal it feels very transactional. When you do deep integration it feels very transactional. And, if Google is building a deep vertical site in travel that will pass leads to companies like American Airlines it will feel very transactional. It will be harder and harder to assume that you are “acquiring a customer,” and it will feel more and more like you are paying a distribution fee to a channel. As such, it may turn out that moving deeper into a vertical will puncture the illusion that marketers are “acquiring a customer” from Google, and get them in touch with the fact that they have a permanent CPA “transaction fee” they need to “pay” to Google. The end result is a lower overall rake for Google with the per transaction model.  Could this be wrong? Absolutely. Perhaps conversion rates will triple due to the incredible design, implementation, and ease of use of the Google’s new product, more than offsetting the two points we just mentioned. Or perhaps, Google will have such a powerful place in the travel ecosystem that travel companies will simply be “price-takers.” If this is the case then Google will once again find the exact right way to optimize their business model. It is equally likely, however, that Google’s current business model is highly, highly optimized and tweaking it may have as much risk as upside.
More often than not, we here in Silicon Valley are prone to idealism. We see a scenario the way we want to see it, and make predictions that fit our view of how we think the world should work, or perhaps even how we would like the world to be. This is especially true when it comes to technology. Outsider “luddites” who do not immediately grok the remarkable disruptive power of our latest and greatest technologies are doomed to the business trash heap – driven there by obsolescence and an obstinate refusal to accept their fate. Often times, our version of them “accepting their fate” would require them to abandon everything they know, walk away from the majority of their revenue, and terminate 80% of their employees. But hey, that’s their problem, not ours. We love disruption. It serves our purpose.  One often discussed target of such criticism is the media industry. There is a widespread belief that Hollywood now faces the same digital threat that has plagued the music industry over the past ten years. The argument goes something like this: There is nothing Hollywood can do to stop this train. The problem, you see, is that technology is merciless, impersonal, and unforgiving. Video can be turned into bits; Moore’s Law will make a pile of bits smaller and smaller over time; and efforts to erect pay walls will prove fruitless and even Quixotic. Studio heads should simply throw in the towel now and take what’s coming to them. Denial equals delay, and delay costs you time away from learning how to execute within your new constraints. All content will be free, and you simply have to live with that fact. The sooner you get in touch with it the sooner you will learn to execute in the new reality.  There are three key reasons why Hollywood is under less duress than Silicon Valley wants to believe. For starters, the leaders are wide-awake. Ever since Boxee offered Hulu (and were told to stop), the executive ranks at the major cable companies have been alert and engaged. Second, Hollywood has a solid track record of enforcement. They understand the stakes are high, and they are willing to invest in lobbying, regulation, litigation, and enforcement. They are also unafraid to throw around their weight (witness Viacom vs. Google). The final and most significant reason is that this is a massive, massive business, and it is critically important to understand where the money flows (most people don’t). You can spend plenty of time talking about other issues, but when it comes to understanding the key factor at play in nearly every major business decision in television, you will find affiliate fees – all $32 billion of them.  For those who do not know, affiliate fees are the primary revenue stream that funds today’s mainstream television content development. These are basically a “share” of the subscription fee you pay to your cable or satellite operator that is then shared back to the content owner/distributor (typically on a per subscriber basis). As an example, you will hear that some less notable cable-only channel was able to negotiate $0.25/sub/month, or that ESPN can negotiate $2.00/sub/month, because any aggregator would be afraid to market a television package without ESPN. Over the past 30 years, these fees have become the lifeblood of the TV content business – affecting how the major aggregators think and operate, and also affecting how content is produced, financed, and packaged.  Here are some specifics to help frame the issue. According to Matthew Harrigan at Wunderlich Securites, in 2009 DirecTV paid approximately $37/sub out of an ARPU of $85/sub to content owners for programming costs (i.e. affiliate fees). In this case, affiliate fees represent roughly 43% of total revenue for DirecTV. Similarly for Comcast, Matthew estimates programming costs at 37% of video revenue (Comcast has high-speed data and voice revenue that are separate). These are just two examples, but to give you a sense of scale these numbers represent around $7-8 billion/year each for Comcast and DirecTV. The recent, and very well written Business Week cover story on this same topic pegs the aggregate fees of all content providers at $32B per year. These are big, big numbers. To put things in perspective this is about 33% higher than Google’s annual global revenues including revenues for its advertising network.  These affiliate dollars flow through to the content producers. Estimates suggest that the annual affiliate fee revenue at companies like Viacom and Disney is around $1.5B and $2.0B respectively. On their own, numbers this large would obviously be motivational to corporate executives. But the reaction is even more intense because affiliate fees “feel like” 100% gross margin revenue. From a cost accounting perspective, a studio should allocate these fees across the content development costs, and therefore, they are not explicitly 100% GM. But as there are no significant variable costs related to the deployment of these programs to the carrier, most content owners cannot help but think about affiliate fees as 100% gross margin and therefore the key contributor to overall profitability.  Affiliate fee optimization is the key objective behind many of the industry’s most high profile strategic moves.  Here are a few examples.  Cablevision vs WABC. Recently, there was a high profile stand-off between WABC in New York City and Cablevision. As is often the case, the content owner here was threatening to cut-off access to their content precisely before a very high profile and high demand piece of content was set to air. This particular piece of content was the Oscars. A cable channel owner holds up a cable company to extract a higher per-sub affiliate fee for the next contract. They always put the customer in the middle, and both sides try to argue that they are virtuous and that the other is greedy. There have been numerous examples like this over the years, and it is common to see one of these showdowns each and every year. Modern Day Cable Channel Strategy. Today’s most typical cable strategy is built entirely around profit maximization utilizing affiliate fees. If you own a cable channel, your goal is to develop one or two key, hit programs, and fill the rest of the linear lineup with very inexpensive content. The “hits” make you a “must have” for any cable or satellite carrier – granting you the right to ask for fees. Too many hits drive up costs. This is why you will see more and more hit shows on the less well-known cable channels. Mad Men on AMC is a perfect example. How can a cable company not offer Mad Men? Once you nail the single channel game, you immediately try to proliferate that into multiple channels a la MTV and ESPN. Comcast Acquires NBC. Why would a cable distribution network want to own content?  First, it’s a hedge against rising content costs (affiliate fees).  Second, it offers leverage vis-à-vis their competition.  DirecTV needs NBC.  DirecTV will have to negotiate affiliate fees for NBC with Comcast (Comcast also owns other channels like E! Entertainment, The Golf Channel and Versus).  This helps keep Comcast’s business model in check.  It’s also why Comcast made a huge play for Disney in 2004. Affiliate fees have been rising for some time. Networks Ask for Fees. For the longest time, the major networks were not part of the affiliate fee gravy train. In fact, due to “must carry” laws, most networks never considered intentionally restricting their own distribution. They were simply pleased to get redistributed over cable and satellite. As these fees have grown in size and importance, the networks have changed their position and have come to the table asking for affiliate fees also. The WABC case above is one such example. Oprah Asks for Fees. Many people seem confused by Oprah’s decision to abandon her network television show after 25+ years of unquestionable success and relaunch it within her own cable network. Why would she do such a thing? Because she can. When Oprah launches her own network (with the help of Discovery), she will get per sub affiliate fees. Which cable company is not going to carry Oprah?  What programs will be on during the other 23 hours? As stated in #2, it really doesn’t matter. They still need to carry the Oprah channel. That said, Oprah has proven she can launch other personalities (Dr. Phil), and one would suspect that any new celebrity she “launches” will be tied to the Oprah network, increasing her leverage and her affiliate fees. Sports Networks Ask for Fees. Affiliate fees are driving an endless supply of channels for anyone that has “must see” content. The NFL has a channel, and had some high profile disagreements with the carriers over the “need” for its affiliate fee. You also see an NBA channel, an MLB channel, and pro wrestling is vying for one as well. If you own exclusive content, you might as well build a channel around it. This endless proliferation of channels will one day reach a limit, but for now it’s the game on the field. Hulu/Boxee. Many people blamed Hulu for its decision to block access on the Boxee platform. These users simply didn’t understand the power of affiliate fees. Comcast told NBC/Fox that if Hulu could distribute their content for free, then they would like to take their own affiliate fees (the newly negotiated ones in #4) to $0.00. This caused NBC/Fox to tell Hulu that maybe Boxee isn’t such a good idea. In addition to not appreciating these money flows, most of the digerati in Silicon Valley have huge misperceptions about the content owner’s preferences. They assume that content owners would like to distribute directly to consumers precisely because the Internet allows them to do so. They would no longer  be in the “death grip” of the content packager (cable and satellite companies) who take an unreasonable fee for their services. This is simply not how these content owners view the world.  Content owners absolutely prefer to be aggregated in a bundle of channels and, as a result, to receive affiliate fees. They also have little interest in “a la carte” packaging, a concept dreamed up by regulators in Washington but not desired by the heads of the content studios. Simply put, there is adequate value provided in distribution and revenue collection. To launch a direct channel (and forgo these fees), and then attempt to regain your customers one by one is a harrowing experience. Why earn your customers one by one when you can get to mass volumes, and a fixed amount of recurring revenue, through a distribution partner? If you create a new piece of camping equipment would you sell it online or try to obtain distribution through REI?  ESPN360 is a solid example of content owner’s preference for the affiliate fee driven/ distribution partner model. As the Internet became fast and pervasive, ESPN (owned by ABC/Disney) saw a clear opportunity to deliver more programming to their users and launched an online-only product called ESPN360 (recently renamed ESPN3). This on-demand, “over the top” offering is a killer product for the true sports fan, offering access to significantly more live games that was ever possible on a traditional linear cable channel. Despite the fact that ESPN has the brand, the reach, the market power, and the technology to charge users directly for this new product, they chose a different path. ESPN sought out distribution partners to bundle ESPN360 in with their standard video television packages, even though this was confusing and even baffling to most Internet users.  So against this backdrop, the cable companies have developed a remarkably shrewd strategy to simultaneously leverage their broadband infrastructure and affiliate-fee money flows. This concept, known as TV Everywhere, has two main components (once again, this move by the cable companies is extremely well articulated in the recent Business Week cover story on the same subject). First, you tell your customers that you want to provide them with a killer new service. They are already paying for all the content they receive through the linear channel stack. What if that same content could be viewed at any time “on-demand” and also through multiple devices (TV, PC, and mobile)? Sounds great so far. Who wouldn’t want this? And “everything” on a service like Comcast is more than any digital aggregator has yet even dreamed of aggregating. Ignore for a moment that this is not completely working just yet and focus on what they will “eventually” deliver. It’s also helpful to show the FCC you are being innovative, and not resting on your laurels the way a true monopolist would. Check.  Next comes the clever part.  The cable companies go to the content owners and make the following argument. With Internet-connected TVs on the horizon, you can no longer separate the Internet from the TV or the office from the living room. We pay you an affiliate fee to distribute your content to the homes we serve. We understand you have multiple distribution partners. What we don’t understand is why you would give content to some of them for free, and still expect us to pay our fees. Check-mate. This is the move that forced Hulu to a subscription model. The content owners, struggling with depressed advertising rates as a result of the global recession, quickly acquiesced to Rupert Murdoch’s assertion that maybe all their content should have a price.  Disruption disrupted.  Some have even suggested that Comcast has approached the large networks and offered an “extra” affiliate fee of around $0.50/sub to pay for over-the-top rights. Proactively increasing your own costs is a fairly unique business strategy. But this move also increases the costs for the disrupters, who are far less likely to be able to afford it.  As a result of these maneuvers, the current trend in the market is for less rather than more prime-time content to be openly available for free on the Internet.  Do you remember when South Park boldy made all episodes available for free on the Internet? Check out where things are today.  Try to watch the recent Facebook parody “You Have 0 Friends,” and you will receive the official message “DUE TO PRE-EXISTING CONTRACTUAL OBLIGATIONS, WE CANNOT STREAM THIS EPISODE UNTIL 05.08.10.” They may have wanted it to be free, until someone threatened to take their affiliate fees away. Viacom also recently removed shows like “The Daily Show” and “The Colbert report” from Hulu noting that “we could not agree on a price.” Suggesting there is a “price” at all would indicated they were discussing affiliate fees, as opposed to ad splits.  While this likely enrages the disruption enthusiasts, expect this trend to continue over the next year. More and more content owners will rip their shows “over the paid wall” as they get reacquainted with their own affection for affiliate fees. There is much speculation about Hulu’s forthcoming subscription launch with many journalists hopefully optimistic that Hulu as we know it will remain free and that all sorts of new features (TV support, iPhone support) and content (movies, back catalog) will be behind the paid wall. They may be surprised to find that “paying” may be necessary just to obtain what users see today. Affiliate fee parity may demand it.  So does this imply the end of all digital packagers? Not at all. Most clearly, NetFlix has successfully built a hybrid physical/digital strategy while maintaining its “all you can eat” model. It is also going toe-to-toe with other packagers by striking deals to lock up digital content (including TV programming). Furthermore, Hulu has executed well beyond anyone’s original expectation, and there is no reason to expect that to change as they move to a new model. One would expect them to continue to lead in terms of ease-of-use and simplicity even within a new model. Also keep in mind that Amazon has a strong VOD offering integrated into its overall purchasing experience, and many suspect both Apple and Google will enter the game as well. Despite this level of competition, all of theses vendors will need to find unique ways to compete against TV Everywhere. And with “free” off the table, the dimensions of competition will be inherently less disruptive.  There are two other potential challenges for non-facilities based content aggregators. First, as was the case with Satellite radio, we may see a “no holds barred” price war break out in an attempt to grab “exclusive” content to distinguish one’s package. As we all know, exclusive deals with the likes of Howard Stern nearly killed XM and Sirrus. DirecTV already pays $700 million per year to the NFL to have an exclusive offering of every NFL game on every weekend (NFL Sunday Ticket), and they recently coughed up over $4 billion to extend this deal. Wow. What if other digital “packagers” look for unique differentiation by leveraging the cash on their balance sheet? If this happens, any digital aggregator without deep pockets will be holding a knife at a gun fight.  The second externality that could cause trouble is “bandwidth limits” or “metered usage” on the Internet. While some people assume this will never happen (especially the idealist in Silicon Valley), the quiet momentum is building. There are continuing tests at AT&T and Time Warner, and AT&T’s president Randall Stephenson spoke openly about metered Internet pricing as recently as a month ago. Also, the Supreme Court recently put the kibosh on the FCC’s deliberate effort to make net neutrality one of its defining policies. This is perhaps an entirely separate post, but one should be confident that the rate charged the consumer by the owner of the transport for one hour of Internet video would be quite a bit higher than that for one hour of the same video over their own “optimized” TV infrastructure (backed up with an ample helping of technical analysis and white papers).  The fox isn’t just guarding the henhouse, he designed it.  There are still two legitimate arguments that trump all these discussions of affiliate fees and deft corporate strategy – piracy and content democratization.  Let’s start with piracy.  What if “BitTorrent 2.0” in whatever form it takes is just blatantly unstoppable?  No matter what you do, content has become too small relative to the big broad pipes and storage devices.  Technology trumps determination, and the minute something has been shown once, it will be free for all takers.  Isn’t this true in China today? It’s a big leap from expecting this to happen “someday” to expecting a content creator/owner to throw caution to the wind and immediately adopt a strategy that is congruent with unlimited free distribution (what is this strategy by the way?  can’t ads be removed also?). Technology is inevitably a tough competitor, but so is regulation and enforcement, and you should expect that a mighty effort on the part of a multi-billion dollar industry would mute any expectation of an overnight transformation. In her latest post at All Things Digital, Kara Swisher suggests that a recent increase in the number of intellectual property enforcement officers at the DOJ may be a direct response to the immediate needs of the entertainment industry.  Other cheerleaders of the disruption bandwagon point to the undeniable future where the availability of low-cost, high-feature camcorders at BestBuy will lead to a mass democratization of content creation. In this brave new world, the bloated and lavish infrastructure of Hollywood will give way to thousands of mini-Tarantinos who produce hit after hit on shockingly low new-world budgets that redefine the content creation business. This is the video equivalent of the infinite monkey theorem.  While this may be true when it comes to low-budget formats like game shows, talk shows, and reality television, today’s fussy television viewer has come to expect a product that is much more equivalent to feature films than home movies. Each episode of Lost costs well over $1mm to produce. Cheap cameras do not disrupt “production quality”. And let’s not forget that The Blair Witch Project was over ten years ago, and desperately stands alone as an exception and not a rule.  In the long run, the disruption zealots may be right. It may all come undone in the unstoppable Armageddon of unlimited “all you can eat” content enabled by the undeniable liberation of all bits big and small. But with $32 billion on the line, don’t expect it to happen overnight. You will be sorely disappointed.
