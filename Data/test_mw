Yesterday's announcement of the Apple iPad -- basically an iPhone with a larger screen and no camera -- ushered in a new era for OS design. That is, an OS that was originally designed for a phone (the "iPhone OS", which is itself a stripped down version of Mac OS X), is now being ported over to larger, much more capable devices. I have no doubt that the iPhone OS will be very successful on the iPad. It would not surprise me to see it running on laptops and desktops in the near future. This simplified OS eliminates most of the complexity that makes people hate computers: the quagmire of configuration options, keeping up with upgrades, and of course the constant battle against viruses and malware. When I first saw the iPad, I immediately recognized that this is going to be the perfect computer for "non-computer users", like my dear mother-in-law, who pretty much only uses her Windows PC to read email and surf the web. (I'm not sure she's ever saved a file to the hard drive.) But it's also going to be the perfect computer for those of us who just want something that works. Like the original Macintosh, the iPad raises the bar of user-centric computer system design. It is elegant in its minimalism and simplicity.  Still, this trend of dumbing down the OS raises some interesting questions. The iPad OS lacks many features that operating systems have had for decades: multitasking, an open application API and development tools, multiple protection domains -- heck, it doesn't even have a proper filesystem. Arguably, it is this lack of features that makes the iPhone and iPad platforms so attractive to application developers and users alike. This trend suggests very strongly that the feature-rich, complex OSs that we love so much are going to look too baroque, top-heavy, and expensive to survive in a field where the "OS" is little more than GUI gloss over a pretty basic system. (Something tells me that in a few years we're going to get iPad OS v2.0 which builds in some of these "advanced" features that date back to the 1960's.)  Basically, what I'm saying is that the iPad would appear to make most current OS research irrelevant. Discuss.
I may be jumping the gun a bit here, but with this WSJ article on the potential pricing models for e-books on the Apple Tablet, I am very worried about the future of e-books. Books used to be these paper things that you could get just about anywhere, not require a specialized device to read, lend to your friends, borrow from a library, scribble in, or use to prop a door open. It seems clear that paper books are about to go the same route as the compact disc, but the e-book industry is getting it all wrong by tying themselves to proprietary, closed formats that only work on certain devices from certain vendors. It seems like I hear about a new e-book reader every day on sites like Engadget, but every vendor has the same problem: getting content and agreements with the publishers to support their device. Amazon has done a great job establishing those relationships for the Kindle, and now it seems Apple is going to have to do the same legwork for their tablet. It seems inevitable that we're going to end up with a VHS-versus-Betamax style format war between the two platforms. This does not bode well for the e-book industry.  Let me make it clear that I love e-books. I've been doing a ton of reading on the Kindle App for the iPhone, having recently read Dennis Johnson's Tree of Smoke and Cormac McCarthy's The Road (and a few other not-so-great books that don't need my endorsement here) exclusively on the iPhone. (Yes, the tiny screen works - I can actually read faster with a narrower horizontal view of the text.) I particularly like having a half-dozen or so full-length books in my pocket at all times: if I weren't so busy reviewing papers for program committees I'd probably do more "fun" reading this way. But I really don't know if the Kindle e-books have any staying power. I certainly won't be able to hand them down to my son when he's old enough to read them. And will I have to buy them again, in a different format, for a different device, ten (or five or two) years from now when the next hot e-reader device comes on the market? What a pain.  Publishers should have learned their lesson from the music industry. Anybody remember the early digital Walkman that required you to encode your music in a DRM format and "check out" music from your library on the PC to the portable device? It was a total failure. Although Apple's AAC format is closed, iPods are quite happy to play MP3s, and arguably MP3 won the format war. I know full well that MP3 is far from the "best" format from a technical perspective, but pretty much every device I own knows how to play it, and it seems likely that it will stick around for a while. Better yet there are plenty of open source implementations that can encode and decode the format, so my music library is not locked down.  My final worry is what closed e-book format mean for accessibility of books and (to risk hyperbole) the wealth of human knowledge. In a decade, maybe only the rich kids with fancy Kindles or iSlates will be able to read the good stuff like Harry Potter, while the poor kids with only access to crummy public libraries will have to make do with yellowing, dog-eared paper (!) copies of Encyclopedia Brown novels. If books are really going all electronic, I think we need to think now about how to avoid creating a technology gap that closes certain people off from getting access.
What was the first sensor network? Thinking back, I bet most people would guess the early demos by Berkeley and UCLA at places like the Intel Developer's Forum; the Twentynine Palms air-dropped mote demo; and Great Duck Island. These were all around 2002 or so. It turns out this is off by about 35 years -- the first bona fide wireless sensor network was actually deployed in Vietnam, along the Ho Chi Minh Trail, in 1967. It was called Igloo White.  I've been doing a lot of reading about Igloo White lately, and while most of the information on the program is still classified, there are a bunch of articles, books, and a few websites that provide some useful details. Igloo White was a system designed to use seismic and acoustic sensors to detect PAVN movements along the Ho Chi Minh Trail from North Vietnam, through Laos, and into South Vietnam (thereby skirting the DMZ). It consisted of wireless sensors, typically dropped from helicopters and aircraft. In all nearly 30,000 sensors were deployed during the war. (The image above shows an airman about to drop an Igloo White sensor from a helicopter.)  You can read much more about Igloo White elsewhere, but some juicy tidbits that caught my attention. The sensors themselves were shaped like large artillery shells and buried themselves in the ground, with a radio antenna designed to look like the surrounding jungle foliage. They used lithium batteries with an expected lifetime of 30 days. Each sensor performed simple local thresholding and triggered a movement alarm when a certain level of activity was detected. (All of this was done with analog circuity: at the time a digital computer was the kind of thing that filled a good portion of a room.)  The sensors would transmit a 2W signal on the UHF band which would be picked up by orbiting EC-121R aircraft that flew 24/7 on rotating 18-hour missions over various parts of the sensor field. The personnel on board would listen to the transmitted acoustic signals, and attempt to classify the targets. They could even identify specific trucks based on the acoustic signature of the engine. Detections were relayed to a top-secret SIGINT center at Nakhon Phanom, Thailand, where the data was stored by IBM 360 computers and processed largely by human analysts. The analysts would then call in air strikes against the targets. Note that in many cases the bombing runs occurred at night, using ground-based RADAR for bomb guidance, so the pilots never even saw what they were hitting. Presumably they could go from target to detection to, ahem, interdiction, in less than five minutes.  The most incredible thing about Igloo White was the sheer amount of resources that were poured into the program: the whole thing went from concept to reality in little more than a year, and cost more than a billion dollars. The system relied entirely on human analysts to interpret the sensor data, which was often noisy; later in the program the North Vietnamese became adept at spoofing the sensors. The number of people involved in deploying the sensors, monitoring the signals, and interpreting the data was almost inconceivable; the operations center in Thailand was the largest building in Southeast Asia at the time, complete with air conditioning, positive pressure, and airlocks to prevent contaminants from damaging the computers inside. This is not the kind of thing you're going to do with two grad students and a half-million in NSF funding!  The I-REMBASS system from L3 Communications represents the state-of-the-art in (deployed) military sensor networks. And, of course, much of the modern work on sensor nets came out of DARPA programs such as NEST. Still, it's fascinating to see the history behind this technology.
I've recently had two conference submissions "rejected with prejudice" for violating the formatting requirements. In both cases, it was because the lead grad student on the paper didn't read the call for papers carefully enough. (I'll take my share of the blame for not checking it, but sometimes it's pretty hard to check without pulling out a ruler and counting lines of text by hand.) Now, I totally agree with those papers being rejected: it's essential that papers adhere to the formatting requirements. On the other hand, it certainly does not help that every conference uses slightly different guidelines. Here's a typical formatting convention: Submitted papers must be no longer than 14 single-spaced pages, including figures, tables, and references. Papers should be formatted in 2 columns, using 10 point type on 12 point leading, in a text block of 6.5" by 9". Another one: Submissions must be full papers, at most 14 single-spaced 8.5" x 11" pages, including figures, tables, and references, two-column format, using 10-point type on 12-point (single-spaced) leading, with a maximum text block of 6.5" wide x 9" deep with 0.25" intercolumn space. Yet another (from SIGCOMM 2009): Submissions MUST be no more than fourteen (14) pages in 10 point Times Roman (or equivalent font). This length includes everything: figures, tables, references, appendices and so forth. Submissions MUST follow ACM guidelines: double column, with each column 9.25" by 3.33", 0.33" space between columns. Each column MUST contain no more than 55 lines of text. NOTE: For the submission, you may use the following LaTeX template to ensure compliance. The final copy will be 12 pages using the SIGCOMM standard 9 pt format; this is less than what you might be able to fit in 14 pages at 10pt, and so there is no value in pushing the envelope. Provide an abstract of fewer than 200 words. Number the pages. Do not identify the papers' authors, per the Anonymity Guidelines. On the front page, in place of the authors' names, the paper MUST indicate: the paper ID number assigned during the paper registration process and the total number of pages in the submission. The paper MUST be submitted in PDF format. Other formats (including Postscript) will not be accepted. We must be able to display and print your submission exactly as we receive it, using only standard tools (Adobe Acrobat Reader), with no loading of special fonts. Make sure that the paper prints well on black-and-white printers, not color printers. This is especially true for plots and graphs in the paper. Make sure that the output has been formatted for printing on LETTER (8.5" by 11") size paper. Make sure that symbols and labels used in the graphs are readable as printed, and not only with a 20x on-screen magnification. Try to limit the file size to less than 15 MB. This is getting silly. Even conferences sponsored by the same organization (say, USENIX or ACM) have different formatting guidelines. In one case, our paper was rejected because it was a resubmission from a previous conference that used an oh-so-slightly different format.  Now, I am all for having a consistent, and firm, formatting requirement for conference submissions. It's really unfair to authors that take pains to adhere to the guidelines when someone violates them by squeezing too much text into the paper. But isn't it time that we define a single standard for conference paper formatting that everyone uses?  Yes, I know there are various templates out there, but most of these are for the final proceedings, which can vary substantially from the submission format. Even worse, many of the "standard" templates floating around out there don't adhere to the submission guidelines anyway. What would help tremendously would be to have a canonical standard template (in various formats, e.g., LaTeX and Word) that everyone uses. Then it would be trivial to tell if someone had tweaked the formatting since their paper wouldn't look the same as the rest. This is the model used by EWSN 2010 (which required submissions to be in the Springer LNCS format) and it worked very well -- all of the submissions had consistent formatting.  A word on automatic format checkers. Both of the conferences we submitted to were supposed to be using an automated format checker that should have flagged the paper as violating the requirements when we submitted it. In both cases, this failed, and the program chairs (quite rightly!) rejected the paper once they discovered that the formatting was wrong. Unfortunately we were not careful enough and assumed that passing the automated check meant that we had done everything correctly. I like Geoff Voelker's Banal system, but it doesn't always work (mostly the fault of the underlying Ghostscript tool that it's based on). Even doing this kind of thing manually is a big pain -- Adobe Acrobat Pro lets you measure things like text blocks and font sizes, but it's a lot of manual effort.  Finally, as a TPC chair I have been on both sides of this, and I always hate rejecting papers due to formatting violations, especially when I know the authors have done good work. Dealing with formatting problems is a huge time sink when you're running a conference, and a standard format would save everyone -- authors, program chairs, reviewers, publication chairs -- a lot of trouble. I think it's time for the systems and networking community to simply define a single standard and get all conferences to use it.
Should CS grad students be required to receive formal training in lab technique?  In most scientific disciplines, a great deal of attention is paid to proper experimental design, data collection, and analysis. A grad student in chemistry or biology learns to adhere to a fairly rigid set of procedures for running an experiment (and documenting the procedure). In CS, we largely assume that grad students (not to mention professors and undergrads) somehow magically know how to do these things properly.  When I was a grad student, I more or less figured out how to run benchmarks, collect and record data, document experimental setup, analyze the raw data, and produce meaningful figures on my own. Sure, I had some mentorship from the more senior grad students in my group (and no small amount of pushback from my advisor when a graph would not make sense to him). But in reality, there was very little oversight in terms of how I ran my experiments and collected results. I logged benchmark output to various homebrew ASCII file formats and cobbled together Perl scripts to churn the output. This evolved considerably over time, adding support for gzipped log files (when they got too big), automatic generation of gnuplot scripts to graph the results, and elaborate use of Makefiles to automate the benchmark runs. Needless to say, I am absolutely certain that all of my scripts were free of bugs and that the results published in my papers are 100% accurate.  In my experience, grad students tend to come up with their own procedures, and few of them are directly verifiable. Sometimes I find myself digging into scripts written by one of my students to understand how the statistics were generated. As an extreme example, at one point Sean Rhea (whom I went to grad school with) logged all of his benchmark results directly to a MySQL database and used a set of complex SQL queries to crunch the numbers. For our volcano sensor network deployments, we opted to log everything using XML and wrote some fairly hairy Python code to parse the logs and generate statistics. The advantage of XML is that the data is self-describing and can be manipulated programmatically (your code walks the document tree). It also decouples the logic of reading and writing the logs from the code that manipulates the data. More recently, students in my group have made heavy use of Python pickle files for data logging, which have the advantage of being absolutely trivial to use, but the disadvantage that changes to the Python data structures can make old log files unusable.  Of course, all of these data management approaches assume sound experimental technique. Obvious things include running benchmarks on an "unloaded" machine, doing multiple runs to eliminate measurement error, and using high-resolution timers (such as CPU cycle counters) when possible. However, some of these things are more subtle. I'll never forget my first benchmarking experience as an undergrad at Cornell -- measuring round-trip latency of the U-Net interface that I implemented on top of Fast Ethernet. My initial set of runs said that the RTT was around 6 microseconds -- below the fabled Culler Constant! -- beating the pants off of the previous implementation over ATM. I was ecstatic. Turns out my benchmark code had a small bug and was not doing round-trip ping-pongs but rather having both ends transmit simultaneously, thereby measuring the packet transmission overhead only. Duh. Fortunately, the results were too good to be true, and we caught the bug well before going to press, but what if we hadn't noticed?  Should the CS systems community come up with a set of established procedures for running benchmarks and analyzing results? Maybe we need a "lab manual" for new CS grad students, laying out the best practices. What do people think?
Being the end of the year, I think it's appropriate to reflect on some of the best things that happened in 2009.  Best use of official signature: graduating two Ph.D. students. I'm finally no longer a leaf node on the academic genealogy. Two of my graduate students, Bor-rong Chen and Konrad Lorincz, finished their degrees this fall, and I am insanely proud of them. It is really amazing to look back on all of their hard work over the last few years and reflect on what they accomplished. At the same time it's kind of sad to no longer have them in my group, although Bor-rong is sticking around as a postdoc on a new project that we have going with the Wyss Institute. (I'm kind of hoping he never leaves!)  Best application of Fernet Branca: The Toronto Cocktail. Fernet was a pre-2009 discovery, to be sure, but this particular combination of rye, Fernet, simple syrup, and bitters is by far the best way to mix it. I've been teaching bartenders around Boston how to make it; ideally with a flamed lemon peel. Runner up: The Trinidad Sour, which uses Angostura bitters as a base. Challenging.   Best album: Animal Collective's Merriweather Post Pavilion. This album is rich, joyful, perplexing, and beautiful. I've probably listened to it more than anything else I got in 2009, although my last.fm profile begs to differ. It's no surprise it topped Pitchfork's list of best albums in 2009. If you're not convinced, check out the video for "Summertime Clothes" here. Runners up: Bitte Orca by the Dirty Projectors; In Prism by Polvo.   Best use of stimulus money: RoboBees. One highlight this year was being a Co-PI on an NSF Expeditions in Computing grant to develop a colony of micro-scale flapping wing robots. Along with nine other faculty, we are tackling a bunch of exciting research problems, my particular focus being on systems and language support for coordinating the activity of the colony. This is going to be a fun project and a bunch of students are getting involved already.   Best reason for sleep deprivation: Becoming a dad. Having a baby has been the most challenging, and most rewarding, thing that has ever happened to me. After raising a puppy and advising eight Ph.D. students, I figured the fatherhood thing would be a cinch. Not so. But it has been a huge learning opportunity -- about myself, about what really matters in life, about setting priorities. Sidney is now almost six months old and is the cutest little fella I've ever seen -- I just can't wait to be able to take him to the zoo and teach him C++.  Here's to a great year and best wishes for 2010.
Like most faculty, I serve on a lot of conference program committees. I estimate I review O(10^2) papers a year for various conferences and journals. When reviewing so many papers, it is amazing to me how many authors make simple mistakes that make it so much more difficult to review (let alone accept!) their papers. Keep in mind that when reviewing 25+ papers for a program committee, you have to do them fairly quickly and the easier it is for the reviewer to digest your paper and get to the core ideas, the more likely they are to look favorably on the paper. I tend to review papers while on the elliptical machine at the gym, which also helps to tamp down any physical aggression I might feel while reading them. (Of course, I have to go back and write up my comments later, but usually in a post-exercise state of unusual mental clarity.)  A few hints on getting papers accepted -- or at least not pissing off reviewers too much.  1. Spellchcek.  Seriously, how hard is it to run your paper through a spellchecker before submission? Whenever I see a paper with typos - more than one or two - I figure the authors were too rushed or lazy to get something as simple as spelling right, and this casts doubt on the technical content of the paper as well. Sometimes typos creep through that a spellchecker won't catch - like using "their" instead of "there". You were supposed to learn that distinction in high school. (I have some bad habits of my own. For some reason, I always type "constrast" instead of "contrast" -- no doubt a holdover muscle memory from my days programming LISP.)  2. Get the English right.  This is a major problem for papers coming from non-native speakers, and although one is supposed to overlook this, nothing grates on a reviewer more than having to slog through a paper full of grammatical mistakes and strange wording choices. Sometimes the nature of the grammatical and stylistic problems can reveal the provenance of the authors: Asian writers tend to confuse singular and plural, while Indian writers tend to use convoluted "Indianized" expressions held over from the days of the Raj. (One paper I once reviewed used the Indian term crore -- meaning 10,000,000 -- as though everyone knew what that meant.) If in doubt, get a native (that is, American) English speaker to review the paper before submission. Be sure to throw a couple of "Go U.S.A.!"s in there for good measure; it'll mask your foreign identity.  3. Make the figures readable!  I can't tell you how many times I have been unable to read a figure because it was formatted assuming the reader would be looking at a PDF on a color screen, and able to zoom in to read the tiny letters in the legend. This is not yet possible with printed paper, and I tend to print in black and white, as I suspect many reviewers do. When formatting figures, I try to use colors that will have adequate contrast even in black and white, use thick lines with a variety of dash styles that make them easy to distinguish, and set 18 pt Helvetica font that will be legible when squashed down to figure size.  4. "Related work" is not just a list of citations.  In general I really dislike "related work" sections that merely list off a bunch of related papers without explaining how they differ from the paper at hand. The point behind this section is not to simply give a shout out to potential reviewers or to prove you've done your homework: it is to contrast the contributions of your paper from what has come before. Also, your goal is not to shoot down every other paper you have read, but rather to place your work in context and explain the lineage. It is OK if another paper has worked on a similar problem and even shown good results. This suggests you may not be barking completely up the wrong tree.  5. Make sure the intro kicks ass.  It is not uncommon for me to decide whether I'll mark a paper as "accept" or "reject" after reading the first page. Actually, more likely I'll have decided on a "reject" early on, and withhold any "accept" decision until I've read the whole thing. Still, a beautifully written introduction that makes a compelling case for the ideas in your paper goes a LONG way towards influencing the reviewer's disposition. David Patterson is the master at this. After you read the intro to, say, the Case for ROC paper, you think to yourself, "but of course! This is the best idea ever!" and then feel really crappy for not having thought of it yourself.  6. Get to the point.  The first paragraph of the introduction is an opportunity to dive into the subject of your paper, not an excuse to toss out some lazy canned problem statement copied from a dozen other papers you read last year. The first sentences from my last three papers were: Wireless sensor networks have the potential to greatly improve the study of diseases that affect motor ability.  The unused portions of the UHF spectrum, popularly referred to as “white spaces”, represent a new frontier for wireless networks, offering the potential for substantial bandwidth and long transmission ranges.  Resources in sensor networks are precious. All three tell you immediately what the paper is about; these are not throw-away statements.  7. State your contributions!  I can't believe how many papers never explicitly state the contributions of the work. Giving a numbered list of your contributions is essential, since it gets the reviewer focused on what you think is important about the paper, and it defines the scope of the eventual review. Too many papers lay forth platitudes of how the work will cure cancer and world hunger, but it's hard to tease that apart from how you've tweaked a timing parameter in 802.11. By the same token, contributions should be focused and concrete. Tell us specifically what you did, what the results were, and why it matters.  8. Don't bullshit.  Finally, don't exaggerate your results or claim more than you have really done. Nothing irks me more than a paper that promises to solve a huge problem and ends up showing a tiny sliver of the solution in a carefully-concocted setting. It is far better to understate your results and impress the heck out of the reviewers than overstate the results and let the reader down. Everyone knows that the path from design to prototype to results is filled with pitfalls, and you will be excused for having cut some corners to demonstrate the idea; but make sure the corners you cut were not too essential to the core contributions you are trying to make.  Following these eight simple rules, I guarantee your paper will be accepted to any program committee that I serve on! (Hope you're planning a SIGCOMM'10 submission!)
Today was the First Post-NSDI Program Committee Meeting Mini-Systems Symposium at Harvard (PNSDIPCMMSS@H2009). That is, I invited four of the NSDI'10 PC members -- Chip Killian, Dejan Kostic, Phil Levis, and Timothy Roscoe -- to give short talks at Harvard on their current research, since they were in Boston for the PC meeting anyway. It was a lot of fun - for me, anyway. Everyone else at least got a free lunch.  Chip started off with a talk on Understanding Distributed Systems, at least those implemented using Mace. He gave an overview of the Mace language (which is one of my favorite pieces of languages-meets-systems work from the past decade) and the application of model checking to automatically verify Mace programs. This, to me, is the main reason we need better high-level languages for specifying complex systems: so we can build useful tools that let us understand how those systems behave before and during deployment.  Phil gave a kind of far-out talk on a new system they are building at Stanford, called Meru, which is a federated, planetary-scale virtual world platform. The goal is to enable virtual worlds that are extensible and scalable in all kinds of ways that existing systems (such as Second Life) are not. Personally I'd like to see how this technology can be leveraged beyond games and entertainment. Why not enable virtual conferences, or at least virtual program committee meetings? There are a lot of challenges here but it's important to tease them apart from the games-driven work that has been done to date (and often quite successfully).  Dejan talked about his work on CrystalBall, which allows for online model checking to catch and even avoid bugs in a distributed system via execution steering. The running example was a buggy implementation of Paxos, and Dejan showed how their approach could avoid the bug by steering execution away from the states that led to it. Mothy asked, "as long as you're going to do this, how much of Paxos do you need to implement, anyway?" In some sense this is shifting the correctness of the system from the original code into the CrystalBall system itself, and that makes me nervous. Margo raised the point that if a system is able to avoid a bug, is there really a bug in the first place. Too deep for me!  Finally, Mothy gave a talk on their experiences implementing BarrelFish (now with Disney-friendly logo!) and some reflections on the value of undertaking a "big OS implemenation project." BarrelFish has led to some interesting offshoot research (such as clever compilers for DSLs and new ways to think about capabilities) and they have gained a lot by departing from just hacking Linux. On the other hand this is a tremendous infrastructure-building effort and having nine authors on a single SOSP paper strikes me as overkill. One thing that helps them a lot is having a couple of full time systems programmers -- not grad students! -- which I find is hard to fund on your typical NSF grant. The longevity of the artifact does seem to depend on having people around who can maintain the code over time.  I'll post slides here as soon as I have them!
I am a huge fan of the social news site Digg. It is where I go to get my daily dose of Internet stupidity, ranging from XKCD comics to pictures of fail. For those that have been living in a cave the last five years, the way it works is that users submit links to random sites they find on the Internet, and those that like the link "digg" it, thereby increasing the link's popularity. Tracking Digg is a good way to keep your finger on the pulse of the Internet, or more accurately, the segment of the Internet that 15-to-22 year old boys seem to care about.  The best part of the site are the incredible comments left by the users. Often these are funnier and more obtuse than the original link, and Digg comments are something of a genre in and of themselves (good examples being repeated ASCII-rendered appearances of Admiral Akbar and something called Pedobear). Indeed, occasionally the comments can get out of hand.  Here's a crazy idea that I came up with (incidentally, while having wine and cheese with the chair of the Harvard stats department this afternoon). Why not adopt the Digg model to crowdsource peer review of grant proposals? Scientists would post their grant proposals publicly and anyone would be allowed to "digg" a proposal -- or "bury" a proposal that has flaws or a particularly bad idea. Public comments would be used to convey feedback to the authors and open up debate on the research plan to the many thousands of highly qualified Internet users who are conventionally excluded from review panels.   This model would seem to have all kinds of benefits. Rather than making funding decisions in the proverbial smoky room, requiring the funding body to spend untold millions in taxpayer money to fly panelists to DC and put them up in hotels for a couple of nights, this approach would bring everything out into the open. The Digg model would also streamline the review cycle to run in "Internet time" -- reducing the typical six month turnaround time to mere hours! Best of all, users on the site would adopt clever screen names like "W1F1d00d" and "Prof. BabyMan" lending the proceedings a certain edginess and panache sorely missing from the current panel review system.  If you like this idea, why not Digg it?
A bunch of students are now applying to graduate schools, and to help them out, every year I give a talk on getting into grad schools in Computer Science (click the link for the slides). Luis von Ahn has an amusing post about the process on his blog - all of his suggestions ring true.  The key thing that gets under my skin about graduate applications is the personal statement. All too often, applicants see this as an opportunity to tell their life story, especially about some experience they had with computers as a kid. "Since I was nine years old..." is the most common opening line in these statements. Frankly, I don't care about any of that. I am looking for potential grad students who have a mature and serious outlook about research. Of course, the best way to demonstrate that is to actually have done some research as an undergrad -- and putting together the Web site for your a cappella group doesn't count. My suggestion is for students to model the personal statement as a mini-research proposal: tell me about a problem you want to work on, have done some thinking about, and how you would approach it. And, convince me that you have the technical experience necessary to do graduate level work.  By the same token, I don't care about how enthusiastic a student comes across in their application to work with me. A lot of ass-kissing goes on in the personal statements sometimes and that drives me crazy. Just tell me how awesome you are, not how awesome I am, or awesome Harvard is. We know that already :-)  Here is my rough algorithm for screening Ph.D. applications: Make sure the GRE scores and GPA are reasonable - not necessarily stellar. (I only had a 3.4 GPA when graduating from Cornell, and was told later that this almost sunk my application to most grad schools. Fortunately, I had also published three papers by the time I applied to grad schools, which offset that. As a result, I tend to use a lower threshold for the GPA than some others, to catch the diamonds in the rough.) See if the student has any evidence of research experience -- supervised by a faculty member. Publications don't really matter but are helpful. If there is any black mark on the transcript (say, making a C in an important CS class), see if there is any explanation of that in the personal statement or elsewhere. (We once had an applicant who failed most of his classes one semester, but retook them and made A's the next term. Until I read the personal statement, it was not clear that this was because he had been hospitalized for a substantial portion of the term.) Finally, read the recommendation letters. These are the most important part. Steps 1-3 are just pre-screening to save myself the trouble of reading the bulk of the applications. Letters from people I know (or know of) are given higher weight. Letters from academics get priority over letters from industry - most industry letters (even from places like MSR) paint a very rosy picture. Letters that simply say "so-and-so took my class and made an A-" with no other content actually hurt an application, all else being equal. If a student is really stellar, even faculty who don't know the student well should be able to say good things.
I am occasionally asked by new faculty and grad students (at other schools) whether they should get involved with sensor networks. The concern is that the field has "peaked" and there are risks associated with jumping into an area that may have past its prime. I'm not sure I agree with this assessment. It is true that much of the low-hanging fruit has been picked: after all, the field has been active for about a decade. Rather, I see this as an opportunity to ask where should the field be going in the next ten years. We had a panel on this topic at SenSys 2009, for example. A few thoughts.  First, I think it's important for anyone (new faculty member or grad student) to avoid being too narrow in their research scope. I have always considered myself a "distributed systems person" with a recent fetish for sensor networks, not a "sensor networks person" per se. The set of questions I am interested in find manifestations in sensor networks, but also in clusters, mobile systems, Internet-scale systems, and other domains. That said, it never hurts to ride a wave of burgeoning interest in a research area, as a way of establishing yourself in a community and having impact. To that end the question is whether sensor networking currently offers the right mix of hard problems and trendiness to build a career.  The popularity issue is hard to gauge, but I am convinced that there are some very hard problems in this space that few groups have yet looked at. Most folks probably agree that MAC protocols, routing, localization, and other foundational system services are well trodden problems by now. I'll throw out a few problems that I consider wide open, and by no means is this an exhaustive list.  Managing networks of billions of sensors: Let's say that sensor nets do become massively widespread (some would argue this is well underway). I'm not sure we know how to effectively manage or evolve billions of wireless sensors distributed throughout our environment. Anyone who has deployed a non-trivial sensor network has had to undertake a great deal of manual effort to program the nodes, track their health and behavior over time, and keep tabs on simply where they are. In most computer systems, the ratio of humans to computers is fairly high, so it is reasonable to expect some level of manual effort. Currently, we largely treat sensor nets the same way, but this simply does not scale. We need much better tools and mechanisms for deploying and managing vast networks. Further, as new sensor nodes are introduced into an environment, with new capabilities and software, how do we integrate them with older sensors and allow the entire ecosystem to evolve?  Coupling sensor networks with actuation: This is a hot topic right now and generally falls under the rubric of "cyber-physical systems." I believe we have a very poor understanding of how to synthesize data from a large number of embedded sensors that may be poorly calibrated, not functioning properly (or at all), and exhibit high degrees of spatial diversity. Adding on actuation -- either control of an external system, or direct actuation by the sensor/actuator nodes themselves -- opens up a whole slew of hard problems. We'll need to reason about stability, robustness, and latency. Existing approaches to distributed control will need to take the nodes' resource limitations and failures into account.  Programming and reasoning about complex network-wide behavior: Finally, I am still convinced that our approach to programming large networks is fairly ad hoc. How many times have you read a paper about a protocol or distributed algorithm that involves a tremendous amount of complexity, and for which it's unclear how it will operate in different settings or in conjunction with other code? There are many smart people (I am not one of them) who know how to design complex distributed algorithms, but the devil is in the implementation details, and it's easy to get those wrong. Reasoning about the dynamics of a large sensor network subject to variable loads and resource conditions is extremely difficult, and currently we resort to tuning our code by hand. It's possible that a combination of language, compiler, and runtime support could help automate much of this painstaking design process, but this will require good models and analytical techniques to understand network dynamics at scale.  I think there is plenty of fruit still on the tree, but we need to climb higher up to see it. An "entry level" project for someone wanting to get involved in sensor nets should probably not be to design a new MAC layer or try to solve the localization problem. It's great that systems like TinyOS have matured to the point that lowers the barriers to entry for new researchers; as David Culler likes to say, the hope is that a new researcher in the field can stand on the shoulders of giants, rather than on their toes.
I'm in Big Sky, Montana for SOSP 2009 -- widely considered the premier conference on systems. The location is stunning although it's bitterly cold here; I was not ready for this kind of weather for a couple more months. This seems to be the biggest SOSP ever with more than 500 people registered; the last SOSP had a mere 471 attendees. I am not sure what this means. The number of paper submissions has not been going up dramatically, so it's hard to tell if this represents an increasing interest in systems as a field. This year there are a number of co-located workshops, and perhaps with the weak economy people are concentrating their conference travel to focus on fewer high-impact events.  Best paper awards went to three papers: FAWN, RouteBricks, and seL4. These papers all represented a substantial amount of work and have a remarkable number of authors; they are not the kind of papers that were dashed off in two weeks by one or two grad students!  Below are some of the highlights from Day One.  Barbara Liskov - ACM Turing Award Lecture, "The Power of Abstraction"  Barbara used her Turing Lecture as an opportunity to survey the development of Abstract Data Types, CLU, and type hierarchy. It is remarkable how much impact these ideas have had on modern programming languages and practice; Java, C#, Python, and many other languages are the direct descendants of this work. She spend a considerable amount of time talking about the context of programming practice before ADTs were developed, and the issues that computer scientists were concerned about (program structure, control flow, and global state). She surveyed a lot of classic papers -- Dijstra, Wirth, Parnas, and others -- that led to the development of ADTs.  After talking about the mechanisms in CLU, Barbara spent some time reflecting on the current state of programming practice. According to her, modern languages (like Java and C#), but the state of programming is "lousy" -- many untrained programmers developing Internet and browser-based systems rife with things like global state. She wrapped up the lecture by pointing the way to some future opportunities in large multicore systems and reasoning about the semantics of programs running across the Internet.  FAWN - A Fast Array of Wimpy Nodes  The premise of this paper is that it is possible to reduce the power consumption of data centers by an order of magnitude by leveraging much more power-efficient hardware. The idea is to focus on queries per joule as the metric, rather than simply peak performance. They propose a cluster of inexpensive, power-efficient nodes (based on the AMD Geode platform with a small amount of memory and Compact Flash rather than hard disks) and develop a key-value store application. The system has to address the limited memory on the nodes, the need to minimize random writes to the flash (using a log-structured data store), and the need to add or remove nodes from the cluster (using consistent hashing for lookups and migrating data in background tasks). Among other things, the paper shows that the FAWN prototype can sustain around 350 queries per second per Watt, compared to around 50 queries/sec/W for conventional hardware. They also do a nice analysis of total cost of ownership and talk about the regime (in terms of load and data set size) where FAWN makes the most sense.  The Multikernel: A New OS Architecture for Scalable Multicore Systems  This paper is about revamping the design of OS kernels to account for massively multicore systems with heterogeneous hardware (CPUs, GPUs, programmable NICs, and FPGAs). The key idea is to shift away from a single kernel with shared memory and locks on shared data structures over to running a separate kernel on each core, using message passing as the only coordination primitive between cores. This approach allows one to design interprocessor communication protocols that leverage the topology of the underlying interconnect, which can vary substantially between platforms. In the MultiKernel design, all shared state is treated as replicated across cores and mechanisms for explicit replica management (which are typical in distributed systems) become necessary. The authors have implemented a prototype of this design, called BarrelFish, and demonstrated its scalability as the number of cores is increased. This is a really nice piece of systems work and is a classic example of "from scratch" OS design to leverage new hardware trends.  Both the MultiKernel and FAWN papers were presented in an earlier form at HotOS 2009, which I have blogged about earlier.  Debugging in the (Very) Large: Ten Years of Implementation and Experience  This paper from Microsoft deals with Windows Error Reporting, which is the largest client-server system in the world by number of installs. WER receives 100 million error reports a day, across 17 million different programs, across a billion or so installed clients. It's been in operation for 10 years and has led to thousands of bugs being fixed, and has fundamentally changed how Microsoft approaches software development. Galen Hunt from MSR gave a great talk on this system.  The system automatically collects "minidumps" from clients when errors occur, which are then classified into buckets by the back-end servers using a variety of statistical analyses. The bucketing strategy takes into account the program name, version, and symbol in which the program crashed. Doing statistical analysis on so many error reports allows them to narrow in on the bugs that affect the most users and therefore concentrate bug fixing efforts. Galen talked about some of the successes of this system over the years, including identifying 5-year-old heisenbugs in the Windows kernel; detection of widespread malware attacks; and even finding bugs in hardware (such as a broken USB controller that did not implement DMA correctly). This is a great paper on a large, real-world system.   I'll blog about the BOFs, banquet, and award ceremony tomorrow.
I happen to be the proud father of a three-month old baby boy named Sidney. He is a great little guy but as a first-time parent I have often been frustrated with the lack of a clear, coherent instruction manual for taking care of newborns. One would think that after a few million years of raising children, someone would have documented how these things work. Of course, there are tons of baby books out there, but few of them are based on scientific principles. Most are based on anecdotal evidence and often the "parenting philosophy" gets in the way of empirical, pragmatic solutions. Searching for answers on the Internet just drives you crazy as you realize that nobody has any idea what they're talking about.  As a scientist and engineer, my only model for approaching a new, complex system with unknown behavior is to treat it as a black box. In the case of babies, the inputs are food, love, environment; the outputs are behavior (some desirable, some less so); occasional vomit, and dirty diapers. My rational mind tells me that there must be an algorithm at work inside, and my goal is to perform system identification and tweak my feedback control loop to maximize desirable behaviors while minimizing undesirable ones. It seems very simple.  Much of my prior experience in this domain comes from raising a puppy, using positive reinforcement-based operant conditioning as the primary approach. This was largely effective and complex behaviors (e.g., "roll over") can be taught through gradual shaping (i.e., when subject approximates desired behavior, issue reward; subject performs multiple rounds of exploitation vs. exploration to maximize reward potential until objective is reached). Mechanistically, positive reinforcement works with dogs, which is remarkable given that they appear to have far less cognitive capacity and language ability than humans.  Unfortunately, my experiments to date suggest that the strict engineering approach does not appear to be as effective on human infants. First, it is unclear how to provide a positive stimulus in response to desired behavior; one does not (say) offer a squirt of formula as a reward. Likewise, negative punishment is unlikely to be effective at this early developmental stage. How, then, does an infant learn new behaviors?  The biggest challenge so far has been teaching Sidney to fall asleep when he is tired. I previously assumed that falling asleep was a natural, almost automatic response to fatigue. But as we all know we actually have to learn how to fall asleep -- that the feeling of exhaustion is best relieved by finding a dark, quiet place, laying down on a flat soft surface (ideally with pillow and optional teddy bear) and closing one's eyes and relaxing. (Personally, I cannot fall asleep without several conditions being met; imagine how hard it is to sleep in coach class on a long flight when it's noisy, you can't get horizontal, and the drink cart keeps slamming into your knee.) Sidney has no way of knowing that fairly obvious set of facts, and even worse I have no direct way of teaching him those skills. Sidney's only possible learning algorithm is random search; effectively trial-and-error. It must be extremely frustrating to be an infant.  What's frustrating to me is that none of the baby books I have read so far appear to be rooted in any theories of infant learning. I am currently reading Ferber's classic work on sleep training, although the book itself does not offer theories as to how the learning process is supposed to work in a four-to-six month old infant (although there is a lot of good information on infant sleep patterns). Perhaps it is time to start reading some of the scientific studies. Although, perhaps one does not need to be conversant in the medical literature in order to be a good parent?  Next article in the series: Black Box Graduate Student Advising.
I have wondered for a while whether the computer science community should not place more value on journal articles, rather than conference papers. Journal articles are not just longer versions of conference papers that take much more time to review -- they are meant to represent a capstone on a large piece of work, which is something we often overlook in our field.  Much systems research is driven by a rapid cycle of develop, evaluate, publish (and not always in that order). With a couple of major conference venues every year, and the need to build a strong publication record as a major determinant of one's (perceived) success in the field, there is a high incentive to push out new papers as quickly as possible, irrespective of how half-baked the content might be. Many conference papers do little more than scratch the surface of an idea -- it is hard to do more in only 14 pages. The expected longevity of a paper (even a good one at a top conference) is little more than a year, two tops. And most systems on which the papers are based never see the light of day, apart from perhaps a tarball slapped together and linked on a student's website.  It's a collective form of ADHD -- hack, publish, move onto the next thing. In some sense, it's more important to be the first person to publish in an area rather than to develop a system to the point where the major problems have been actually solved, and the concept thoroughly vetted. Research fads come and go pretty quickly. (Remember distributed hash tables?) Once the first few papers have been published in an area people start to get antsy looking for the next big idea.  In other scientific communities, there is a vastly different expectation of the maturity of a piece of work before it can be published, using journal articles as the primary means of dissemination. As much as we scorn journals, they do have the virtue of slowing things down -- requiring more in-depth presentation of the ideas, extensive reviews, and sometimes multiple revisions before the work can be published. (My wife, who is a psychiatrist, reports that several of her articles have been in the review and revision cycle for more than a year and a half. Computer scientists don't have this kind of patience.)  One can argue that the journal editorial cycle is too slow for a fast-moving field like CS. I think that's naive; other scientific disicplines -- molecular biology, particle physics -- are innovating at least as rapidly and manage to do so within the content of a journal article framework. Those communities have the means for getting early results out there -- posters and oral presentations at conferences, online repositories like arXiv -- but there is a much clearer line drawn between the early work and the culmination of a major research effort. In the systems community, we have workshops like HotOS for floating new ideas, but it's not uncommon for a HotOS paper to turn into a major conference publication just a few months later. (One could argue that a project at that point of maturity should not be a candidate for a "hot topics" workshop. CS research seems to exhibit a high degree of entropy: work goes from "hot" to "cold" pretty quickly.)  I wonder what this rapid cycle does to the quality and depth of the work in our field, compared to that in other fields. I like to think that CS has shed the antiquated, lumbering trappings of other academic disciplines, but in our rush to keep the publication cycle going, what are we missing? Does our rapid-fire approach to research cause us to spend too much time on playing small-ball, rather than investing time into the hard problems that could take years to bear fruit? Does it make sense to place more value on the currency of journal articles in CS?
Last year was a huge success for faculty hiring at Harvard CS -- we added three new faculty members to our ranks, two of whom are starting this fall. (Yiling Chen started last year -- she works at the intersection of Computer Science and Economics.) Fortunately, we managed to do the search before the present economic unpleasantness; so I'm pleased to welcome Stephen Chong and Krzysztof Gajos to Harvard.  Stephen got his Ph.D. from Cornell and works in the area of programming languages and security. His work on the Swift system (published in SOSP'07) allows one to build secure Web applications where the client- and server-side code are automatically partitioned from a single, unified program written using the Jif variant of Java, which incorporates support for information flow in the programming model. This is a very practical approach to providing information flow support in a real system.  Krzysztof got his Ph.D. from University of Washington and is the first HCI person that we've hired at Harvard. Actually we've been looking to hire in this area for some time, but never found someone we really liked -- until Krzysztof. He is a great match for the kind of multidisciplinary work that we do here. Among other things, he developed the SUPPLE system, which automatically generates user interfaces for users with motor disabilities -- a nice combination of HCI and machine learning work.  Harvard grad students can look forward to the fact that Stephen and Krzysztof are both teaching graduate seminars this term.  (On a related note, I am delighted to have a new name-spelling challenge on my hands. It took me a couple of years to learn how to spell Mema Roussopoulos' surname without looking it up -- I am resisting the urge to create a macro for Krzysztof. This reminds me of my colleague from Berkeley, Rob Szewczyk, who once cheekily explained to me that his name is spelled exactly as it is pronounced.)
This week our paper, joint with Microsoft Research, on White Space Networking with Wi-Fi Like Connectivity was presented at SIGCOMM 2009, where it actually won the best paper award. This paper lays the foundations for the first Wi-Fi like network operating in the UHF white spaces (that is, the portions of the TV spectrum unoccupied by TV channels, wireless mics, and other devices). There's been some press on this work from Technology Review, Engadget, and other sites. My student, Rohan Murty, gave the talk. He is pictured to the right, apparently wearing the UHF antenna on his head -- I am not sure whether this improves his mental capacity or not. (Update 8/24/09: The slides are now available.)  By way of background, in 2008 the FCC issued a ruling allowing unlicensed devices to operate in the UHF white spaces, under certain restrictions. Opening up this spectrum for unlicensed wireless networks is a huge opportunity -- for example, UHF devices would achieve much longer range than networks operating in the 2.4 GHz and 5 GHz ISM bands. There's been a lot of recent research on establishing individual links in the UHF white spaces, but to our knowledge nobody has proposed a network design allowing multiple clients to communicate via an access point. That's where WhiteFi comes in.  Networking in the UHF white spaces raises a number of interesting challenges. The first is that the spectrum is fairly fragmented, and we can make use of variable-width channels (unlike the standard 5 MHz channels used by existing 802.11 networks). This makes AP discovery more difficult since there are many combinations of center frequencies and channel widths that would require scanning.  The second is that, by FCC mandate, a white space device must avoid interfering with any "primary users" of the spectrum. TV channels are relatively easy to avoid, given that they don't tend to come and go (although a mobile device would need to determine when it is coming in range of a new station). It turns out that wireless microphones also operate in this band, and of course you can't predict when one might be turned on. This requires the use of channel sensing to rapidly determine the presence of a wireless mic and mechanisms for switching an access point and any associated clients over to a new channel when interference is detected.  In WhiteFi, the key idea is to use a software-defined radio to scan the physical RF channel and use an efficient algorithm for performing AP discovery without performing a full decode on the signal. The SIFT technique (described in the paper) is a simple time-series analysis of the raw samples from the SDR that quickly determines if there is an AP operating at the chosen center frequency, as well as its probable channel width. The SDR is also used to detect incumbents. WhiteFi also includes algorithms for assigning channels to APs based on spectrum availability, as well as for handling disconnections due to interference or station mobility.  Going forward, we are continuing to collaborate with Microsoft Research and are developing a white space testbed here at Harvard that will allow us to experiment with these ideas at larger scales. Ranveer Chandra, Thomas Moscibroda, and Victor Bahl from the Microsoft Networking Research group are all involved in this effort.
We're looking into the options for publishing the proceedings for SenSys 2009 (which I'm co-chairing with Jie Liu). Sensys is an ACM conference and traditionally has had a printed proceedings. I'm interested in what people think about an all-electronic (that is, online) proceedings, with no printed copies or CD-ROMs. The idea would be to put the PDFs on the conference website and ensure that they are archived by the ACM Digital Library. (Assuming, of course, that ACM would allow this -- I haven't looked into their policies.)  SenSys is an ACM conference, and has traditionally had printed proceedings. As anyone who has been a conference organizer knows, this can often be a cumbersome and slow process, requiring many weeks of lead time from the publisher, and quite a lot of work on the part of the authors and the publications chair. This also shortens the time available for paper shepherding. Going the traditional route also incurs a nontrivial cost which is passed down to every conference attendee, whether they want the printed book or not. Finally, there is an environmental cost to all those dead trees that never get read, and to physically ship out the proceedings to all of the SIG members.  Personally, I don't have a need for printed proceedings. I get my papers online, through Google or the ACM DL. I realize there are still plenty of people who want to hold a physical paper in their hands, which is what printers are for -- indeed, if we put the PDFs online a week or so before the conference, anyone who wants to flip through the papers physically could print them out themselves and bring them to the conference. Printed proceedings seem to me to be a holdover from the time when scientific literature was chiefly distributed through printed books and journals archived in libraries. That is not the reality today. Many journals have now gone all-electronic, so it seems odd to me that a conference could not go the same route.  At the same time, I realize there are some (real or perceived) downsides to electronic-only proceedings. There is a question about whether it really constitutes a "publication," and I have heard that some institutions in Europe and Asia expect a printed proceedings in order for a paper to "count." (I have no hard evidence of this so would like to learn more.) The copyright issue becomes a little sticky, but I think that it would work fine to allow authors to retain copyright in their papers but require that they release them under a Creative Commons license permitting the conference organizers, and ACM, to distribute them. Finally, do electronic-only proceedings diminish the gravitas of a paper being accepted into a major venue? I would hope that those bulky yellow-spined books alone do not impart validity to a conference paper, but you never know.  So, I'm curious to know what others think about going all electronic.
The Chronicle of Higher Education has a great article on the importance of writing skills for graduate students. (Thanks to Jitu Padhye for the pointer.) Though nothing in the article surprises me, the article highlights a widespread concern about the lack of formal writing training for grad students. Learning to write effectively is one of the most important skills you need as a grad student, and, of course, as a researcher or faculty member later in life. But we don't actually teach this skill. Most faculty (myself included) seem to expect students know how to write, or will somehow pick it up in the course of their research -- and, presumably, having enough conference papers rejected. Even worse, most students don't realize how bad their writing is. This becomes a real problem if you're a new faculty member trying to get funding, and people can't follow your papers or are unconvinced by your grant proposals. Good writing is everything.  I'm not sure how to solve this problem. Putting my grad students through dry technical writing classes doesn't seem to be the answer. Good scientific writing involves a great deal of subtlety. It is not just about being grammatically correct, but conveying ideas in a convincing manner. This is especially true in computer science, where the goal of most conference papers is to persuade -- to seduce the reader with a Big New Idea, not just to report on the results of a study or new finding. Many courses on scientific writing fail to meet the needs of CS, focusing instead on the dry presentation of methods and data. That is important in CS, of course, but if you compare the structure of your typical Nature or Science article to, say, an SOSP paper, the differences are stark. (On the flip side, I often find CS papers tend to fluff up a fairly simple idea with a lot of marketing to make the ideas seem more earth-shattering than they really are. Seriously, how is a minor tweak to the 802.11 MAC protocol going to change the way we think about the nature of human communication?)  Any suggestions on better ways to teach our grad students how to become great writers?
I've been fortunate to have several productive collaborations with domain scientists in fields such as seismology, emergency medicine, rehabilitation medicine, and public health. One of the exciting things about sensor networks is that they open up avenues for this kind of cross-disciplinary work, but there are always challenges in getting new collaborations off the ground. I've been doing some thinking about some of the keys to successful links between CS and domain science.  Mutual respect for each other's field. I often hear CS people say that "those physicists" (for example) just need CS people to "make their code run fast." It's a conceit that only computer scientists know how to program -- the domain scientists I've worked with often build and maintain complex software systems. By the same token, it's really helpful when domain scientists "gets" what turns a computer scientist on -- not just implementing something to get the job done, but trying to do it in the right way, or more efficiently, generally, or elegantly than what you might try at first blush. If each side sees a caricature of the other it's harder to find common ground where you can work together for mutual benefit.  Research potential. Of course, for a collaboration to make sense there has to be publishable research on both the CS and the domain science side. Unfortunately, when first starting out it is rarely evident that this is the case, so some amount of blind faith is necessary to get the ball rolling. Hopefully, by the time you've solved the "easy" problems you've opened the door to some really exciting "hard" problems. I'm better now than I used to be at determining whether a given collaboration has long-term potential, but it's not always straightforward. As an example, when we first started working with clinicians at the Spaulding Rehabilitation Hospital, most of the work was straightforward hacking to stream data from a set of motes to a laptop base station. This has evolved into a long-term research effort involving innovations in sensor network resource management, signal processing, and data quality optimizations. This was not obvious when we first started the project.  Patience. Cross-domain collaborations take a lot of time to get established and yield results. This steep learning curve definitely slows down the research effort. Our first volcano sensor network deployment in 2004 was really about us learning how the seismologists do field work, and how they collect and analyze data. It wasn't for another year that we were able to go back to the field with a system that was remotely useful. Likewise, our colleagues in seismology haven't yet been able to directly use the data we have collected with our sensor networks, since it is not a direct replacement for how they do their work. All the same, we've been able to publish a number of papers together and have learned many things that have fed into our ongoing research and no small number of grant proposals.  Blind luck. It's also true that I've been extremely lucky to find some of the collaborators that I have. I got connected with seismology through a former student of Margo Seltzer's who happened to do some summer work with Jonathan Lees, a geophysicist at UNC who happens to work on volcanoes (and is a huge geek to boot). Some of our medical projects got started by people simply coming up to me after I gave a talk on my work. It helps to be open to opportunities, though it involves building many "bridges to nowhere." In the end I think it's been worth it.
The Twelfth Workshop on Hot Topics in Operating Systems (HotOS) is under way this week in Ascona, Switzerland, at a former nudist colony called Monte Verità. (Sadly, this aspect of the locale has not had much influence on the attendees.) HotOS is about pushing the boundaries of our field and defining new research agendas. It is typically held in a remote, beautiful location and involves an unhealthy amount of alcohol. It's also by invitation only and you usually have to get a paper accepted to attend. This year there were 84 paper submissions and 22 papers accepted.  A few highlights from Day One...  Adam Greenfield, Nokia gave a keynote talk on The Elements of Networked Urbanism. Adam painted a very broad picture of the impact of new technologies on how people in cities connect, share information, and learn about their environment. Of course, this topic is near and dear to my heart since one of my projects is building a city-scale wireless sensor network. Adam's premise is that every physical object in a city (fire hydrants, streetlights, slabs of pavement) will be networked and addressable, turning them into networked and interactive resources, not just passive objects. (Personally, I think this a bit off mark --the fact that you can give every physical object an IPv6 address doesn't mean it makes any sense to talk to it. The real issue is the protocol, not the addressing.) Adam touched on many issues -- technical, legal, social, and economic -- tying into this vision. I wish he had said more about how this will impact the poorest denizens of the world's cities, and though he mentioned the slums of Mexico City, Manila, and Nairobi, it wasn't clear how "networked urbanism" help these people very much.  Margo Seltzer got the technical sessions off to a great start with a talk on Hierarchical Filesystems are Dead. She probably made it through two slides before the ornery audience started peppering her with questions - exactly what I like to see at HotOS. Her basic argument is that files are objects with multiple attributes and forcing them into a hierarchical namespace conflates naming and storage. The proposed approach is to allow users to build their own personalized namespaces for objects (in any structure they see fit) and apply well-established techniques from the Web (like bookmarks and search).  Colin Dixon argued that we should banish network middleboxes as too expensive, complex, and difficult to manage. The talk was focused on enterprise networks (Cisco middleboxes cost several thousand dollars) and ignored the prevalance of cheap (essentially free) middleboxes in home networks. He argued that end hosts are overprovisioned so it's better to run middlebox services at the edge where they are easier to manage. (I'm not sure I buy this. It seems easier to manage a single middlebox than a bunch of possibly misconfigured end hosts.) Also, the assumption of overprovisioned hardware doesn't necessarily hold for smartphones, which are increasingly important as network clients. (Colin's response to my question on this was that you should run a proxy for smartphones. But that's a middlebox. This probably deserves a bit more thought.)  Bingsheng He from Microsoft Research Asia talked about a new apporach for optimizing dataflow queries in clouds, called wave computing. The basic idea is to co-optimize multiple concurrent queries running within a framework like DryadLINQ or MapReduce to reduce redundant computation and I/O. It wasn't clear to me how this relates to conventional multiquery optimization in database systems; Bingsheng's response was that this is a hybrid approach between conventional query optimization and streaming query optimization.  Byung-Gon Chun from Intel Research Berkeley made the case for automatic partitioning of smart phone apps between the phone and a back-end cloud. The idea is to run a "clone" of the phone image in the cloud and use it to do things like background processing, verification, and running heavyweight computations that you can't do on a phone. The proposed approach involves partitioning and migration of process state between the phone and the cloud. This strikes me as unnecessarily complex. After all, most smartphone apps are already partitioned between the phone and a back-end service -- Byung-Gon suggested that this was painful and complex for programmers, but thousands of iPhone apps and hundreds of millions of users of services like Facebook suggest otherwise.  Prabal Dutta from Berkeley made the case that mobility changes everything in sensor networks. He focused on "mobiscopes" involving mobile sensors that collect data from their environment and exchange messages opportunistically. The key idea is that if a sensor can detect and characterize its own mobility, the networking stack can use this information to do a better job at link estimation, routing, and so forth. He described some cool new hardware that has a hardware vibration detection circuit that can wake itself up when it starts moving.  Jason Waterman gave the talk for our paper on Peloton. He got a lot of good questions, mostly dealing with concerns about the overhead of our proposed global tuple space for sharing resource state across nodes. Michael Scott made the point that a "one size fits all" approach probably won't work well. That's undoubtedly true, just as it is true for the wide range of services we have built up in more conventional distributed systems (RPC, shared memory, you name it). The question is whether the overhead of our underlying mechanisms can be recouped through better coordination between nodes in managing their resources.  Over dinner, I got a chance to gab with Mothy, Dave Anderson, Garth Gibson, Prabal, George Candea, Michael Kaminsky, and Shivnath Babu on a wide range of topics, including my wacky idea about "The Next Billion Programmers". I'll blog on that one later.
I have posted the camera-ready PDF of our forthcoming paper to be presented at HotOS 2009, entitled Peloton: Coordinated Resource Management for Sensor Networks. This paper describes a distributed operating system for sensor nets that is intended to provide the right abstractions to permit coordinated resource-management decisions to be made across multiple nodes. The key idea is that to get the best energy efficiency, it is often necessary for multiple nodes to orchestrate their roles.  The canonical example arises in a simple data-collection application, where each node locally samples data, perhaps performing some local processing, and forwards the data to a base station along a multihop spanning tree. To achieve long lifetimes, each node needs to determine how to mete out its limited energy reserves for sampling, computation, transmission, listening for and forwarding packets for other nodes, and other overheads such as maintaining time synchronization. Today this is generally done in an ad hoc manner, using manually-tuned, static schedules that fail to react to changes in the network load and energy availability over time. As a result, existing systems are typically very energy-inefficient and there is a lot of room for improvement.  Our Pixie operating system provides a node-level abstractions for reasoning about resource availability and giving applications the opportunity to adapt their behavior accordingly. The idea in Peloton is to extend this model across the network. (We named the system after the peloton in a road cycling race, in which cyclists draft each other closely to reduce wind drag and increase efficiency.)  There are three basic mechanisms in Peloton. First, nodes share information on their resource state using a global but weakly-consistent tuple space in order to promote informed decision-making. Second, we extend Pixie's resource ticket abstraction to support distributed resource allocations, called vector tickets. Third, we decouple the allocation of resources from use through a distributed ticket agent model. Nodes can delegate responsibility for resource allocation to a ticket agent operating centrally, remotely, or collectively.  In the paper we describe three application vignettes that would benefit from the abstractions in Peloton. My Ph.D. student Jason Waterman will be giving the talk at HotOS and we hope to have a prototype released this summer.
Many prestigious conferences in systems and networking -- such as SOSP, SenSys, and SIGCOMM -- constrain themselves to a single track over two and a half days. This limits the number of papers that can be presented at the conference to around 25-30 at most, assuming 30-minute talk slots.  The problem is that the field has been growing, but the publication venues have not. This means it is becoming increasingly competitive to get a paper accepted to one of these venues. You can check out the stats for yourself here. Although the stats are not yet on that page, SIGCOMM 2009 accepted only 10% of submitted papers. Conference publications in top venues are now a highly prized commodity, and one that is becoming increasingly valued over time. Unfortunately this leads to spiraling inflation, in the sense that the harder it is to get a paper accepted, the more it is seen as a superhuman achievement ("Wow! You got a paper into SIGCOMM!"), causing more people to submit more papers to the conference: a vicious cycle.  This value inflation is evident in the CVs of the hotshot graduate students on the job market last year. Several of us on the hiring committee were amazed at how many freshly-minted Ph.D.s were coming out with multiple papers in places like SOSP, OSDI, and NSDI. Clearly there is a lot more weight placed on getting those papers accepted than there used to be. When I was a grad student, publicatons were important, but nobody pushed me to publish relentlessly -- I submitted papers when I had something to submit. (I'll admit this was not very strategic.) Somewhere along the way the ante has been upped considerably.  Of course, this is a "great" thing for those lucky few who are successful at publishing at these venues. (To be fair, I count myself as part of that cohort.) But it does little to foster the community as a whole. Grad students need publications in top conferences to be taken seriously for faculty jobs. Junior faculty need them for tenure. If the conference venues become more and more selective, I don't see how we can sustain growth in the field as a whole. It seems to me that the number of papers accepted at conferences needs to keep pace with the number of new faculty and students entering the field. Either that or we need to reset our expectations of what constitutes making a research contribution.  There is also a potential loss to the diversity of the research community. Getting a paper into SOSP or NSDI takes a substantial investment of money and time. Research groups with more firepower (like, say, Microsoft Research) have a much easier time than those who might have good ideas but fewer resources. I don't have hard data to back this up, but it feels that it is increasingly rare to see papers from anything other than "top ten" universities and labs in the top venues. One thing that would help would be a quota on the number of papers that a given institution or author could submit to a conference, much as the NSF does for some of its programs. (It seems that everyone I know at MSR is working on at least least three papers for every major deadline. This is insane.)  Now, I am not suggesting that conferences lower their standards. But we need to decide what is reasonable and scale up as the community grows. One way to scale up is to create new venues, but this is not very cost-effective: it is expensive and time-consuming to run a conference, and who has time to go to so many events each year? Accepting a few more papers once you already have the whole program committee in the room incurs much less overhead.  This may mean going to multiple tracks, having shorter talks (my preferred solution), or not presenting every paper at the conference orally. As much as people bemoan multi-track conferences, every "single track" conference is really double-track: there is the track of people who stay in the room and listen to the talks, and the track of those who stand in the hallway and chat.  Finally, it's clear that paper quality varies over time, and it seems unlikely that any specific target for acceptance rates (say, 25%) makes sense for a given conference in a given year. But I think we should take a hard look at where we are headed, and ask whether we shouldn't open up the field more by devaluing the currency of the conference paper.
The NY Times is carrying an editorial today from Mark C. Taylor, the chair of the Religion department at Columbia, saying that we need to rethink the structure of graduate education, and universities as a whole, to make them more relevant in today's world. The article is generally thought-provoking, but dead wrong when it comes to science and engineering. Unfortunately, the article does not qualify its statements as being relevant only to the humanities and social sciences, which is too bad considering that some readers might extend this flawed line of thinking to apply to other fields.  I'm surprised the author would be so careless to say things like: "Most graduate programs in American universities produce a product for which there is no market..." and "Young people enroll in graduate programs ... all because of the illusory promise of faculty appointments." [Emphasis mine.] What planet is this guy from? What he really means is that in the areas of "religion, politics, history, economics, anthropology, sociology, literature, art, religion and philosophy" (the author's all-encompassing list of the realms of human thought that apparently really matter) it is damned hard to get a decent job after graduate school, and I agree. But this has little to do with the situation in the sciences and engineering, where graduate students go on to a wide range of careers in industry, government, military, and, yes, academia.  I grant that it is possible that we are headed towards a brick wall in these other fields. According to the latest Taulbee survey, Ph.D. production in Computer Science has been skyrocketing, at its highest levels since the survey was started. However, far more students are going into industry than academia. Even for students dead set on a faculty position, many can get a job straight out of graduate school -- postdocs are still the exception rather than the rule. This situation could change, but I'm not sure it's time to end universities as we know them. Religion departments are maybe another matter.
A few weeks ago I noticed some very strange looking pages showing up on the TinyOS Docs Wiki which I maintain. These pages contained what appeared to be ASCII-encoded binary data of some kind, although the format was not anything I recognized. Cursory searches for what might be causing this turned up nothing, so I ended up spending a couple of hours locking down the site to prevent malicious edits.  Turns out this was (what appears to be) a student project from Brown called Graffiti which is intended to provide a kind of encrypted, distributed filesystem (I gather, since the paper isn't available) on top of "public" MediaWiki sites. (I should point out that the bogus pages on my site did not have the explanatory message at the top saying that they were related to this project - I guess this was only added in a later version of their code.)  The authors seem to be reticent about the trouble they have caused, but a comment that previously appeared on the project page suggests that they don't quite get why this is such a problem: 03/09/2009 - Rejection! Our paper got rejected from IPTPS. One of the main points brought up by the reviewers was that our system was not a true peer-to-peer system. Most reviewers also seemed appalled at the idea of commandeering abandoned websites in order to store illegal content. Nevertheless, we are not deterred and will be searching for the next workshop/conference that is bold enough to take on the ideas of the Graffiti project! (Seen on this discussion board.)  Now, while the idea of a distributed filesystem riding on top of "open" sites is cool, the way the authors went about this is problematic. Just because some MediaWiki sites are open doesn't make it OK to spam them with bogus pages for the purpose of doing your research -- I am sure this violates both the terms of service of Brown's network as well as the networks of those sites they spammed.  There are better ways to evaulate this system than to hammer on unprotected wiki sites without permission. They could have used PlanetLab and run their own wikis to evaluate the system's scalability and robustness. They could have asked permission from site owners with a promise to clean up after themselves after the experiments were run. I hope the authors are kidding about the "bold enough" comment above. It suggests they underestimated the legal and ethical issues raised by spamming open sites just to get a paper published, nor the amount of hassle they have caused sysadmins of the affected sites. I just hope they learned some kind of lesson from this.
A lot has been said lately about the decline and fall of the newspaper industry. In the last week I've seen at least two TV interviews with newspaper publishers moaning that blogs (ahem) can't provide the same quality of reporting as they can. Yet, they give newspapers away for free, online, which seems to me to be a race to the bottom. If everything is free, how are readers supposed to value the reporting provided by newspapers over what they can get from the Huffington Post or (God forbid) Digg?  Look, if newspapers want to stay in business, they have to start charging money for online access. It's as simple as that. The trick is balancing revenue from subscriptions with revenue for online ads driven by "free" access. The NY Times ran an experiment a couple of years ago where they started charging for "prime" content such as the editorial pages. In the end they pulled the plug since they were losing hits. But the question is not how many hits - it's dollars per hit that matter. With web browsers like Firefox making it trivial to block online ads, a site can't continue to rely on ads alone to keep the business afloat.  This is going to require some creative pricing models. Personally I'd like to see newspapers make the current day's stories free, but require that you be a subscriber to access the archives. If I email a link to a story and the recipient doesn't read it that very day, well, too bad, they need to sign up to get access. A (generous) free trial period will lure people in. This model can work. I pay something like $5 a month for Angie's List yet I access the site only a couple of times a year. Mostly it's because the opportunity cost of not having access to the site is high when I need it (i.e., to find a good plumber when my dishwasher explodes). Same goes for sites like the NY Times. If someone emailed me a link to Bittman's latest recipe for chocolate chip waffles and I couldn't read it, I would just have to subscribe, now wouldn't I?  Another model would be to bundle access to a wide range of online publications together, much like you get when you sign up for cable or satellite TV. The NY Times, Boston Globe, WSJ, Wired, Salon, etc. should form subscription packages where you get access to all of the sites for "one low monthly payment." Just like I can't watch anywhere near all of the 200+ channels I get with my DirecTV subscription, most people can't consume all of this content, so it gives consumers the appearance of getting a lot more than they are paying for.  Newspapers aren't going anywhere fast. But they do need to get with the times and reset readers' expectations that you get what you pay for.
The NY Times is running an article today on the rise of netbooks, which are roughly defined as cheap laptops using low-power chips, sometimes without a hard drive. Of course, the terminology is fuzzy and confusing. The article claims that netbooks are poised to eat into the conventional laptop and PC market in a big way, mainly because they're cheaper. I don't buy it.  The potential game-changer for netbooks is that companies traditionally associated with the cell phone market are bringing out new processors and other components that bring the cost and power consumption down. The low-cost and low-power ARM chips don't run Windows, so some netbooks run a stripped down version of Linux (though where did the NY Times get the idea that Linux costs $3 versus $25 for Windows XP?). And many current netbooks are too puny to run "real" applications -- by which I mean things like PowerPoint, games, or playing a DVD.  Now, I'm a big Linux advocate, but I don't buy the idea that just because netbooks are cheaper, they're going to take over a significant chunk of the market. If history has taught us anything, it's clear that Intel and Microsoft will bring down their prices and power consumption profiles to compete in this space. At the end of the day, it's software that sells devices, not price or power consumption. It's wrongheaded to take the technology-centric view that because the technology changes, users will follow suit and go along with a stripped-down sub-laptop that can't run real software. We've seen this before (anybody remember WebTV?) and it hasn't worked out. The Nokia n770 "Internet Tablets" are another classic example of a device that never took off in a significant way -- just because you can build something like this, doesn't mean that anybody wants it.  So I think the industry should look at where netbooks fit in with user needs. I'm hoping Apple does a netbook (a rumor that keeps circulating) since the iPhone OS is, in many ways, an ideal netbook platform -- and the amazing growth of the iPhone app market says a lot about its capabilities. And, knowing Apple, they'll put the user first, rather than the technology.
Today in my graduate course we discussed the Berkeley snlog system. This is a declarative programming language for sensor nets, based on Datalog, and derived from the authors' previous work on P2.  Mike Lyons, one of the Ph.D. students in the course, made an interesting observation. He pointed out that programming in these very concise, domain-specific languages is like "writing haiku." The idea behind these various languages is to make programming sensornets simpler by abstracting away details and making the code tighter and easier to grok. Mike observed that he'd "rather write 10 pages of C code than 10 lines of Datalog," which is a very good point -- and one that speaks to a number of projects that equate "fewer lines of code" with "ease of programming."  A number of projects, including several of my own (Flask, Regiment), have focused on simplifying programming for sensor nets. The idea is that building up complex distributed behaviors from low-level programs, usually implemented in C, is too painful for non-experts. The main challenge is grappling with the complex dynamics that arise when you have uncertain environmental conditions, message loss, node failure, and so forth. This is difficult in conventional distributed systems, and even more painful in the extremely resource-challenged domain of sensor nets. To this end, there has been a slew of sensor net language papers, ranging from simple node-based languages to more ambitious, network-wide programming environments. For those without the patience to run them all down, Luca Mottola's PhD thesis does a great job at surveying the field.  Evaluating a language paper, especially in a relatively new domain, is often challenging. In sensor nets, CPU performance is not a good metric -- often we care more about energy consumption, communication overheads, and robustness to failure. Evaluating "ease of programming" is much more difficult. The most common quantitative measure is lines of code, but this is potentially misleading. As Mike pointed out today, the more compact a language is, the more important it is that every line of code is absolutely right -- there's less wiggle room or flexibility to explore alternative ways of implementing the same thing.  Peter Dinda's group at Northwestern has done an actual user study (gasp!) of a simple, BASIC-based language for sensor nets. While this is far better methodology, I'm worried that such an approach is too heavily biased towards absolute neophyte programmers, and this tells us very little about how effective the language will be for writing real applications. After all, most domain scientists who want to employ sensor nets are already programmers (though often in languages like MATLAB). So what is good for a user study may not be good for actual users.  So maybe we need to get away from "ease of programming" as our key metric, and focus on what domain experts really need to leverage sensor nets. It's clear that people can learn a new language if it will help them get their work done. In the end, the language is probably a lot less important than the mental abstraction required to capture a given program.
Amazon recently released a free Kindle e-book reader for the iPhone, and I love it. Normally I don't shill products, but I was pretty skeptical about this one and have been pleasantly surprised at how good it is. I've been reading Denis Johnson's Tree of Smoke on my iPhone over the last couple of weeks -- mostly at the gym but also during a few long flights. It's a long book -- over 600 pages -- and having it in my pocket at all times has made it much easier to read bits and pieces whenever I get a chance.  The app is dead simple to use: you simply flick left or right to turn pages, and it automatically remembers your place so that when you relaunch the app you are back where you left off. In the current version, you have to buy e-books via the Web, and the next time the app launches it downloads the content to your phone. I guess this is not so great for spur-of-the-moment purchases while getting ready to board a flight, but my understanding is that a future version will let you buy content directly from the iPhone. As far as I can tell, it eats very little power -- not surprising, but nice to know in case you're worried that spending a few hours reading will drain the battery.  Amazon is definitely undercutting their own Kindle e-book reader by providing this app, and they claim that it was mainly intended as a stop-gap for Kindle owners who want to do a bit of reading in places like the supermarket check-out line. (Is anyone so impatient they really need to read a book while waiting to buy groceries?)  What's it like to read a book on the iPhone? Well, the app lets you pick the font size, which is nice. For reading on the elliptical machine I crank it up fairly large, which means flicking to the next page every 10-15 seconds. When I can hold the phone close and steady I use a smaller font. Actually, I tend to prefer the narrow column of text as it helps me keep my place through saccadic movements -- not unlike some speed-reading techniques -- as I tend to get lost on a large page of text. As it turns out, I actually prefer reading on the iPhone rather than a paper book -- odd.  The best part is that more than 250,000 titles are available and I can use a device I carry in my pocket at all times -- no need for another gadget, charger, USB cable, what have you. I was sorely tempted to buy a Kindle before this app came out, but now there's really no need.