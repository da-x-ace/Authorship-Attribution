More often than not, we here in Silicon Valley are prone to idealism. We see a scenario the way we want to see it, and make predictions that fit our view of how we think the world should work, or perhaps even how we would like the world to be. This is especially true when it comes to technology. Outsider “luddites” who do not immediately grok the remarkable disruptive power of our latest and greatest technologies are doomed to the business trash heap – driven there by obsolescence and an obstinate refusal to accept their fate. Often times, our version of them “accepting their fate” would require them to abandon everything they know, walk away from the majority of their revenue, and terminate 80% of their employees. But hey, that’s their problem, not ours. We love disruption. It serves our purpose.  One often discussed target of such criticism is the media industry. There is a widespread belief that Hollywood now faces the same digital threat that has plagued the music industry over the past ten years. The argument goes something like this: There is nothing Hollywood can do to stop this train. The problem, you see, is that technology is merciless, impersonal, and unforgiving. Video can be turned into bits; Moore’s Law will make a pile of bits smaller and smaller over time; and efforts to erect pay walls will prove fruitless and even Quixotic. Studio heads should simply throw in the towel now and take what’s coming to them. Denial equals delay, and delay costs you time away from learning how to execute within your new constraints. All content will be free, and you simply have to live with that fact. The sooner you get in touch with it the sooner you will learn to execute in the new reality.  There are three key reasons why Hollywood is under less duress than Silicon Valley wants to believe. For starters, the leaders are wide-awake. Ever since Boxee offered Hulu (and were told to stop), the executive ranks at the major cable companies have been alert and engaged. Second, Hollywood has a solid track record of enforcement. They understand the stakes are high, and they are willing to invest in lobbying, regulation, litigation, and enforcement. They are also unafraid to throw around their weight (witness Viacom vs. Google). The final and most significant reason is that this is a massive, massive business, and it is critically important to understand where the money flows (most people don’t). You can spend plenty of time talking about other issues, but when it comes to understanding the key factor at play in nearly every major business decision in television, you will find affiliate fees – all $32 billion of them.  For those who do not know, affiliate fees are the primary revenue stream that funds today’s mainstream television content development. These are basically a “share” of the subscription fee you pay to your cable or satellite operator that is then shared back to the content owner/distributor (typically on a per subscriber basis). As an example, you will hear that some less notable cable-only channel was able to negotiate $0.25/sub/month, or that ESPN can negotiate $2.00/sub/month, because any aggregator would be afraid to market a television package without ESPN. Over the past 30 years, these fees have become the lifeblood of the TV content business – affecting how the major aggregators think and operate, and also affecting how content is produced, financed, and packaged.  Here are some specifics to help frame the issue. According to Matthew Harrigan at Wunderlich Securites, in 2009 DirecTV paid approximately $37/sub out of an ARPU of $85/sub to content owners for programming costs (i.e. affiliate fees). In this case, affiliate fees represent roughly 43% of total revenue for DirecTV. Similarly for Comcast, Matthew estimates programming costs at 37% of video revenue (Comcast has high-speed data and voice revenue that are separate). These are just two examples, but to give you a sense of scale these numbers represent around $7-8 billion/year each for Comcast and DirecTV. The recent, and very well written Business Week cover story on this same topic pegs the aggregate fees of all content providers at $32B per year. These are big, big numbers. To put things in perspective this is about 33% higher than Google’s annual global revenues including revenues for its advertising network.  These affiliate dollars flow through to the content producers. Estimates suggest that the annual affiliate fee revenue at companies like Viacom and Disney is around $1.5B and $2.0B respectively. On their own, numbers this large would obviously be motivational to corporate executives. But the reaction is even more intense because affiliate fees “feel like” 100% gross margin revenue. From a cost accounting perspective, a studio should allocate these fees across the content development costs, and therefore, they are not explicitly 100% GM. But as there are no significant variable costs related to the deployment of these programs to the carrier, most content owners cannot help but think about affiliate fees as 100% gross margin and therefore the key contributor to overall profitability.  Affiliate fee optimization is the key objective behind many of the industry’s most high profile strategic moves.  Here are a few examples.  Cablevision vs WABC. Recently, there was a high profile stand-off between WABC in New York City and Cablevision. As is often the case, the content owner here was threatening to cut-off access to their content precisely before a very high profile and high demand piece of content was set to air. This particular piece of content was the Oscars. A cable channel owner holds up a cable company to extract a higher per-sub affiliate fee for the next contract. They always put the customer in the middle, and both sides try to argue that they are virtuous and that the other is greedy. There have been numerous examples like this over the years, and it is common to see one of these showdowns each and every year. Modern Day Cable Channel Strategy. Today’s most typical cable strategy is built entirely around profit maximization utilizing affiliate fees. If you own a cable channel, your goal is to develop one or two key, hit programs, and fill the rest of the linear lineup with very inexpensive content. The “hits” make you a “must have” for any cable or satellite carrier – granting you the right to ask for fees. Too many hits drive up costs. This is why you will see more and more hit shows on the less well-known cable channels. Mad Men on AMC is a perfect example. How can a cable company not offer Mad Men? Once you nail the single channel game, you immediately try to proliferate that into multiple channels a la MTV and ESPN. Comcast Acquires NBC. Why would a cable distribution network want to own content?  First, it’s a hedge against rising content costs (affiliate fees).  Second, it offers leverage vis-à-vis their competition.  DirecTV needs NBC.  DirecTV will have to negotiate affiliate fees for NBC with Comcast (Comcast also owns other channels like E! Entertainment, The Golf Channel and Versus).  This helps keep Comcast’s business model in check.  It’s also why Comcast made a huge play for Disney in 2004. Affiliate fees have been rising for some time. Networks Ask for Fees. For the longest time, the major networks were not part of the affiliate fee gravy train. In fact, due to “must carry” laws, most networks never considered intentionally restricting their own distribution. They were simply pleased to get redistributed over cable and satellite. As these fees have grown in size and importance, the networks have changed their position and have come to the table asking for affiliate fees also. The WABC case above is one such example. Oprah Asks for Fees. Many people seem confused by Oprah’s decision to abandon her network television show after 25+ years of unquestionable success and relaunch it within her own cable network. Why would she do such a thing? Because she can. When Oprah launches her own network (with the help of Discovery), she will get per sub affiliate fees. Which cable company is not going to carry Oprah?  What programs will be on during the other 23 hours? As stated in #2, it really doesn’t matter. They still need to carry the Oprah channel. That said, Oprah has proven she can launch other personalities (Dr. Phil), and one would suspect that any new celebrity she “launches” will be tied to the Oprah network, increasing her leverage and her affiliate fees. Sports Networks Ask for Fees. Affiliate fees are driving an endless supply of channels for anyone that has “must see” content. The NFL has a channel, and had some high profile disagreements with the carriers over the “need” for its affiliate fee. You also see an NBA channel, an MLB channel, and pro wrestling is vying for one as well. If you own exclusive content, you might as well build a channel around it. This endless proliferation of channels will one day reach a limit, but for now it’s the game on the field. Hulu/Boxee. Many people blamed Hulu for its decision to block access on the Boxee platform. These users simply didn’t understand the power of affiliate fees. Comcast told NBC/Fox that if Hulu could distribute their content for free, then they would like to take their own affiliate fees (the newly negotiated ones in #4) to $0.00. This caused NBC/Fox to tell Hulu that maybe Boxee isn’t such a good idea. In addition to not appreciating these money flows, most of the digerati in Silicon Valley have huge misperceptions about the content owner’s preferences. They assume that content owners would like to distribute directly to consumers precisely because the Internet allows them to do so. They would no longer  be in the “death grip” of the content packager (cable and satellite companies) who take an unreasonable fee for their services. This is simply not how these content owners view the world.  Content owners absolutely prefer to be aggregated in a bundle of channels and, as a result, to receive affiliate fees. They also have little interest in “a la carte” packaging, a concept dreamed up by regulators in Washington but not desired by the heads of the content studios. Simply put, there is adequate value provided in distribution and revenue collection. To launch a direct channel (and forgo these fees), and then attempt to regain your customers one by one is a harrowing experience. Why earn your customers one by one when you can get to mass volumes, and a fixed amount of recurring revenue, through a distribution partner? If you create a new piece of camping equipment would you sell it online or try to obtain distribution through REI?  ESPN360 is a solid example of content owner’s preference for the affiliate fee driven/ distribution partner model. As the Internet became fast and pervasive, ESPN (owned by ABC/Disney) saw a clear opportunity to deliver more programming to their users and launched an online-only product called ESPN360 (recently renamed ESPN3). This on-demand, “over the top” offering is a killer product for the true sports fan, offering access to significantly more live games that was ever possible on a traditional linear cable channel. Despite the fact that ESPN has the brand, the reach, the market power, and the technology to charge users directly for this new product, they chose a different path. ESPN sought out distribution partners to bundle ESPN360 in with their standard video television packages, even though this was confusing and even baffling to most Internet users.  So against this backdrop, the cable companies have developed a remarkably shrewd strategy to simultaneously leverage their broadband infrastructure and affiliate-fee money flows. This concept, known as TV Everywhere, has two main components (once again, this move by the cable companies is extremely well articulated in the recent Business Week cover story on the same subject). First, you tell your customers that you want to provide them with a killer new service. They are already paying for all the content they receive through the linear channel stack. What if that same content could be viewed at any time “on-demand” and also through multiple devices (TV, PC, and mobile)? Sounds great so far. Who wouldn’t want this? And “everything” on a service like Comcast is more than any digital aggregator has yet even dreamed of aggregating. Ignore for a moment that this is not completely working just yet and focus on what they will “eventually” deliver. It’s also helpful to show the FCC you are being innovative, and not resting on your laurels the way a true monopolist would. Check.  Next comes the clever part.  The cable companies go to the content owners and make the following argument. With Internet-connected TVs on the horizon, you can no longer separate the Internet from the TV or the office from the living room. We pay you an affiliate fee to distribute your content to the homes we serve. We understand you have multiple distribution partners. What we don’t understand is why you would give content to some of them for free, and still expect us to pay our fees. Check-mate. This is the move that forced Hulu to a subscription model. The content owners, struggling with depressed advertising rates as a result of the global recession, quickly acquiesced to Rupert Murdoch’s assertion that maybe all their content should have a price.  Disruption disrupted.  Some have even suggested that Comcast has approached the large networks and offered an “extra” affiliate fee of around $0.50/sub to pay for over-the-top rights. Proactively increasing your own costs is a fairly unique business strategy. But this move also increases the costs for the disrupters, who are far less likely to be able to afford it.  As a result of these maneuvers, the current trend in the market is for less rather than more prime-time content to be openly available for free on the Internet.  Do you remember when South Park boldy made all episodes available for free on the Internet? Check out where things are today.  Try to watch the recent Facebook parody “You Have 0 Friends,” and you will receive the official message “DUE TO PRE-EXISTING CONTRACTUAL OBLIGATIONS, WE CANNOT STREAM THIS EPISODE UNTIL 05.08.10.” They may have wanted it to be free, until someone threatened to take their affiliate fees away. Viacom also recently removed shows like “The Daily Show” and “The Colbert report” from Hulu noting that “we could not agree on a price.” Suggesting there is a “price” at all would indicated they were discussing affiliate fees, as opposed to ad splits.  While this likely enrages the disruption enthusiasts, expect this trend to continue over the next year. More and more content owners will rip their shows “over the paid wall” as they get reacquainted with their own affection for affiliate fees. There is much speculation about Hulu’s forthcoming subscription launch with many journalists hopefully optimistic that Hulu as we know it will remain free and that all sorts of new features (TV support, iPhone support) and content (movies, back catalog) will be behind the paid wall. They may be surprised to find that “paying” may be necessary just to obtain what users see today. Affiliate fee parity may demand it.  So does this imply the end of all digital packagers? Not at all. Most clearly, NetFlix has successfully built a hybrid physical/digital strategy while maintaining its “all you can eat” model. It is also going toe-to-toe with other packagers by striking deals to lock up digital content (including TV programming). Furthermore, Hulu has executed well beyond anyone’s original expectation, and there is no reason to expect that to change as they move to a new model. One would expect them to continue to lead in terms of ease-of-use and simplicity even within a new model. Also keep in mind that Amazon has a strong VOD offering integrated into its overall purchasing experience, and many suspect both Apple and Google will enter the game as well. Despite this level of competition, all of theses vendors will need to find unique ways to compete against TV Everywhere. And with “free” off the table, the dimensions of competition will be inherently less disruptive.  There are two other potential challenges for non-facilities based content aggregators. First, as was the case with Satellite radio, we may see a “no holds barred” price war break out in an attempt to grab “exclusive” content to distinguish one’s package. As we all know, exclusive deals with the likes of Howard Stern nearly killed XM and Sirrus. DirecTV already pays $700 million per year to the NFL to have an exclusive offering of every NFL game on every weekend (NFL Sunday Ticket), and they recently coughed up over $4 billion to extend this deal. Wow. What if other digital “packagers” look for unique differentiation by leveraging the cash on their balance sheet? If this happens, any digital aggregator without deep pockets will be holding a knife at a gun fight.  The second externality that could cause trouble is “bandwidth limits” or “metered usage” on the Internet. While some people assume this will never happen (especially the idealist in Silicon Valley), the quiet momentum is building. There are continuing tests at AT&T and Time Warner, and AT&T’s president Randall Stephenson spoke openly about metered Internet pricing as recently as a month ago. Also, the Supreme Court recently put the kibosh on the FCC’s deliberate effort to make net neutrality one of its defining policies. This is perhaps an entirely separate post, but one should be confident that the rate charged the consumer by the owner of the transport for one hour of Internet video would be quite a bit higher than that for one hour of the same video over their own “optimized” TV infrastructure (backed up with an ample helping of technical analysis and white papers).  The fox isn’t just guarding the henhouse, he designed it.  There are still two legitimate arguments that trump all these discussions of affiliate fees and deft corporate strategy – piracy and content democratization.  Let’s start with piracy.  What if “BitTorrent 2.0” in whatever form it takes is just blatantly unstoppable?  No matter what you do, content has become too small relative to the big broad pipes and storage devices.  Technology trumps determination, and the minute something has been shown once, it will be free for all takers.  Isn’t this true in China today? It’s a big leap from expecting this to happen “someday” to expecting a content creator/owner to throw caution to the wind and immediately adopt a strategy that is congruent with unlimited free distribution (what is this strategy by the way?  can’t ads be removed also?). Technology is inevitably a tough competitor, but so is regulation and enforcement, and you should expect that a mighty effort on the part of a multi-billion dollar industry would mute any expectation of an overnight transformation. In her latest post at All Things Digital, Kara Swisher suggests that a recent increase in the number of intellectual property enforcement officers at the DOJ may be a direct response to the immediate needs of the entertainment industry.  Other cheerleaders of the disruption bandwagon point to the undeniable future where the availability of low-cost, high-feature camcorders at BestBuy will lead to a mass democratization of content creation. In this brave new world, the bloated and lavish infrastructure of Hollywood will give way to thousands of mini-Tarantinos who produce hit after hit on shockingly low new-world budgets that redefine the content creation business. This is the video equivalent of the infinite monkey theorem.  While this may be true when it comes to low-budget formats like game shows, talk shows, and reality television, today’s fussy television viewer has come to expect a product that is much more equivalent to feature films than home movies. Each episode of Lost costs well over $1mm to produce. Cheap cameras do not disrupt “production quality”. And let’s not forget that The Blair Witch Project was over ten years ago, and desperately stands alone as an exception and not a rule.  In the long run, the disruption zealots may be right. It may all come undone in the unstoppable Armageddon of unlimited “all you can eat” content enabled by the undeniable liberation of all bits big and small. But with $32 billion on the line, don’t expect it to happen overnight. You will be sorely disappointed.
American journalists and corporate executives have been slow to appreciate the beauty, brilliance, and consumer allure of the virtual goods business model.It’s not that they did not have data points – China is chock full of multi-US$billion market capitalization companies that are based on this business model. That said, many luddites predicted it was an “Eastern” fascination that would never spread to the West. They never fully understood it.  As a result of this headstrong denial, I have often wondered what data point would finally convince me that the West had fully accepted the reality of the virtual goods business model. Last week I received my answer. Jeff Grabow from Ernst and Young asked my partner Mitch Lasky and I to sit down with Mick Bobroff, an audit partner developing an expertise in virtual item based revenue recognition. Now I wasn’t exactly waiting for a sign from God or anything – rather just a small signal that confirmed this new model was legit. Having an audit partner at a top three accounting firm become an expert certainly qualifies as a step in the right direction.  Mick had prepared a remarkably succinct and information rich presentation (here is a link to their PDF on the subject). I was fairly excited to go through it – at least as excited as anyone should get when discussing accounting principles. Here is a summary of what Mick had to say in his presentation titled “Revenue Recognition Considerations for the Sale of Virtual Goods”. [If you want to reach Mick, his contact info: 415-894-8205, michael.bobroff@ey.com]  [Legal Point First] Michael made it clear that this document represented general observations and should not be used specifically as accounting advice. I understand and concur with regards to this post also. For your own books verify with your own accountant. There are already a ton of companies that trade on American stock exchanges (NYSE, NASDAQ) that use virtual goods models and adhere to GAAP. To the point above, they are all in China.  Examples: ChangYou (CYOU), Giant (GA), NetEase (NTES), Perfect World (PWRD), The 9 (NCTY), Shanda Games (GAME).  [For the record, TenCent is Hong Kong listed.] The current GAAP revenue recognition policies were honed with these companies. When a company sells virtual currency, this is not a revenue event (even though it may clearly be a cash event). When purchased but not yet used,  virtual currency sits on the balance sheet as a customer deposit or deferred revenue (i.e. a liability). Revenue recognition commences when virtual goods are bought with virtual currency by the consumer. Exactly how it “commences” depends on the following. There are two categories of virtual goods – (1) consumable items that are used once and gone, and (2) durable items that “work” over an extended period of time. For “consumable items” you can recognize revenue when it is consumed. For durable items (which many are), things are much trickier. You need to amortize the revenue (linearly) over the useful life of the good, or the average life of the actual user (i.e. – what is the average customer life of your customer?). This is a messy problem, especially when you understand how difficult it is to measure “customer life” when some customers never pay and others come and go in fairly random patterns. Also, your “average customer life” may change over time creating very complex accruals. The bottom line: getting this right requires quite a few database entries for tracking the sale and usage of every single virtual good sold in your digital world, in addition to the supply and usage of each virtual currency account, and the activity levels of each user (to estimate average life). These policies were not particularly surprising. That said, when I was listening to the complications of the “durable item” revenue accounting, it reminded me of something I learned for the early leaders in the virtual items space — innovative Korean companies such as Nexon.  THE “RENTAL” MODEL  About four to five years ago, the team at LindenLab (SecondLife) held a pizza night at their offices with the goal of learning more about the virtual item games that were wildly popular in Korea. We invited two bilingual gamers to install and play Audition, Kartrider, and FreeStyle. My big takeaway from that night was that not one of these titles actual allowed for the “sale” of virtual goods. Rather, each virtual item could only be “rented.” In each case, the user was given the option of one, seven, or thirty-day rental. I assumed this was Darwinian, and immediately began to wonder why “renting” might be better than outright ownership when it comes to virtual goods.  In world inventory gluts. As virtual worlds mature, they often suffer from game-wide inventory glut. Items that were once useful to newbies become throw-aways for the more advanced user, and can literally pollute the world and compromise the in-world economy. Allowing rental is like having free garbage collection.  Everything eventually goes away. User inventory clutter. More advanced users typically have a huge problem managing large inventories of items. Also, many items are trend-oriented and trends change. With the rental model, no user sits around thinking “wow, why did I really buy that two months ago and what do I do with it now?,” and “why am I buying all this stuff?” The rental model simplifies inventory management for the user. More marketing opportunities. When an item expires, it offers a unique time to re-market to the user for either an extension of the current good, to a trade to a newer, fresher, and perhaps more interesting item. Price segmentation. By offering 1, 7, and 30 day rentals, the merchant has basically price-segmented the market. This theoretically allows more users to experience the good than may have with a single, and arguably higher, price point. Good business. Why sell something that lasts forever if you can sell something that has to be naturally repurchased? Simpler accounting. I didn’t think of this sixth point until my meeting last week. The rental model does away with the notion of a “durable” virtual good, as they all expire. What’s more the time frame over which you recognize the revenue is now fixed at 1, 7, or 30 days. This dramatically reduces the accounting complexity. Thanks again to Michael and Jeff at E&Y for reaching out and setting up the meeting. It’s great to recognize that virtual goods businesses are finally mainstream here in North America, and that they even have their own appropriate accounting policies. I also appreciate having one more reason to favor “rentals” vs “sales” when it comes to virtual items.  [I have received several comments that concern this post and how it relates to SecondLife. For those of you that don't know, SecondLife doesn't actually sell virtual items, its residents do. As such, this post does not relate to SecondLife at all.  It pertains to the 98% of virtual worlds where the hosting companies ALSO is in the digital goods business. Nothing would stop SL from offering rental as a choice to its developers, but the main message is that this post does not pertain to SL (which has a different business model altogether.)]
In a recent New York Times article, Kathryn Huberty, a Morgan Stanley analyst was quoted suggesting that Apple’s iPhone is the key catalyst for an important new technology trend. “Applications make the smartphone trend a revolutionary trend – one we haven’t seen in consumer technology for many years.” This argument rings true in that the “after iPhone” smartphone market is dramatically more interesting than the “pre-iPhone” smartphone market. Later, Ms. Huberty made an even bolder statement, “The iPhone is something different. It’s changing our behavior…The game that Apple is playing is to become the Microsoft of the smartphone market.”  Or perhaps not.  Many analysts and bloggers have worked hard to position “iPhone vs. Android” as the title fight of the decade in the technology industry. It is an easy comparison to want to make. Both phones use rich microprocessors, are graphical, both have GPS and Wifi. They both run a sophisticated operating system, and they both give you access to thousands and thousands of third party applications. In most practical ways, they seem similar. However, there is one fundamental difference – business model choice.  When Apple launched the iPhone, it was able to secure an unprecedentedly strong business relationship with AT&T. Not only did Apple want control over the user interface, something carriers had been extremely reluctant to cede, it also wanted previously unrealized economics for a handset or OS designer. Apple insisted on upfront revenue dollars as well as a cut of the cellular service stream. AT&T, desperate for a win vs. Verizon, acquiesced.  The product was launched to rave reviews from analysts and consumers alike. It really was a brand new market and a brand new product. As noted earlier, we only “thought” we had seen smartphones before the iPhone. This market, as Ms. Huberty notes, looks like one that is Apple’s to lose.  With the iPhone’s massive success, it would be hard in retrospect to challenge the thinking behind Apple’s business model choice. After all, it will always be true that Apple was the company that “cracked open” the famed Walled Garden of carrier-land. They also did it with style, demanding golden economics as it disrupted a previously obstinate industry. And although AT&T may have become “comfortable” with its choices as a result of the iPhone’s success, other carriers suddenly had an “iPhone problem.” Enter Google.  If Apple’s business model is aggressive relative to the carriers, in contrast Google’s seems unrealistically accommodating. You want to control the user interface? No problem. Want access to the code? We’ll make it open source. What kind of economics do we want? Nothing at all.  What the hell, we will pay you!  That’s right.  Google will give the carrier ad splits that result from implementing the Google search box on any Android phone. FBR Capital Markets suggests that Google is taking this idea one step further in its November 24, 2009 report titled Implications of a Potential Share Shift to Android-Based Wireless Devices. “Recent support for Android-based devices appears to be correlated with significant up-front financial incventives paid by Google to both carriuer and handset vendors.” FBR goes on to suggest that these incentives may be as high as $25-50 per device. This is simply an offer that no carrier can refuse, particularly when U.S. carriers are currently in the habit of paying $50-150 per handset sold in subsidies.  While Apple may have opened the proverbial Walled Garden, it is Google, with its aggressive Android offering, that aims to obliterate it. Make no mistake about it; Apple was the pioneer with the amazing revolutionary product. Also, with no iPhone, there is no Android. This is not to say that Android copied iPhone, but rather the impetus to adopt and trust Google’s Android offering was driven by a market dynamic that resulted directly from the iPhone’s success.  Without the iPhone, it is possible that most carriers might have opted not to use Google’s OS solely for the reason that letting a powerful company like Google in the front door can be a risky strategic bet.  All of this is now history. The iPhone does exist, and it is wildly popular. There are an estimated 55 million iPhones in use around the world. Despite this remarkable success, history will also show that Apple intentionally chose a business model with plenty of room for disruption underneath its pricing structure. It also chose a single carrier as a partner, which resultantly threatened others. Then Google built a product and a strategy that allayed the carrier’s relative fears. Google gave them what they wanted, and then even gave them money. It could afford to do this because Google aims solely to protect the great business they already have in advertising, not to make money directly from the product (HW or SW in this case). Microsoft Windows, Internet Explorer, and Mozilla’s Firefox represent choke points on the personal computer whereby Google could lose search share, or at least be forced to pay a toll. In mobile, they see a chance to potentially eliminate the toll-takers.  With a business model that allows for much broader distribution and price points that are well beneath the iPhone, Google’s Android won’t compete directly with the iPhone.  For the iPhone loyalist, like Stewart Alsop who railed against Android, Android is simply not an option.  This price insensitive user demands the very best experience they can possibly have and this is still the iPhone. Users won’t switch in mass from the iPhone to the Android. It’s the other 3.95 billion cell phone users that are highly likely to consider Android a step up from their current feature phone. The Android strategy results in phones at much lower prices with much more diversity which will hit a braoder set of demographics. Apple can and will quintuple its current market share and still have a small portion of the overall cell phone market.  This is why the two products do not compete head to head. With its super aggressive model, Android will be the choice of the masses, and with its sleek design and non-compromising price point, Apple will rule the high end. Many have suggested that Apple is perfectly happy with its high-margin spot at the top of the food chain. They are doing exceptionally well with that position in the personal computer market – in fact, they are currently gaining share at an accelerating pace. So no need to worry about Apple, they are doing just fine (as their stock price suggests). They are just not currently executing a model to become the “Microsoft” of the smartphone market.  Some will argue that the best product will win the market and that Apple will still dominate the smartphone market. The history of the personal computer market is no omen for this thesis. If you think about it, the people that know this better than anyone are the exact Apple loyalists who have been frustrated for years at Apple’s lack of dominance in the PC market. Disruptive business strategies can and have trumped better products. And with no change to the current market, the Android leveraged position in the market could result in staggering unit share gains. This is not to say that the Google Android is better than or as good as the Apple iPhone. The key point is that it does not have to be. It only needs to be dramatically better than the current feature phone. Which it is.  While Apple will be fine as Android gains steam, the amount of shrapnel flying around this new marketplace is immense, so expect innocent bystanders to be compromised.  Recognize that as Google’s play here is as much defense as offense, they have less of a need to “make a profit,” at least right out of the gate.  This type of attitude always makes for a messy competitor.  Also, because of the sheer breadth of the effort in terms of number of handset makers and number of carriers, Android will be marketed extremely aggressively.  Lastly, the early application leaders are beginning to believe it’s a two horse race.  Currently the iPhone is priority number one.  That said, increasingly these application vendors are seeing Android as the primary second platform to support.  Others are falling further and further behind.  Also, Android doesn’t appear to be an OS that stops at the smartphone market.  Expect much experimentation with a variety of hardware manufactures and almost any and every embedded device market from navigation devices to e-readers to tablets and beyond.  Android gives every Korean, Taiwanese, and Chinese manufacture whoever wanted to approach these markets a huge head-start.  Additionally, the more of these vendors that build on Android, the more Android will evolve for the better.  The number of applications will increase, and the problems will get worked out.  Just like Microsoft worked its way from Windows to Windows 3 and eventually to Windows 7, Android will improve with time as well.  With its disruptive and leveraged strategy, it is Google that is attempting to be the Microsoft of the smartphone market.  Perhaps ironically, Apple is well positioned to be the “Apple” of the smartphone market.
I like to think of myself as an aficionado of business disruption. After all, as a venture capitalist it is imperative to understand ways in which a smaller private company can gain the upper hand on a large incumbent. One of the most successful ways to do this is to change the rules of the game in such a way that the incumbent would need to abandon or destroy its core business in order to lay chase to your strategy. This thinking, which was eloquently chronicled in Clay Christiansen’s The Innovator’s Delimma, is the key premise behind recently successful business movements like SAAS (Software as a Service), open source software, and the much-discussed Freemium Internet model.  And while each of these disruptions are impressive in their own right, when I read this week that Google was including free turn-by-turn navigation directions with each and every Android mobile OS, I had an immediate feeling that I was witnessing a disruptive play of a magnitude heretofore unseen.  Google has long had an interest in maps. Early in its history, the company added “Maps” as one of the coveted tab alternatives offered at the top of the screen above its famed search box. At that time, Google did what many others did to enter the mapping business – they licensed data from the two duopolists that ruled the mapping business – Tele Atlas and NavTeq. Over the years, as these two companies gained more and more power, and larger and larger market capitalizations, Google’s ambitions were growing too. Google wanted to spread its maps across the web, and to allow others to build on top of its mapping API.  The duopolists, recognizing a fox in the henhouse, were apprehensive to allow such activity.  In the summer of 2007, excitement regarding the criticality of map data (specifically turn-by-turn navigation data) reached a fever pitch.  On July 23, 2007, TomTom, the leading portable GPS device maker, agreed to buy Tele Atlas for US$2.7 billion. Shortly thereafter, on October 1, Nokia agreed to buy NavTeq for a cool US$8.1 billion. Meanwhile Google was still evolving its strategy and no longer wanted to be limited by the terms of its two contracts. As such, they informed Tele Atlas and NavTeq that they wanted to modify their license terms to allow more liberty with respect to syndication and proliferation. NavTeq balked, and in September of 2008 Google quietly dropped NavTeq, moving to just one partner for its core mapping data. Tele Atlas eventually agreed to the term modifications, but perhaps they should have sensed something bigger at play.  Rumors abound about just how many cars Google has on the roads building it own turn-by-turn mapping data as well as its unique “Google Streetview” database. Whatever it is, it must be huge. This October 13th, just over one year after dropping NavTeq, the other shoe dropped as well. Google disconnected from Tele Atlas and began to offer maps that were free and clear of either license. These maps are based on a combination of their own data as well as freely available data. Two weeks after this, Google announces free turn-by-turn directions for all Android phones. This couldn’t have been a great day for the deal teams that worked on the respective Tele Atlas and NavTeq acquisitions.  To understand just how disruptive this is to the GPS data market, you must first understand that “turn-by-turn” data was the lynchpin that held the duopoly together. Anyone could get map data (there are many free sources), but turn-by-turn data was remarkably expensive to build and maintain. As a result, no one could really duplicate it. The duopolists had price leverage and demanded remarkably high royalties, and the GPS device makers (TomTom, Garmin, Nokia) were forced to be price takers. You can see evidence of this price umbrella in the uniquely high $99.99 price point TomTom now charges for its iPhone application. When TomTom bought Tele Atlas, the die was cast.  Eat or be eaten. If you didn’t control your own data, how could you compete in the GPS market?  This is what prompted the Nokia-NavTeq deal.  Google’s free navigation feature announcement dealt a crushing blow to the GPS stocks. Garmin fell 16%. TomTom fell 21%. Imagine trying to maintain high royalty rates against this strategic move by Google. Android is not only a phone OS, it’s a CE OS. If Ford or BMW want to build an in-dash Android GPS, guess what? Google will give it to them for free. As we noted in our take on the free business model, “if a disruptive competitor can offer a product or service similar to yours for ‘free,’ and if they can make enough money to keep the lights on, then you likely have a problem.” It would be one thing if this were merely a mean-spirited play by Google to put an end to the GPS data duopoly. But it is not. There are multiple facets to this remarkably disruptive move.  While it is obvious that this maneuver creates a problem for the multi-billion dollar GPS market, it also poses real challenges for the leading smart phone players – RIM’s Blackberry and Apple’s iPhone. Without access to their own mapping data, these vendors now face an interesting dilemma. Do you risk flying naked without free navigation or do you suck it up and swallow the above average royalty fee for each and every handset? Neither option is stellar. This problem isn’t nearly as daunting as the one now faced by the Windows Mobile and Symbian teams.  As software providers, they are lucky to get a per unit royalty equal to that extracted by the GPS data guys. If they are now forced to integrate this data merely to keep their product competitive, their gross margin just went negative. Ouch!  This is not just incredible defense. Google is apt to believe that the geographic taxonomy is a wonderful skeleton for a geo-based ad network.  If your maps are distributed everywhere on the Internet and in every mobile device, you control that framework. Cash starved startups, building interesting and innovative mobile apps, will undoubtedly build on Google’s map API.  It’s rich, it is easy to use, and quite frankly the price is right. In the future, if you want to advertise your local business to people with an interest in your local market, chances are you will look to Google for that access.  Introducing the “Less Than Free” Business Model  Google’s brilliance doesn’t stop there. It is hard not to have been surprised by the rapid rise in recent buzz surrounding the Google Android Smartphone OS. When I asked a mobile industry veteran why carriers were so willing to dance with Google, a company they once feared, he suggested that Google was the “lesser of two evils.” With Blackberry and iPhone grabbing more and more subs, the carriers were losing control of the customer UI, which undoubtedly represents power and future monetization opportunities. With Android, carriers could re-claim their customer “deck.” Additionally, because Google has created an open source version of Android, carriers believe they have an “out” if they part ways with Google in the future.  I then asked my friend, “so why would they ever use the Google (non open source) license version.”  (EDIT: One of the commenters below pointed out that all Android is open source, and the Google apps pack, including the GPS, is licensed on top.  Doesn’t change the argument, but wanted the correct data included here.)  Here was the big punch line – because Google will give you ad splits on search if you use that version!  That’s right; Google will pay you to use their mobile OS. I like to call this the “less than free” business model. This is a remarkable card to play. Because of its dominance in search, Google has ad rates that blow away the competition. To compete at an equally “less than free” price point, Symbian or windows mobile would need to subsidize. Double ouch!!  “Less than free” may not stop with the mobile phone. Google’s CEO Eric Schmidt has been quite outspoken about his support for the Google Chrome OS. And there is no reason to believe that the “less than free” business model will not be used here as well. If Sony or HP or Dell builds a netbook based on Chrome OS, they will make money on every search each user initiates. Google, eager to protect its search share and market volume, will gladly pay the ad splits. Microsoft, who was already forced to lower Windows netbook pricing to fend off Linux, will be dancing with a business model inversion of epic proportion – from “you pay me” to “I pay you.”  It’s really hard to build a compensation package for your sales team on those economics.  Naysayers of these assertions will likely have the same retort – quality is key. They will argue that Google’s turn-by-turn apps are inferior to their well honed market leading products. With regard to Android, Google will lack the user interface or embedded software expertise necessary and will deliver a subpar product.  Plus, because the Android OS will be so splintered, QA testing will be difficult and incompatibility issues will abound. In the short run, these issues will exist.  Despite these challenges, it would be a dangerous strategy for any of the many threatened players in these markets to hang on to this “quality” rationalization for very long. First, Google’s products will get better over time. The sheer volume of the Android phones in the market will give them new data feeds to complement their own mapping effort. Also, they can create UGC hooks for users to embellish their own maps (like in Google Earth), offering themselves further differentiation.  With regard to Android, version 3 will be better than version 2 will be better than version 1.  Microsoft knows this game well.  Another perhaps even more important factor is that when a product is completely free, consumer expectations are low and consumer patience is high. Customers seem to really like free as a price point. I suspect they will love “less than free.”
We are clearly at a very important point in time when it comes to Internet video, especially video that is served to your television, but over the Internet (also known as “over-the-top” Internet video). Christmas of 2009 and Christmas of 2010 will mark the point in time that Internet menus began to show up in-mass on televisions, DVD players, and game machines. That said, one would be hard pressed to predict exactly how this market will evolve.  There are simply way more questions than answers. For example:  Who will own the operating system layer? Who will own the menu “stack” which dictates discovery? What will be the key features of this menu system, and which applications will be most useful and successful? What type of programs are most popular in this format? What are the typical pricing/product offerings? Will this product live inside the carrier set-top box or outside? Will the carriers that control the pipe also control this interface (either directly or indirectly)? Are these systems selling at rates that are above or below expectation?  Why or why not? Is it considered a viable alternative to cable or satellite? One way to have an advantage in “predicting” what will happen is to look at other countries that are further evolved in terms of broadband. The most obvious of these, with over 90% broadband penetration, is South Korea. Three providers in Korea offer an over-the-top Internet set-top box, and recent press suggests that there are now just over 800,000 subscribers of these services (out of roughly 16-17mm South Korean HHs).  The leader is KT with their Mega TV offering, followed by  LG Dacom, and then SK Broadband.  While these numbers are certainly impressive, if memory serves, the estimates from a few years back were for multiple-millions at this point, so for some reason the roll out has not gone exactly as planned.  According to this article from January, MegaTV has 38 live channels and 85,000 episodes in VOD format.  Also, the video included immediately after this paragraph shows an integration of the Mega TV service on the Playstation 3 (unfortunately its not in English).  This highlights the complexity of the “who owns the menu” question. Mega TV is a set-top box as well as service offering on other boxes.  Unfortunately, outside of what is shared here, I do not have much detail on exactly how this market is evolving. If any readers have more data, or have perspectives or answers to any of the questions listed above, add them to the comments or send them to me at bgurley@benchmark.com with “Korea IPTV” in the subject, and I will incorporate the responses into this post. In other words, I will try to make it a living blog post with the latest and greatest on the Korean “over-the-top” video market.  Thanks a ton – I look forward to hearing from you!
Many are speculating that the year two thousand and nine represents a fundamental turning point for the venture capital industry. Some are arguing that the industry is in dire straits after years of poor performance. Others have argued that the math simply does not work for the industry’s current size. Another theory suggests that permanent challenges with the IPO market call into question the fundamental economics of the VC industry. Lastly, some credible authors have suggested that things are so bad that a federal bailout may be in order.  What is really happening in the venture capital industry? It is indeed quite likely that the venture industry is in the process of a very substantial reduction in size, perhaps the first in the history of the industry. However, the specific catalyst for this reduction is not directly related to the issues just mentioned. In order to fully understand what is happening, one must look upstream from the venture capitalists to the source of funds, for that is where the wheels of change are in motion.  Venture capital funds receive the majority of their funds from large pension funds, endowments, and foundations which represent some of the largest pools of capital in the world. This “institutional capital” is typically managed by active fund managers who invest with the objective of earning an optimal return in order to meet the needs of the specific institution and/or to grow the size of their overall fund. These fund managers have one primary tool in their search for optimal returns: deciding which investment categories (referred to as “asset classes”) should receive which percentage of the overall capital allocation. This process is known in the financial field as “asset allocation.”  Asset allocation is the strategy an investor uses to choose specifically how to divide up capital amongst asset classes such as stocks, bonds, international stocks, international bonds, real-estate funds, leveraged buys-outs (LBOs), venture capital, as well as other obscure classes such as timber funds.  Some of these asset classes, such as stocks and bonds, are known as “liquid assets,” because these instruments trade on a daily basis on exchanges around the world. For these assets, investors can be quite sure of the exact value of their holdings, as the price is set continuously in the market. Also, if they need to sell, there is a ready market to accept the trade. Illiquid assets, also known as alternative assets, include all the other investment classes that do not trade on a daily exchange. These “private” investments (as compared to “public” liquid investments) are considered higher risk due to their illiquidity, but also are expected to earn a higher return. Some hedge funds are included in alternative assets either because they themselves invest in illiquid investments or because they put strict limitations on the trading capability of the institutional investors, rendering themselves “illiquid”.  Asset allocation is a well-studied area within the field of finance. A prototypical U.S.-based asset allocation model might allocate 25% to U.S. stocks, 30% to U.S. debt, 25% to international equity and debt, and let’s say 20% to all alternative assets. Within alternative assets, LBOs might be 60%, and venture capital could be as low as 10% (of the 20%). As a result, venture capital could be as low as 2% of a institutional fund’s overall capital allocation. Most people fail to realize just how small venture capital is in the overall scheme of things.  Very generally speaking, experts and academicians have considered it “conservative” to have a smaller allocation to all alternative assets reflecting the risks of illiquidity, the inability to ascertain price, and the higher difficulty in analyzing the non-standard vehicles. It is a fairly straightforward, conservative investment approach to favor liquidity and certainty over absolute potential upside (this is the same argument for holding bonds over stocks).  Over the past decade or so, a large number of very influential institutional funds have substantially increased their allocation in alternative assets. In some extreme cases, these investors have taken this allocation from a conservative amount of say 15-20% to well over 50% of their fund. Many people suggest that David Swensen at Yale was the original architect of a strategy to adopt a much higher allocation to alternative assets. Regardless of whether he was the leader or not, several funds simultaneously adopted this higher-risk, higher-return model. (For a more detailed look at how this evolved and why, see Ivy League Schools Learn a Lesson in Liquidity and How Harvard Investing Superstars Crashed. For an even deeper dive including comparative asset allocation models see Tough Lessons for Harvard and Yale.)  Contributing to this dynamic on the field, the early movers to this model were able to post above-average returns.* Also, due to the high disclosure policy of most universities, these above average performances were often touted in press releases. This “public benchmarking” put further pressure on competing fund managers who were not seeing equal returns, which as you might guess, led to them mimicking the same strategy. As a result, alternative assets have grown quite substantially over the past ten years. This is perhaps best seen in the size of the overall LBO market. The included chart shows the money raised in the LBO market over the past 30 years. As you can see, the amount of dollars pouring into this category over the past five years is nothing short of breathtaking.  The market contraction of late 2008 and early 2009 severely compromised the high-alternative asset allocation strategy. The liquid portion of  average portfolio contracted as much as 30-40%, which had two resulting impacts. Initially, this resulted in most fund managers having an even higher portion of their funds in illiquid investments. Ironically this was largely an accounting issue. Most likely, the illiquid pieces of their portfolio had declined just as much, but as illiquid investments are not valued on a day-to-day basis, they simply were not properly discounted at this point (over time they “would” and “are” eventually coming down). But with one’s fund already down 30% or so, no one is eager to further decrement the value. Despite that this may have only been an “accounting” issue, it presented a problem nonetheless, as many fund managers have triggers that force them to reallocate capital if they go above or below a certain asset allocation. This is one of those policies that encouraged selling at a point that may be the exact wrong time, contributing to further declines.  A second and more complicated problem also emerged. It turns out that when an institutional investor “invests” in an LBO fund they don’t actually invest the dollars all at once, rather they commit to an investment over time, which is “drawn down” by the LBO manager (venture capital works in the same way, but once again is a much smaller category). As these funds substantially increased their commitment to the LBO category, they were de facto increasing a guaranteed negative cash flow in the future to meet these draw-downs. Now, with portfolios out of balance, and lack of new liquidity events from the M&A and IPO markets, these funds have cash needs (to meet the draw-downs) that are not offset by cash availability. If anything, the universities and endowments these managers represent want more cash now to deal with the difficult overall economic environment.  To meet these new liquidity needs an institutional investor could:  Sell more of it’s liquid securities. This is problematic because it further compromises the target asset allocation. Try to sell the LBO commitments on the secondary market. As you might suspect the secondary market is extremely depressed. Some have even suggested that due to the forward cash need on an early LBO fund, an institution might have to “pay” to get out of the position, and to encourage someone else take on the future cash commitment. Default on the commitment. While this does have penalties in most cases, it would not be out of the realm of possibilities for this to occur if the investor has lost faith in the manager, and it is early in the fund (with more cash needs in the future). Try to raise more capital. Not surprisingly, donations to foundations and universities are down dramatically due to the overall decline in the capital markets. This makes this strategy unlikely. As you can see, none of these options are overly compelling.  If this is not bad enough, many institutional fund managers and the groups to whom they report (such as a board of trustees) are now second-guessing the high-alternative asset allocation model. As a result, they may desire to return to the more conservative and more traditional asset allocation of 10-20% allocated to alternative assets. Ironically, they are in no position to rebalance their portfolio precisely because they lack incremental liquidity. Think about it this way – it is very easy to shift a portfolio from liquid assets to illiquid. You simply sell positions in highly liquid securities, and buy or commit to illiquid ones. Going the other way is not so simple, as there is no ability to conveniently exit the illiquid positions.  This is a very long explanation, but the punch line is that as these large institutions adjust their portfolios and potentially abandon these more aggressive strategies, the amount of overall capital committed to alternative assets will undoubtedly shrink. As this happens, the VC industry will shrink in kind. How much will it go down? It is very hard to say. It would not be surprising for many of these funds to cut their allocation in the category in half, and as a result, it shouldn’t be surprising for the VC industry to get cut in half also.  One could argue that poor returns in the VC industry is the primary reason the category will shrink and that, as a result, the VC industry could be cut even further – or perhaps even go away. There are two key reasons that this is highly unlikely. First, one of the key tenets of finance theory is the Capital Asset Pricing Model (CAPM). The CAPM model argues that each investment has a risk, measured as Beta, which is correlated with return vs. that of the risk-free return. Venture Capital is obviously a high-Beta investment category. As of August 3rd, 2009, the S&P 500 has a negative 10-year return. As a higher-Beta category, no rational investor could reasonably expect the VC industry as a whole to outperform in a catastrophic overall equity market. In fact, the expectation would be for lower returns than the equity benchmark. This multiplicative correlation with traditional equity markets is the exact same reason that venture capital outperformed traditional equities in the late 1990’s. The bottom line is that no institutional investor should be surprised by the recent below-average performance of the entire category, all things being equal.  The second reason the category will not be abandoned is contrarianism. Most students of financial history have read the famous quote attributed to Warren Buffet, “We simply attempt to be fearful when others are greedy and to be greedy only when others are fearful.” One of the biggest fears of any investor is to abandon an investment at its low point, and then miss the corresponding recovery that would have helped offset previous poor returns. While this mindset will not guarantee the 100-year viability of the venture capital category, it should act as a governor on any mass exodus of the category. The more people that exit, the more the true believers will want to double-down.  So when will this happen? One thing for sure is it will not happen quickly. The VC industry has low barriers to entry and high barriers to exit. Theoretically, a fund raised in 2008, where all the LPs have no plans to commit to their next fund, may still be doing business in 2018. VC funds have long lives, and the point at which they decide to “not continue” is usually when they go to raise a new fund. This would typically be 3-5 years after they raised their last fund, but could be expanded to 5-7 years in a tough market. In some ways the process has already started. Stories are starting to pop up about VC funds that were unable to raise their next fund. Also, some entrepreneurs are starting to discuss favoring VCs of which they can be confident of their longevity. All in all, one should expect a large number of VC firms to call it quits over the next five years.  How should Silicon Valley think about these changes? It is important to realize that there are approximately 900 active VC firms in the U.S. alone. If that number fell to 450, it is not clear that the average Silicon Valley resident would take much notice. Another interesting data point can be found in the NVCA data outlining how much money VCs are investing in startups (as opposed to LP’s committing to VC firms). VC firms invested about $3.7B in the second quarter of 2009. Interestingly, this number is about half of the recent peak of around $8B/quarter. It is also quite similar to the investment level in the mid 1990s, prior to both the Internet bubble, and the rise of the aggressive asset allocation model. So from that perspective, this, meaning the investment level we see right now in Q2 of 2009, may be what it is going to be like in the future.  There are many reasons to believe that a reduction in the size of the VC industry will be healthy for the industry overall and should lead to above average returns in the future. This is not simply because less supply of dollars will give VCs more pricing leverage. We have seen over and over again how excess capital can lead to crowded emerging markets with as many as 5-6 VC backed competitors. Reducing this to 2-3 players will result in less cutthroat behavior and much healthier returns for all companies and entrepreneurs in the market. Additionally, at a stabilized market size of well over $15B a year, there should be plenty of capital to fund the next Microsoft, Ebay, or Google.  